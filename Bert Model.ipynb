{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/84/1bea6c34d38f3e726830d3adeca76e6e901b98cf5babd635883dbedd7ecc/bert-for-tf2-0.14.1.tar.gz (40kB)\n",
      "Collecting py-params>=0.9.6 (from bert-for-tf2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
      "Collecting params-flow>=0.8.0 (from bert-for-tf2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/0d/615c0d4aea541b4f47c761263809a02e160e7a2babd175f0ddd804776cf4/params-flow-0.8.0.tar.gz\n",
      "Requirement already satisfied: numpy in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.31.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py): started\n",
      "  Building wheel for bert-for-tf2 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Shintaki\\AppData\\Local\\pip\\Cache\\wheels\\dd\\f1\\10\\861fd7899727e4034293fb1dfef45b00f8cd476d21d3b3821e\n",
      "  Building wheel for py-params (setup.py): started\n",
      "  Building wheel for py-params (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Shintaki\\AppData\\Local\\pip\\Cache\\wheels\\67\\f5\\19\\b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
      "  Building wheel for params-flow (setup.py): started\n",
      "  Building wheel for params-flow (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Shintaki\\AppData\\Local\\pip\\Cache\\wheels\\88\\41\\05\\1a9955d1d01575bbd58aab76e22f8c7eeabba905d551576f43\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.1 params-flow-0.8.0 py-params-0.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart = sns.countplot(data.type)\n",
    "plt.title(\"Number of users per personality\")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "  \n",
    "unique_type_list = unique_type_list + [x.lower() for x in unique_type_list]\n",
    "# Preprocess data\n",
    "def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments using regular expressions\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "        temp = re.sub('\\|\\|\\|',\"[SEP] \",temp)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp)\n",
    "        temp = re.sub(\"(SEP )+\",\"[SEP] \",temp) \n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t,\"\")\n",
    "        '''temp= \"[CLS]\" + temp\n",
    "        temp = re.sub(\"(\\[CLS\\] *\\[SEP\\])\",\"[CLS] \",temp)'''\n",
    "        temp = re.sub(\"\\[SEP\\]+\",\"| \",temp) \n",
    "        type_labelized = translate_personality(row[1].type)\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |   and  moments sportscenter not top ten plays pranks |  What has been the most life changing experience in your life |  On repeat for most of today |  May the PerC Experience immerse you |  The last thing my  friend posted on his facebook before committing suicide the next day Rest in peace |  Hello  Sorry to hear of your distress It s only natural for a relationship to not be perfection all the time in every moment of existence Try to figure the hard times as times of growth as |  Welcome and stuff |  Game Set Match |  Prozac wellbrutin at least thirty minutes of moving your legs and I don t mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternative |  Basically come up with three items you ve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left by |  All things in moderation Sims is indeed a video game and a good one at that Note a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim |  Dear  What were your favorite video games growing up and what are your now current favorite video games cool |  It appears to be too late sad |  There s someone out there for everyone |  Wait I thought confidence was a good thing |  I just cherish the time of solitude b c i revel within my inner world more whereas most other time i d be workin just enjoy the me time while you can Don t worry people will always be around to |  Yo  ladies if you re into a complimentary personality well hey |  when your main social outlet is xbox live conversations and even then you verbally fatigue quickly |  I really dig the part from to |  Banned because this thread requires it of me |  Get high in backyard roast and eat marshmellows in backyard while conversing over something intellectual followed by massages and kisses |  Banned for too many b s in that sentence How could you Think of the B |  Banned for watching movies in the corner with the dunces |  Banned because Health class clearly taught you nothing about peer pressure |  Banned for a whole host of reasons |  Two baby deer on left and right munching on a beetle in the middle Using their own blood two cavemen diary today s latest happenings on their designated cave diary wall I see it as |  a pokemon world an  society everyone becomes an optimist |  Not all artists are artists because they draw It s the idea that counts in forming something of your own like a signature |  Welcome to the robot ranks person who downed my self esteem cuz I m not an avid signature artist like herself proud |  Banned for taking all the room under my bed Ya gotta learn to share with the roaches |  Banned for being too much of a thundering grumbling kind of storm yep |  Ahh old high school music I haven t heard in ages |  I failed a public speaking class a few years ago and I ve sort of learned what I could do better were I to be in that position again A big part of my failure was just overloading myself with too |  I like this person s mentality He s a confirmed  by the way |  Move to the Denver area and start a new life for myself \n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X=list_posts\n",
    "Y=list_personality\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', \"'\", 'm', 'here', 'to', 'stay', ',', 'hello', 'nice', 'to', 'meet', 'you', 'sir', '[SEP]']\n",
      "[101, 1045, 1005, 1049, 2182, 2000, 2994, 1010, 7592, 3835, 2000, 3113, 2017, 2909, 102]\n"
     ]
    }
   ],
   "source": [
    "# importing the tokenizer and testing it\n",
    "tokenizer = FullTokenizer(\n",
    "  vocab_file=\"./dataset/bert/vocab.txt\"\n",
    ")\n",
    "tokens=tokenizer.tokenize(\"I'm here to stay , hello nice to meet you sir\")\n",
    "tokens=[\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "print(tokens)\n",
    "indices=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 8675 rows\n",
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the features\n",
    "def tokenize_features(X):\n",
    "    X_tokenized=[]\n",
    "    for i,entry in enumerate(X):\n",
    "        if (i % 500 == 0 or i == 1 or i == len(X)):\n",
    "            print(\"%s of %s rows\" % (i, len(X)))\n",
    "        temp=[]\n",
    "        temp=tokenizer.tokenize(entry)\n",
    "        temp = [w.replace('|', '[SEP]') for w in temp]\n",
    "        if temp[0] == \"[SEP]\":\n",
    "            temp.pop(0)\n",
    "        temp.insert(0,\"[CLS]\")\n",
    "        temp.append(\"[SEP]\")\n",
    "        X_tokenized.append(temp)\n",
    "    return(X_tokenized)\n",
    "\n",
    "X_tokenized=tokenize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1998, 5312, 2998, 13013, 2121, 2025, 2327, 2702, 3248, 26418, 2015, 102, 2054, 2038, 2042, 1996, 2087, 2166, 5278, 3325, 1999, 2115, 2166, 102, 2006, 9377, 2005, 2087, 1997, 2651, 102, 2089, 1996, 2566, 2278, 3325, 10047, 16862, 2063, 2017, 102, 1996, 2197, 2518, 2026, 2767, 6866, 2006, 2010, 9130, 2077, 16873, 5920, 1996, 2279, 2154, 2717, 1999, 3521, 102, 7592, 3374, 2000, 2963, 1997, 2115, 12893, 2009, 1055, 2069, 3019, 2005, 1037, 3276, 2000, 2025, 2022, 15401, 2035, 1996, 2051, 1999, 2296, 2617, 1997, 4598, 3046, 2000, 3275, 1996, 2524, 2335, 2004, 2335, 1997, 3930, 2004, 102, 6160, 1998, 4933, 102, 2208, 2275, 2674, 102, 4013, 4143, 2278, 2092, 19892, 21823, 2078, 2012, 2560, 4228, 2781, 1997, 3048, 2115, 3456, 1998, 1045, 2123, 1056, 2812, 3048, 2068, 2096, 3564, 1999, 2115, 2168, 4624, 3242, 17901, 1999, 5549, 8156, 2672, 3046, 21006, 2015, 2004, 1037, 2740, 3771, 4522, 102, 10468, 2272, 2039, 2007, 2093, 5167, 2017, 2310, 4340, 2008, 2169, 2828, 2030, 29221, 4127, 2017, 2215, 2000, 2079, 2052, 2062, 2084, 3497, 2224, 2445, 2169, 4127, 10699, 4972, 1998, 2054, 17048, 2043, 2187, 2011, 102, 2035, 2477, 1999, 5549, 8156, 18135, 2003, 5262, 1037, 2678, 2208, 1998, 1037, 2204, 2028, 2012, 2008, 3602, 1037, 2204, 2028, 2012, 2008, 2003, 5399, 20714, 1999, 2008, 1045, 2572, 2025, 3294, 7694, 1996, 2331, 1997, 2151, 2445, 21934, 102, 6203, 2054, 2020, 2115, 5440, 2678, 2399, 3652, 2039, 1998, 2054, 2024, 2115, 2085, 2783, 5440, 2678, 2399, 4658, 102, 2009, 3544, 2000, 2022, 2205, 2397, 6517, 102, 2045, 1055, 2619, 2041, 2045, 2005, 3071, 102, 3524, 1045, 2245, 7023, 2001, 1037, 2204, 2518, 102, 1045, 2074, 24188, 4509, 1996, 2051, 1997, 22560, 1038, 1039, 1045, 7065, 2884, 2306, 2026, 5110, 2088, 2062, 6168, 2087, 2060, 2051, 1045, 1040, 2022, 2147, 2378, 2074, 5959, 1996, 2033, 2051, 2096, 2017, 2064, 2123, 1056, 4737, 2111, 2097, 2467, 2022, 2105, 2000, 102, 10930, 6456, 2065, 2017, 2128, 2046, 1037, 19394, 5649, 6180, 2092, 4931, 102, 2043, 2115, 2364, 2591, 13307, 2003, 12202, 2444, 11450, 1998, 2130, 2059, 2017, 12064, 2135, 16342, 2855, 102, 1045, 2428, 10667, 1996, 2112, 2013, 2000, 102, 7917, 2138, 2023, 11689, 5942, 2009, 1997, 2033, 102, 2131, 2152, 1999, 16125, 25043, 1998, 4521, 9409, 10199, 8261, 2015, 1999, 16125, 2096, 9530, 14028, 2075, 2058, 2242, 7789, 2628, 2011, 21881, 2015, 1998, 8537, 102, 7917, 2005, 2205, 2116, 1038, 1055, 1999, 2008, 6251, 2129, 2071, 2017, 2228, 1997, 1996, 1038, 102, 7917, 2005, 3666, 5691, 1999, 1996, 3420, 2007, 1996, 24654, 9623, 102, 7917, 2138, 2740, 2465, 4415, 4036, 2017, 2498, 2055, 8152, 3778, 102, 7917, 2005, 1037, 2878, 3677, 1997, 4436, 102, 2048, 3336, 8448, 2006, 2187, 1998, 2157, 14163, 12680, 2075, 2006, 1037, 7813, 1999, 1996, 2690, 2478, 2037, 2219, 2668, 2048, 5430, 3549, 9708, 2651, 1055, 6745, 6230, 2015, 2006, 2037, 4351, 5430, 9708, 2813, 1045, 2156, 2009, 2004, 102, 1037, 20421, 2088, 2019, 2554, 3071, 4150, 2019, 23569, 27605, 3367, 102, 2025, 2035, 3324, 2024, 3324, 2138, 2027, 4009, 2009, 1055, 1996, 2801, 2008, 9294, 1999, 5716, 2242, 1997, 2115, 2219, 2066, 1037, 8085, 102, 6160, 2000, 1996, 8957, 6938, 2711, 2040, 20164, 2026, 2969, 19593, 12731, 2480, 1045, 1049, 2025, 2019, 18568, 8085, 3063, 2066, 2841, 7098, 102, 7917, 2005, 2635, 2035, 1996, 2282, 2104, 2026, 2793, 8038, 10657, 4553, 2000, 3745, 2007, 1996, 20997, 2229, 102, 7917, 2005, 2108, 2205, 2172, 1997, 1037, 8505, 2075, 24665, 25438, 2989, 2785, 1997, 4040, 15624, 102, 6289, 2232, 2214, 2152, 2082, 2189, 1045, 4033, 1056, 2657, 1999, 5535, 102, 1045, 3478, 1037, 2270, 4092, 2465, 1037, 2261, 2086, 3283, 1998, 1045, 2310, 4066, 1997, 4342, 2054, 1045, 2071, 2079, 2488, 2020, 1045, 2000, 2022, 1999, 2008, 2597, 2153, 1037, 2502, 2112, 1997, 2026, 4945, 2001, 2074, 2058, 18570, 2870, 2007, 2205, 102, 1045, 2066, 2023, 2711, 1055, 5177, 3012, 2002, 1055, 1037, 4484, 2011, 1996, 2126, 102, 2693, 2000, 1996, 7573, 2181, 1998, 2707, 1037, 2047, 2166, 2005, 2870, 102]\n"
     ]
    }
   ],
   "source": [
    "# indexing the tokens\n",
    "def tokens_to_indices(tokens):\n",
    "    result = []\n",
    "    for element in tokens:\n",
    "        indices =[]\n",
    "        indices=tokenizer.convert_tokens_to_ids(element)\n",
    "        result.append(indices)\n",
    "    return result\n",
    "X_indexed=tokens_to_indices(X_tokenized)\n",
    "print(X_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n",
      "1418\n"
     ]
    }
   ],
   "source": [
    "# getting the longest indexed array\n",
    "def max_post_words(posts):\n",
    "    maxLen=0\n",
    "    averageLen=0\n",
    "    for post in posts:\n",
    "        averageLen=averageLen+len(post)\n",
    "        if maxLen < len(post):\n",
    "            maxLen=len(post)\n",
    "    averageLen=int(averageLen/len(posts))\n",
    "    return maxLen,averageLen\n",
    "maxLen,avgLen=max_post_words(X_indexed)\n",
    "print(maxLen)\n",
    "print(avgLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding padding \n",
    "def padding(indices):\n",
    "    for i,element in enumerate(indices):\n",
    "        pad=maxLen-len(element)\n",
    "        l = [0] * pad\n",
    "        indices[i]=indices[i] + l\n",
    "    return(indices)\n",
    "X=np.array(padding(X_indexed))\n",
    "Y=(np.array(list_personality)).astype(np.float32)\n",
    "# Not enough memory to train model with maxLen input size so i'll take the first 512\n",
    "X=X[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    with tf.io.gfile.GFile(\"./dataset/bert/bert_config.json\", \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "    input_ids = keras.layers.Input(shape=(512,), dtype='int16', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    X1 = keras.layers.Dense(units=1,name=\"I/E_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X2 = keras.layers.Dense(units=1,name=\"N/S_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X3 = keras.layers.Dense(units=1,name=\"F/T_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X4 = keras.layers.Dense(units=1,name=\"J/P_classifier\", activation=\"sigmoid\")(logits)\n",
    "    finalOutput=keras.layers.concatenate(\n",
    "    inputs=[X1,X2,X3,X4],\n",
    "    name='final_output')\n",
    "    model = keras.Model(inputs=input_ids, outputs=finalOutput)\n",
    "    model.build(input_shape=(None, 512))\n",
    "    load_stock_weights(bert, \"./dataset/bert/bert_model.ckpt\")\n",
    "    model.layers[1].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 512, 768)\n",
      "Done loading 196 BERT weights from: ./dataset/bert/bert_model.ckpt into <bert.model.BertModelLayer object at 0x00000214247C9160> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModelLayer)           (None, 512, 768)     108890112   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 768)          590592      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "I/E_classifier (Dense)          (None, 1)            769         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "N/S_classifier (Dense)          (None, 1)            769         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "F/T_classifier (Dense)          (None, 1)            769         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "J/P_classifier (Dense)          (None, 1)            769         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "final_output (Concatenate)      (None, 4)            0           I/E_classifier[0][0]             \n",
      "                                                                 N/S_classifier[0][0]             \n",
      "                                                                 F/T_classifier[0][0]             \n",
      "                                                                 J/P_classifier[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 109,483,780\n",
      "Trainable params: 593,668\n",
      "Non-trainable params: 108,890,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bertModel = create_model()\n",
    "bertModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss(y_true,y_pred):\n",
    "    return K.mean(K.sum(K.binary_crossentropy(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6940 samples, validate on 867 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4384/6940 [=================>............] - ETA: 1:09:14 - loss: 121.9415 - accuracy: 0.1875 - f1: 0.46 - ETA: 42:07 - loss: 114.4962 - accuracy: 0.2969 - f1: 0.4960 - ETA: 32:59 - loss: 115.5896 - accuracy: 0.3125 - f1: 0.50 - ETA: 28:29 - loss: 113.0035 - accuracy: 0.2891 - f1: 0.50 - ETA: 25:42 - loss: 114.3981 - accuracy: 0.2688 - f1: 0.50 - ETA: 23:49 - loss: 115.3243 - accuracy: 0.2396 - f1: 0.49 - ETA: 22:27 - loss: 116.0373 - accuracy: 0.2455 - f1: 0.49 - ETA: 21:24 - loss: 117.0668 - accuracy: 0.2305 - f1: 0.48 - ETA: 20:34 - loss: 116.9474 - accuracy: 0.2188 - f1: 0.48 - ETA: 19:53 - loss: 116.1530 - accuracy: 0.2125 - f1: 0.48 - ETA: 19:19 - loss: 114.9049 - accuracy: 0.2188 - f1: 0.48 - ETA: 18:51 - loss: 114.4932 - accuracy: 0.2188 - f1: 0.48 - ETA: 18:26 - loss: 113.6901 - accuracy: 0.2212 - f1: 0.48 - ETA: 18:04 - loss: 113.5540 - accuracy: 0.2121 - f1: 0.47 - ETA: 17:44 - loss: 112.9033 - accuracy: 0.2125 - f1: 0.47 - ETA: 17:26 - loss: 112.1172 - accuracy: 0.2070 - f1: 0.47 - ETA: 17:10 - loss: 111.1408 - accuracy: 0.2132 - f1: 0.47 - ETA: 16:55 - loss: 109.9051 - accuracy: 0.2240 - f1: 0.48 - ETA: 16:42 - loss: 109.6430 - accuracy: 0.2237 - f1: 0.48 - ETA: 16:29 - loss: 108.7078 - accuracy: 0.2328 - f1: 0.48 - ETA: 16:17 - loss: 108.1118 - accuracy: 0.2381 - f1: 0.48 - ETA: 16:06 - loss: 107.5958 - accuracy: 0.2401 - f1: 0.48 - ETA: 15:55 - loss: 106.7160 - accuracy: 0.2514 - f1: 0.48 - ETA: 15:45 - loss: 106.4564 - accuracy: 0.2526 - f1: 0.48 - ETA: 15:35 - loss: 105.8239 - accuracy: 0.2537 - f1: 0.48 - ETA: 15:26 - loss: 105.2533 - accuracy: 0.2512 - f1: 0.48 - ETA: 15:17 - loss: 105.3291 - accuracy: 0.2500 - f1: 0.48 - ETA: 15:08 - loss: 105.0530 - accuracy: 0.2500 - f1: 0.48 - ETA: 15:00 - loss: 104.7382 - accuracy: 0.2489 - f1: 0.48 - ETA: 14:52 - loss: 104.6107 - accuracy: 0.2479 - f1: 0.48 - ETA: 14:44 - loss: 104.6565 - accuracy: 0.2440 - f1: 0.48 - ETA: 14:37 - loss: 104.3345 - accuracy: 0.2432 - f1: 0.48 - ETA: 14:29 - loss: 104.0378 - accuracy: 0.2396 - f1: 0.48 - ETA: 14:22 - loss: 103.6820 - accuracy: 0.2426 - f1: 0.48 - ETA: 14:15 - loss: 103.3843 - accuracy: 0.2402 - f1: 0.48 - ETA: 14:08 - loss: 103.1038 - accuracy: 0.2378 - f1: 0.48 - ETA: 14:02 - loss: 102.6452 - accuracy: 0.2399 - f1: 0.48 - ETA: 13:55 - loss: 102.4831 - accuracy: 0.2385 - f1: 0.48 - ETA: 13:49 - loss: 102.4078 - accuracy: 0.2364 - f1: 0.48 - ETA: 13:42 - loss: 102.1976 - accuracy: 0.2352 - f1: 0.48 - ETA: 13:36 - loss: 101.8472 - accuracy: 0.2348 - f1: 0.48 - ETA: 13:30 - loss: 101.6126 - accuracy: 0.2366 - f1: 0.48 - ETA: 13:24 - loss: 101.4050 - accuracy: 0.2340 - f1: 0.48 - ETA: 13:18 - loss: 101.0387 - accuracy: 0.2386 - f1: 0.48 - ETA: 13:12 - loss: 100.9488 - accuracy: 0.2375 - f1: 0.48 - ETA: 13:06 - loss: 100.8433 - accuracy: 0.2371 - f1: 0.48 - ETA: 13:00 - loss: 100.7595 - accuracy: 0.2354 - f1: 0.48 - ETA: 12:55 - loss: 100.7605 - accuracy: 0.2350 - f1: 0.47 - ETA: 12:49 - loss: 100.3565 - accuracy: 0.2379 - f1: 0.47 - ETA: 12:44 - loss: 100.1595 - accuracy: 0.2387 - f1: 0.47 - ETA: 12:38 - loss: 99.9485 - accuracy: 0.2390 - f1: 0.4804 - ETA: 12:33 - loss: 99.6988 - accuracy: 0.2392 - f1: 0.478 - ETA: 12:27 - loss: 99.3793 - accuracy: 0.2423 - f1: 0.478 - ETA: 12:22 - loss: 99.2759 - accuracy: 0.2413 - f1: 0.476 - ETA: 12:16 - loss: 99.2161 - accuracy: 0.2398 - f1: 0.474 - ETA: 12:11 - loss: 99.0104 - accuracy: 0.2394 - f1: 0.475 - ETA: 12:06 - loss: 98.7470 - accuracy: 0.2379 - f1: 0.475 - ETA: 12:01 - loss: 98.5464 - accuracy: 0.2365 - f1: 0.476 - ETA: 11:55 - loss: 98.2533 - accuracy: 0.2373 - f1: 0.476 - ETA: 11:50 - loss: 97.9568 - accuracy: 0.2380 - f1: 0.476 - ETA: 11:45 - loss: 97.9211 - accuracy: 0.2362 - f1: 0.477 - ETA: 11:40 - loss: 97.7356 - accuracy: 0.2364 - f1: 0.475 - ETA: 11:35 - loss: 97.5196 - accuracy: 0.2351 - f1: 0.476 - ETA: 11:30 - loss: 97.3420 - accuracy: 0.2344 - f1: 0.475 - ETA: 11:25 - loss: 97.1316 - accuracy: 0.2361 - f1: 0.476 - ETA: 11:20 - loss: 96.8011 - accuracy: 0.2358 - f1: 0.478 - ETA: 11:15 - loss: 96.6654 - accuracy: 0.2351 - f1: 0.479 - ETA: 11:10 - loss: 96.5438 - accuracy: 0.2353 - f1: 0.477 - ETA: 11:05 - loss: 96.4710 - accuracy: 0.2346 - f1: 0.477 - ETA: 11:00 - loss: 96.3324 - accuracy: 0.2348 - f1: 0.476 - ETA: 10:55 - loss: 96.2834 - accuracy: 0.2350 - f1: 0.476 - ETA: 10:50 - loss: 96.0764 - accuracy: 0.2365 - f1: 0.477 - ETA: 10:45 - loss: 95.9449 - accuracy: 0.2376 - f1: 0.478 - ETA: 10:40 - loss: 95.6055 - accuracy: 0.2386 - f1: 0.479 - ETA: 10:36 - loss: 95.5454 - accuracy: 0.2392 - f1: 0.479 - ETA: 10:31 - loss: 95.2617 - accuracy: 0.2405 - f1: 0.480 - ETA: 10:26 - loss: 95.3564 - accuracy: 0.2394 - f1: 0.479 - ETA: 10:21 - loss: 95.2825 - accuracy: 0.2404 - f1: 0.479 - ETA: 10:16 - loss: 95.0652 - accuracy: 0.2425 - f1: 0.480 - ETA: 10:11 - loss: 94.9717 - accuracy: 0.2426 - f1: 0.480 - ETA: 10:07 - loss: 94.7773 - accuracy: 0.2442 - f1: 0.480 - ETA: 10:02 - loss: 94.6157 - accuracy: 0.2443 - f1: 0.481 - ETA: 9:57 - loss: 94.5246 - accuracy: 0.2440 - f1: 0.482 - ETA: 9:52 - loss: 94.2860 - accuracy: 0.2448 - f1: 0.48 - ETA: 9:48 - loss: 94.1971 - accuracy: 0.2456 - f1: 0.48 - ETA: 9:43 - loss: 94.0918 - accuracy: 0.2467 - f1: 0.48 - ETA: 9:38 - loss: 94.0406 - accuracy: 0.2468 - f1: 0.48 - ETA: 9:34 - loss: 93.9265 - accuracy: 0.2472 - f1: 0.48 - ETA: 9:29 - loss: 93.8966 - accuracy: 0.2475 - f1: 0.48 - ETA: 9:24 - loss: 93.8574 - accuracy: 0.2476 - f1: 0.47 - ETA: 9:20 - loss: 93.7581 - accuracy: 0.2469 - f1: 0.47 - ETA: 9:15 - loss: 93.6444 - accuracy: 0.2486 - f1: 0.47 - ETA: 9:10 - loss: 93.5305 - accuracy: 0.2480 - f1: 0.47 - ETA: 9:06 - loss: 93.4670 - accuracy: 0.2473 - f1: 0.47 - ETA: 9:01 - loss: 93.3992 - accuracy: 0.2477 - f1: 0.47 - ETA: 8:56 - loss: 93.2930 - accuracy: 0.2484 - f1: 0.47 - ETA: 8:52 - loss: 93.1653 - accuracy: 0.2487 - f1: 0.47 - ETA: 8:47 - loss: 93.1704 - accuracy: 0.2490 - f1: 0.47 - ETA: 8:42 - loss: 93.1279 - accuracy: 0.2481 - f1: 0.47 - ETA: 8:38 - loss: 92.9029 - accuracy: 0.2506 - f1: 0.47 - ETA: 8:33 - loss: 92.8741 - accuracy: 0.2506 - f1: 0.47 - ETA: 8:29 - loss: 92.8481 - accuracy: 0.2506 - f1: 0.47 - ETA: 8:24 - loss: 92.7772 - accuracy: 0.2512 - f1: 0.47 - ETA: 8:20 - loss: 92.6095 - accuracy: 0.2518 - f1: 0.47 - ETA: 8:15 - loss: 92.4578 - accuracy: 0.2530 - f1: 0.47 - ETA: 8:10 - loss: 92.4303 - accuracy: 0.2527 - f1: 0.47 - ETA: 8:06 - loss: 92.2109 - accuracy: 0.2544 - f1: 0.47 - ETA: 8:01 - loss: 92.1152 - accuracy: 0.2555 - f1: 0.47 - ETA: 7:57 - loss: 92.0399 - accuracy: 0.2543 - f1: 0.47 - ETA: 7:52 - loss: 91.8809 - accuracy: 0.2557 - f1: 0.47 - ETA: 7:48 - loss: 91.7956 - accuracy: 0.2556 - f1: 0.48 - ETA: 7:43 - loss: 91.8234 - accuracy: 0.2550 - f1: 0.48 - ETA: 7:38 - loss: 91.6272 - accuracy: 0.2550 - f1: 0.48 - ETA: 7:34 - loss: 91.5325 - accuracy: 0.2555 - f1: 0.48 - ETA: 7:29 - loss: 91.5079 - accuracy: 0.2552 - f1: 0.48 - ETA: 7:25 - loss: 91.3341 - accuracy: 0.2548 - f1: 0.48 - ETA: 7:20 - loss: 91.2878 - accuracy: 0.2540 - f1: 0.48 - ETA: 7:16 - loss: 91.3008 - accuracy: 0.2540 - f1: 0.48 - ETA: 7:11 - loss: 91.2243 - accuracy: 0.2542 - f1: 0.48 - ETA: 7:07 - loss: 91.1244 - accuracy: 0.2547 - f1: 0.48 - ETA: 7:02 - loss: 91.1041 - accuracy: 0.2557 - f1: 0.48 - ETA: 6:58 - loss: 91.0730 - accuracy: 0.2554 - f1: 0.48 - ETA: 6:53 - loss: 91.0371 - accuracy: 0.2558 - f1: 0.48 - ETA: 6:49 - loss: 90.9649 - accuracy: 0.2555 - f1: 0.48 - ETA: 6:44 - loss: 90.9130 - accuracy: 0.2560 - f1: 0.48 - ETA: 6:40 - loss: 90.8613 - accuracy: 0.2562 - f1: 0.48 - ETA: 6:35 - loss: 90.7975 - accuracy: 0.2569 - f1: 0.48 - ETA: 6:31 - loss: 90.7698 - accuracy: 0.2559 - f1: 0.48 - ETA: 6:26 - loss: 90.6971 - accuracy: 0.2563 - f1: 0.48 - ETA: 6:22 - loss: 90.6132 - accuracy: 0.2572 - f1: 0.48 - ETA: 6:17 - loss: 90.5480 - accuracy: 0.2567 - f1: 0.48 - ETA: 6:13 - loss: 90.4368 - accuracy: 0.2573 - f1: 0.48 - ETA: 6:09 - loss: 90.4420 - accuracy: 0.2566 - f1: 0.48 - ETA: 6:04 - loss: 90.3491 - accuracy: 0.2563 - f1: 0.48 - ETA: 6:00 - loss: 90.2357 - accuracy: 0.2560 - f1: 0.48 - ETA: 5:55 - loss: 90.1993 - accuracy: 0.2562 - f1: 0.48 - ETA: 5:51 - loss: 90.1239 - accuracy: 0.2571 - f1: 0.4851\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 5:47 - loss: 90.0400 - accuracy: 0.2570 - f1: 0.48 - ETA: 5:42 - loss: 90.0002 - accuracy: 0.2576 - f1: 0.48 - ETA: 5:38 - loss: 89.9871 - accuracy: 0.2578 - f1: 0.48 - ETA: 5:33 - loss: 90.0164 - accuracy: 0.2571 - f1: 0.48 - ETA: 5:29 - loss: 90.0277 - accuracy: 0.2566 - f1: 0.48 - ETA: 5:24 - loss: 90.0232 - accuracy: 0.2561 - f1: 0.48 - ETA: 5:20 - loss: 89.9856 - accuracy: 0.2559 - f1: 0.48 - ETA: 5:16 - loss: 89.9902 - accuracy: 0.2560 - f1: 0.48 - ETA: 5:11 - loss: 89.9115 - accuracy: 0.2566 - f1: 0.48 - ETA: 5:07 - loss: 89.8468 - accuracy: 0.2566 - f1: 0.48 - ETA: 5:02 - loss: 89.8447 - accuracy: 0.2570 - f1: 0.48 - ETA: 4:58 - loss: 89.8522 - accuracy: 0.2565 - f1: 0.48 - ETA: 4:54 - loss: 89.7697 - accuracy: 0.2567 - f1: 0.48 - ETA: 4:49 - loss: 89.8246 - accuracy: 0.2554 - f1: 0.48 - ETA: 4:45 - loss: 89.7964 - accuracy: 0.2551 - f1: 0.48 - ETA: 4:40 - loss: 89.7185 - accuracy: 0.2551 - f1: 0.48 - ETA: 4:36 - loss: 89.6015 - accuracy: 0.2555 - f1: 0.48 - ETA: 4:32 - loss: 89.4753 - accuracy: 0.2548 - f1: 0.48 - ETA: 4:27 - loss: 89.4521 - accuracy: 0.2554 - f1: 0.48 - ETA: 4:23 - loss: 89.4548 - accuracy: 0.2550 - f1: 0.48 - ETA: 4:18 - loss: 89.4394 - accuracy: 0.2549 - f1: 0.48 - ETA: 4:14 - loss: 89.5318 - accuracy: 0.2537 - f1: 0.48 - ETA: 4:10 - loss: 89.5046 - accuracy: 0.2533 - f1: 0.48 - ETA: 4:05 - loss: 89.5266 - accuracy: 0.2533 - f1: 0.48 - ETA: 4:01 - loss: 89.4758 - accuracy: 0.2535 - f1: 0.48 - ETA: 3:56 - loss: 89.4517 - accuracy: 0.2536 - f1: 0.48 - ETA: 3:52 - loss: 89.4016 - accuracy: 0.2542 - f1: 0.48 - ETA: 3:48 - loss: 89.4275 - accuracy: 0.2540 - f1: 0.48 - ETA: 3:43 - loss: 89.4455 - accuracy: 0.2540 - f1: 0.48 - ETA: 3:39 - loss: 89.4059 - accuracy: 0.2547 - f1: 0.48 - ETA: 3:34 - loss: 89.3992 - accuracy: 0.2545 - f1: 0.48 - ETA: 3:30 - loss: 89.3458 - accuracy: 0.2544 - f1: 0.48 - ETA: 3:26 - loss: 89.3085 - accuracy: 0.2544 - f1: 0.48 - ETA: 3:21 - loss: 89.2628 - accuracy: 0.2546 - f1: 0.48 - ETA: 3:17 - loss: 89.3216 - accuracy: 0.2540 - f1: 0.48 - ETA: 3:12 - loss: 89.2700 - accuracy: 0.2543 - f1: 0.48 - ETA: 3:08 - loss: 89.3182 - accuracy: 0.2538 - f1: 0.48 - ETA: 3:04 - loss: 89.3034 - accuracy: 0.2537 - f1: 0.48 - ETA: 2:59 - loss: 89.2895 - accuracy: 0.2532 - f1: 0.48 - ETA: 2:55 - loss: 89.2325 - accuracy: 0.2530 - f1: 0.48 - ETA: 2:50 - loss: 89.1229 - accuracy: 0.2532 - f1: 0.48 - ETA: 2:46 - loss: 89.1054 - accuracy: 0.2535 - f1: 0.48 - ETA: 2:42 - loss: 89.0354 - accuracy: 0.2535 - f1: 0.48 - ETA: 2:37 - loss: 89.0156 - accuracy: 0.2531 - f1: 0.48 - ETA: 2:33 - loss: 88.9402 - accuracy: 0.2534 - f1: 0.48 - ETA: 2:28 - loss: 88.9891 - accuracy: 0.2532 - f1: 0.48 - ETA: 2:24 - loss: 88.9248 - accuracy: 0.2539 - f1: 0.48 - ETA: 2:20 - loss: 88.8755 - accuracy: 0.2534 - f1: 0.48 - ETA: 2:15 - loss: 88.8916 - accuracy: 0.2530 - f1: 0.48 - ETA: 2:11 - loss: 88.8837 - accuracy: 0.2530 - f1: 0.48 - ETA: 2:06 - loss: 88.7797 - accuracy: 0.2532 - f1: 0.48 - ETA: 2:02 - loss: 88.7404 - accuracy: 0.2531 - f1: 0.48 - ETA: 1:58 - loss: 88.7136 - accuracy: 0.2530 - f1: 0.48 - ETA: 1:53 - loss: 88.6572 - accuracy: 0.2531 - f1: 0.48 - ETA: 1:49 - loss: 88.7181 - accuracy: 0.2528 - f1: 0.48 - ETA: 1:44 - loss: 88.7401 - accuracy: 0.2523 - f1: 0.48 - ETA: 1:40 - loss: 88.7170 - accuracy: 0.2519 - f1: 0.48 - ETA: 1:36 - loss: 88.6735 - accuracy: 0.2518 - f1: 0.48 - ETA: 1:31 - loss: 88.6582 - accuracy: 0.2510 - f1: 0.48 - ETA: 1:27 - loss: 88.5623 - accuracy: 0.2510 - f1: 0.48 - ETA: 1:23 - loss: 88.5218 - accuracy: 0.2508 - f1: 0.48 - ETA: 1:18 - loss: 88.5242 - accuracy: 0.2506 - f1: 0.48 - ETA: 1:14 - loss: 88.4781 - accuracy: 0.2506 - f1: 0.48 - ETA: 1:09 - loss: 88.4430 - accuracy: 0.2511 - f1: 0.48 - ETA: 1:05 - loss: 88.4404 - accuracy: 0.2508 - f1: 0.48 - ETA: 1:01 - loss: 88.4097 - accuracy: 0.2506 - f1: 0.48 - ETA: 56s - loss: 88.3843 - accuracy: 0.2503 - f1: 0.4865 - ETA: 52s - loss: 88.3486 - accuracy: 0.2500 - f1: 0.487 - ETA: 47s - loss: 88.3546 - accuracy: 0.2497 - f1: 0.487 - ETA: 43s - loss: 88.3372 - accuracy: 0.2498 - f1: 0.487 - ETA: 39s - loss: 88.3361 - accuracy: 0.2498 - f1: 0.487 - ETA: 34s - loss: 88.2661 - accuracy: 0.2503 - f1: 0.487 - ETA: 30s - loss: 88.2353 - accuracy: 0.2504 - f1: 0.487 - ETA: 25s - loss: 88.2004 - accuracy: 0.2503 - f1: 0.487 - ETA: 21s - loss: 88.2058 - accuracy: 0.2503 - f1: 0.487 - ETA: 17s - loss: 88.2476 - accuracy: 0.2504 - f1: 0.487 - ETA: 12s - loss: 88.2459 - accuracy: 0.2500 - f1: 0.487 - ETA: 8s - loss: 88.2277 - accuracy: 0.2496 - f1: 0.487 - ETA: 3s - loss: 88.1554 - accuracy: 0.2496 - f1: 0.48 - 1058s 152ms/sample - loss: 88.1471 - accuracy: 0.2494 - f1: 0.4874 - val_loss: 74.0338 - val_accuracy: 0.2203 - val_f1: 0.5165\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/6940 [==================>...........] - ETA: 15:47 - loss: 76.7117 - accuracy: 0.3438 - f1: 0.571 - ETA: 15:43 - loss: 80.3439 - accuracy: 0.2500 - f1: 0.551 - ETA: 15:38 - loss: 79.1343 - accuracy: 0.2708 - f1: 0.568 - ETA: 15:34 - loss: 78.3850 - accuracy: 0.2891 - f1: 0.576 - ETA: 15:29 - loss: 78.2343 - accuracy: 0.2750 - f1: 0.568 - ETA: 15:25 - loss: 79.5030 - accuracy: 0.2552 - f1: 0.556 - ETA: 15:21 - loss: 79.7016 - accuracy: 0.2634 - f1: 0.540 - ETA: 15:16 - loss: 79.1469 - accuracy: 0.2734 - f1: 0.538 - ETA: 15:12 - loss: 80.1168 - accuracy: 0.2743 - f1: 0.533 - ETA: 15:07 - loss: 81.3960 - accuracy: 0.2656 - f1: 0.527 - ETA: 15:03 - loss: 81.2329 - accuracy: 0.2699 - f1: 0.523 - ETA: 14:59 - loss: 82.0108 - accuracy: 0.2734 - f1: 0.523 - ETA: 14:54 - loss: 82.5675 - accuracy: 0.2740 - f1: 0.510 - ETA: 14:50 - loss: 81.6582 - accuracy: 0.2746 - f1: 0.503 - ETA: 14:45 - loss: 81.8314 - accuracy: 0.2708 - f1: 0.504 - ETA: 14:41 - loss: 81.3449 - accuracy: 0.2637 - f1: 0.508 - ETA: 14:37 - loss: 81.2000 - accuracy: 0.2702 - f1: 0.510 - ETA: 14:32 - loss: 81.0676 - accuracy: 0.2691 - f1: 0.510 - ETA: 14:28 - loss: 81.9882 - accuracy: 0.2648 - f1: 0.510 - ETA: 14:23 - loss: 81.3924 - accuracy: 0.2672 - f1: 0.511 - ETA: 14:19 - loss: 81.6066 - accuracy: 0.2693 - f1: 0.515 - ETA: 14:15 - loss: 81.2679 - accuracy: 0.2713 - f1: 0.515 - ETA: 14:10 - loss: 80.8670 - accuracy: 0.2731 - f1: 0.517 - ETA: 14:06 - loss: 80.6738 - accuracy: 0.2695 - f1: 0.516 - ETA: 14:02 - loss: 80.3886 - accuracy: 0.2713 - f1: 0.518 - ETA: 13:57 - loss: 80.3347 - accuracy: 0.2668 - f1: 0.516 - ETA: 13:53 - loss: 80.5630 - accuracy: 0.2662 - f1: 0.514 - ETA: 13:48 - loss: 80.6371 - accuracy: 0.2645 - f1: 0.517 - ETA: 13:44 - loss: 80.7205 - accuracy: 0.2694 - f1: 0.519 - ETA: 13:40 - loss: 80.7880 - accuracy: 0.2698 - f1: 0.519 - ETA: 13:35 - loss: 80.6554 - accuracy: 0.2692 - f1: 0.518 - ETA: 13:31 - loss: 80.7673 - accuracy: 0.2676 - f1: 0.518 - ETA: 13:26 - loss: 81.2734 - accuracy: 0.2642 - f1: 0.517 - ETA: 13:22 - loss: 81.1700 - accuracy: 0.2619 - f1: 0.519 - ETA: 13:18 - loss: 81.4825 - accuracy: 0.2580 - f1: 0.516 - ETA: 13:13 - loss: 81.5382 - accuracy: 0.2569 - f1: 0.514 - ETA: 13:09 - loss: 81.7946 - accuracy: 0.2551 - f1: 0.512 - ETA: 13:04 - loss: 81.5751 - accuracy: 0.2558 - f1: 0.516 - ETA: 13:00 - loss: 81.7992 - accuracy: 0.2540 - f1: 0.513 - ETA: 12:56 - loss: 81.8240 - accuracy: 0.2539 - f1: 0.514 - ETA: 12:51 - loss: 81.7142 - accuracy: 0.2553 - f1: 0.516 - ETA: 12:47 - loss: 81.6621 - accuracy: 0.2560 - f1: 0.517 - ETA: 12:42 - loss: 81.7532 - accuracy: 0.2580 - f1: 0.518 - ETA: 12:38 - loss: 82.1265 - accuracy: 0.2528 - f1: 0.515 - ETA: 12:34 - loss: 82.1339 - accuracy: 0.2521 - f1: 0.515 - ETA: 12:29 - loss: 82.0784 - accuracy: 0.2527 - f1: 0.516 - ETA: 12:25 - loss: 82.5777 - accuracy: 0.2507 - f1: 0.514 - ETA: 12:21 - loss: 82.5557 - accuracy: 0.2500 - f1: 0.512 - ETA: 12:16 - loss: 82.6211 - accuracy: 0.2487 - f1: 0.510 - ETA: 12:12 - loss: 82.5545 - accuracy: 0.2488 - f1: 0.508 - ETA: 12:07 - loss: 82.7020 - accuracy: 0.2469 - f1: 0.508 - ETA: 12:03 - loss: 82.8756 - accuracy: 0.2470 - f1: 0.508 - ETA: 11:59 - loss: 82.9674 - accuracy: 0.2482 - f1: 0.508 - ETA: 11:54 - loss: 82.9799 - accuracy: 0.2506 - f1: 0.507 - ETA: 11:50 - loss: 83.3123 - accuracy: 0.2472 - f1: 0.504 - ETA: 11:45 - loss: 83.2265 - accuracy: 0.2461 - f1: 0.502 - ETA: 11:41 - loss: 83.4420 - accuracy: 0.2456 - f1: 0.500 - ETA: 11:37 - loss: 83.3375 - accuracy: 0.2473 - f1: 0.500 - ETA: 11:32 - loss: 83.2891 - accuracy: 0.2479 - f1: 0.499 - ETA: 11:28 - loss: 83.1962 - accuracy: 0.2484 - f1: 0.498 - ETA: 11:23 - loss: 83.1523 - accuracy: 0.2495 - f1: 0.499 - ETA: 11:19 - loss: 83.1618 - accuracy: 0.2495 - f1: 0.497 - ETA: 11:15 - loss: 83.1533 - accuracy: 0.2490 - f1: 0.498 - ETA: 11:10 - loss: 83.1126 - accuracy: 0.2485 - f1: 0.499 - ETA: 11:06 - loss: 83.3468 - accuracy: 0.2481 - f1: 0.498 - ETA: 11:02 - loss: 83.5832 - accuracy: 0.2481 - f1: 0.498 - ETA: 10:57 - loss: 83.5258 - accuracy: 0.2486 - f1: 0.498 - ETA: 10:53 - loss: 83.5298 - accuracy: 0.2482 - f1: 0.498 - ETA: 10:48 - loss: 83.4258 - accuracy: 0.2491 - f1: 0.497 - ETA: 10:44 - loss: 83.5602 - accuracy: 0.2491 - f1: 0.496 - ETA: 10:40 - loss: 83.6149 - accuracy: 0.2478 - f1: 0.495 - ETA: 10:35 - loss: 83.8418 - accuracy: 0.2452 - f1: 0.493 - ETA: 10:31 - loss: 83.9746 - accuracy: 0.2457 - f1: 0.493 - ETA: 10:26 - loss: 83.9467 - accuracy: 0.2458 - f1: 0.493 - ETA: 10:22 - loss: 83.9729 - accuracy: 0.2454 - f1: 0.493 - ETA: 10:18 - loss: 84.0323 - accuracy: 0.2438 - f1: 0.491 - ETA: 10:13 - loss: 84.1278 - accuracy: 0.2431 - f1: 0.491 - ETA: 10:09 - loss: 84.1149 - accuracy: 0.2420 - f1: 0.490 - ETA: 10:05 - loss: 84.1442 - accuracy: 0.2401 - f1: 0.489 - ETA: 10:00 - loss: 84.0284 - accuracy: 0.2414 - f1: 0.490 - ETA: 9:56 - loss: 84.4110 - accuracy: 0.2400 - f1: 0.488 - ETA: 9:51 - loss: 84.4368 - accuracy: 0.2401 - f1: 0.48 - ETA: 9:47 - loss: 84.3572 - accuracy: 0.2406 - f1: 0.48 - ETA: 9:43 - loss: 84.2588 - accuracy: 0.2392 - f1: 0.48 - ETA: 9:38 - loss: 84.1674 - accuracy: 0.2393 - f1: 0.48 - ETA: 9:34 - loss: 84.1741 - accuracy: 0.2369 - f1: 0.48 - ETA: 9:29 - loss: 84.0768 - accuracy: 0.2381 - f1: 0.48 - ETA: 9:25 - loss: 83.9823 - accuracy: 0.2390 - f1: 0.48 - ETA: 9:21 - loss: 83.9865 - accuracy: 0.2377 - f1: 0.49 - ETA: 9:16 - loss: 83.9362 - accuracy: 0.2385 - f1: 0.49 - ETA: 9:12 - loss: 83.9112 - accuracy: 0.2387 - f1: 0.49 - ETA: 9:07 - loss: 83.8980 - accuracy: 0.2388 - f1: 0.49 - ETA: 9:03 - loss: 84.0180 - accuracy: 0.2372 - f1: 0.49 - ETA: 8:59 - loss: 84.0167 - accuracy: 0.2367 - f1: 0.49 - ETA: 8:54 - loss: 83.9114 - accuracy: 0.2365 - f1: 0.49 - ETA: 8:50 - loss: 83.8702 - accuracy: 0.2367 - f1: 0.49 - ETA: 8:46 - loss: 83.8772 - accuracy: 0.2361 - f1: 0.48 - ETA: 8:41 - loss: 83.7475 - accuracy: 0.2366 - f1: 0.49 - ETA: 8:37 - loss: 83.6509 - accuracy: 0.2371 - f1: 0.49 - ETA: 8:32 - loss: 83.8342 - accuracy: 0.2359 - f1: 0.49 - ETA: 8:28 - loss: 83.8064 - accuracy: 0.2367 - f1: 0.49 - ETA: 8:24 - loss: 83.8161 - accuracy: 0.2362 - f1: 0.49 - ETA: 8:19 - loss: 83.8615 - accuracy: 0.2357 - f1: 0.49 - ETA: 8:15 - loss: 83.8144 - accuracy: 0.2353 - f1: 0.49 - ETA: 8:10 - loss: 83.8374 - accuracy: 0.2351 - f1: 0.49 - ETA: 8:06 - loss: 83.8613 - accuracy: 0.2350 - f1: 0.49 - ETA: 8:02 - loss: 83.9651 - accuracy: 0.2342 - f1: 0.49 - ETA: 7:57 - loss: 83.9345 - accuracy: 0.2344 - f1: 0.49 - ETA: 7:53 - loss: 83.9004 - accuracy: 0.2351 - f1: 0.49 - ETA: 7:49 - loss: 83.8103 - accuracy: 0.2355 - f1: 0.49 - ETA: 7:44 - loss: 83.7979 - accuracy: 0.2359 - f1: 0.49 - ETA: 7:40 - loss: 83.7850 - accuracy: 0.2358 - f1: 0.49 - ETA: 7:35 - loss: 83.7954 - accuracy: 0.2359 - f1: 0.49 - ETA: 7:31 - loss: 83.8336 - accuracy: 0.2352 - f1: 0.49 - ETA: 7:27 - loss: 83.9074 - accuracy: 0.2342 - f1: 0.49 - ETA: 7:22 - loss: 83.8264 - accuracy: 0.2355 - f1: 0.49 - ETA: 7:18 - loss: 83.8058 - accuracy: 0.2358 - f1: 0.49 - ETA: 7:13 - loss: 83.8135 - accuracy: 0.2354 - f1: 0.49 - ETA: 7:09 - loss: 83.8266 - accuracy: 0.2358 - f1: 0.49 - ETA: 7:05 - loss: 83.8577 - accuracy: 0.2352 - f1: 0.49 - ETA: 7:00 - loss: 83.8904 - accuracy: 0.2348 - f1: 0.49 - ETA: 6:56 - loss: 83.9491 - accuracy: 0.2341 - f1: 0.49 - ETA: 6:51 - loss: 83.8917 - accuracy: 0.2337 - f1: 0.49 - ETA: 6:47 - loss: 83.8031 - accuracy: 0.2341 - f1: 0.49 - ETA: 6:43 - loss: 83.8324 - accuracy: 0.2335 - f1: 0.49 - ETA: 6:38 - loss: 83.8728 - accuracy: 0.2329 - f1: 0.49 - ETA: 6:34 - loss: 83.9465 - accuracy: 0.2320 - f1: 0.49 - ETA: 6:30 - loss: 84.0294 - accuracy: 0.2329 - f1: 0.49 - ETA: 6:25 - loss: 84.0034 - accuracy: 0.2333 - f1: 0.49 - ETA: 6:21 - loss: 83.9687 - accuracy: 0.2337 - f1: 0.49 - ETA: 6:16 - loss: 84.0276 - accuracy: 0.2323 - f1: 0.49 - ETA: 6:12 - loss: 84.0228 - accuracy: 0.2325 - f1: 0.49 - ETA: 6:08 - loss: 84.0846 - accuracy: 0.2331 - f1: 0.49 - ETA: 6:03 - loss: 84.0989 - accuracy: 0.2332 - f1: 0.49 - ETA: 5:59 - loss: 84.1587 - accuracy: 0.2331 - f1: 0.49 - ETA: 5:54 - loss: 84.1311 - accuracy: 0.2332 - f1: 0.49 - ETA: 5:50 - loss: 84.2043 - accuracy: 0.2327 - f1: 0.49 - ETA: 5:46 - loss: 84.2349 - accuracy: 0.2323 - f1: 0.4906"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 5:41 - loss: 84.3017 - accuracy: 0.2322 - f1: 0.49 - ETA: 5:37 - loss: 84.2690 - accuracy: 0.2328 - f1: 0.49 - ETA: 5:32 - loss: 84.3127 - accuracy: 0.2329 - f1: 0.49 - ETA: 5:28 - loss: 84.3392 - accuracy: 0.2335 - f1: 0.49 - ETA: 5:24 - loss: 84.3093 - accuracy: 0.2340 - f1: 0.49 - ETA: 5:19 - loss: 84.2918 - accuracy: 0.2350 - f1: 0.49 - ETA: 5:15 - loss: 84.3362 - accuracy: 0.2351 - f1: 0.48 - ETA: 5:11 - loss: 84.2823 - accuracy: 0.2357 - f1: 0.49 - ETA: 5:06 - loss: 84.3486 - accuracy: 0.2351 - f1: 0.48 - ETA: 5:02 - loss: 84.3491 - accuracy: 0.2352 - f1: 0.48 - ETA: 4:57 - loss: 84.4347 - accuracy: 0.2357 - f1: 0.48 - ETA: 4:53 - loss: 84.3933 - accuracy: 0.2358 - f1: 0.48 - ETA: 4:49 - loss: 84.3738 - accuracy: 0.2355 - f1: 0.48 - ETA: 4:44 - loss: 84.3653 - accuracy: 0.2354 - f1: 0.48 - ETA: 4:40 - loss: 84.3174 - accuracy: 0.2355 - f1: 0.48 - ETA: 4:35 - loss: 84.3699 - accuracy: 0.2354 - f1: 0.48 - ETA: 4:31 - loss: 84.3264 - accuracy: 0.2357 - f1: 0.48 - ETA: 4:27 - loss: 84.2855 - accuracy: 0.2354 - f1: 0.48 - ETA: 4:22 - loss: 84.2601 - accuracy: 0.2349 - f1: 0.48 - ETA: 4:18 - loss: 84.2540 - accuracy: 0.2346 - f1: 0.48 - ETA: 4:13 - loss: 84.3547 - accuracy: 0.2349 - f1: 0.48 - ETA: 4:09 - loss: 84.3483 - accuracy: 0.2357 - f1: 0.48 - ETA: 4:05 - loss: 84.3298 - accuracy: 0.2360 - f1: 0.48 - ETA: 4:00 - loss: 84.3349 - accuracy: 0.2361 - f1: 0.48 - ETA: 3:56 - loss: 84.2947 - accuracy: 0.2366 - f1: 0.48 - ETA: 3:52 - loss: 84.2864 - accuracy: 0.2367 - f1: 0.48 - ETA: 3:47 - loss: 84.3442 - accuracy: 0.2364 - f1: 0.48 - ETA: 3:43 - loss: 84.3320 - accuracy: 0.2361 - f1: 0.48 - ETA: 3:38 - loss: 84.3283 - accuracy: 0.2360 - f1: 0.48 - ETA: 3:34 - loss: 84.3248 - accuracy: 0.2359 - f1: 0.48 - ETA: 3:30 - loss: 84.3060 - accuracy: 0.2365 - f1: 0.48 - ETA: 3:25 - loss: 84.2998 - accuracy: 0.2362 - f1: 0.48 - ETA: 3:21 - loss: 84.2634 - accuracy: 0.2368 - f1: 0.48 - ETA: 3:16 - loss: 84.3182 - accuracy: 0.2366 - f1: 0.48 - ETA: 3:12 - loss: 84.3690 - accuracy: 0.2366 - f1: 0.48 - ETA: 3:08 - loss: 84.4091 - accuracy: 0.2364 - f1: 0.48 - ETA: 3:03 - loss: 84.3788 - accuracy: 0.2373 - f1: 0.48 - ETA: 2:59 - loss: 84.3521 - accuracy: 0.2374 - f1: 0.48 - ETA: 2:54 - loss: 84.4102 - accuracy: 0.2371 - f1: 0.48 - ETA: 2:50 - loss: 84.4014 - accuracy: 0.2372 - f1: 0.48 - ETA: 2:46 - loss: 84.3832 - accuracy: 0.2380 - f1: 0.48 - ETA: 2:41 - loss: 84.3775 - accuracy: 0.2384 - f1: 0.48 - ETA: 2:37 - loss: 84.3861 - accuracy: 0.2383 - f1: 0.48 - ETA: 2:33 - loss: 84.3966 - accuracy: 0.2383 - f1: 0.48 - ETA: 2:28 - loss: 84.4046 - accuracy: 0.2379 - f1: 0.48 - ETA: 2:24 - loss: 84.3890 - accuracy: 0.2381 - f1: 0.48 - ETA: 2:19 - loss: 84.3415 - accuracy: 0.2380 - f1: 0.48 - ETA: 2:15 - loss: 84.2911 - accuracy: 0.2384 - f1: 0.48 - ETA: 2:11 - loss: 84.2516 - accuracy: 0.2383 - f1: 0.48 - ETA: 2:06 - loss: 84.2629 - accuracy: 0.2379 - f1: 0.48 - ETA: 2:02 - loss: 84.2070 - accuracy: 0.2379 - f1: 0.48 - ETA: 1:57 - loss: 84.1927 - accuracy: 0.2383 - f1: 0.48 - ETA: 1:53 - loss: 84.1827 - accuracy: 0.2385 - f1: 0.49 - ETA: 1:49 - loss: 84.1663 - accuracy: 0.2381 - f1: 0.48 - ETA: 1:44 - loss: 84.1472 - accuracy: 0.2379 - f1: 0.48 - ETA: 1:40 - loss: 84.0989 - accuracy: 0.2371 - f1: 0.48 - ETA: 1:35 - loss: 84.0925 - accuracy: 0.2377 - f1: 0.48 - ETA: 1:31 - loss: 84.0520 - accuracy: 0.2380 - f1: 0.48 - ETA: 1:27 - loss: 84.0410 - accuracy: 0.2376 - f1: 0.49 - ETA: 1:22 - loss: 84.0626 - accuracy: 0.2378 - f1: 0.49 - ETA: 1:18 - loss: 84.0611 - accuracy: 0.2382 - f1: 0.48 - ETA: 1:14 - loss: 84.1062 - accuracy: 0.2378 - f1: 0.48 - ETA: 1:09 - loss: 84.0879 - accuracy: 0.2382 - f1: 0.48 - ETA: 1:05 - loss: 84.1218 - accuracy: 0.2381 - f1: 0.48 - ETA: 1:00 - loss: 84.1268 - accuracy: 0.2378 - f1: 0.48 - ETA: 56s - loss: 84.0481 - accuracy: 0.2379 - f1: 0.4891 - ETA: 52s - loss: 84.0400 - accuracy: 0.2380 - f1: 0.488 - ETA: 47s - loss: 84.0596 - accuracy: 0.2379 - f1: 0.489 - ETA: 43s - loss: 84.1144 - accuracy: 0.2381 - f1: 0.488 - ETA: 38s - loss: 84.1419 - accuracy: 0.2375 - f1: 0.488 - ETA: 34s - loss: 84.1334 - accuracy: 0.2377 - f1: 0.488 - ETA: 30s - loss: 84.1231 - accuracy: 0.2378 - f1: 0.488 - ETA: 25s - loss: 84.1723 - accuracy: 0.2374 - f1: 0.487 - ETA: 21s - loss: 84.1517 - accuracy: 0.2379 - f1: 0.487 - ETA: 17s - loss: 84.1436 - accuracy: 0.2387 - f1: 0.487 - ETA: 12s - loss: 84.1422 - accuracy: 0.2380 - f1: 0.487 - ETA: 8s - loss: 84.0977 - accuracy: 0.2382 - f1: 0.487 - ETA: 3s - loss: 84.0762 - accuracy: 0.2384 - f1: 0.48 - 1053s 152ms/sample - loss: 84.0651 - accuracy: 0.2382 - f1: 0.4872 - val_loss: 73.6235 - val_accuracy: 0.2203 - val_f1: 0.5185\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/6940 [==================>...........] - ETA: 15:47 - loss: 79.7257 - accuracy: 0.3750 - f1: 0.549 - ETA: 15:42 - loss: 80.1938 - accuracy: 0.2969 - f1: 0.508 - ETA: 15:38 - loss: 81.7850 - accuracy: 0.2708 - f1: 0.488 - ETA: 15:33 - loss: 80.1280 - accuracy: 0.2812 - f1: 0.500 - ETA: 15:29 - loss: 79.8698 - accuracy: 0.2688 - f1: 0.497 - ETA: 15:25 - loss: 80.4172 - accuracy: 0.2708 - f1: 0.501 - ETA: 15:20 - loss: 82.9647 - accuracy: 0.2500 - f1: 0.489 - ETA: 15:16 - loss: 81.2679 - accuracy: 0.2695 - f1: 0.487 - ETA: 15:12 - loss: 81.3683 - accuracy: 0.2569 - f1: 0.487 - ETA: 15:07 - loss: 80.1430 - accuracy: 0.2625 - f1: 0.494 - ETA: 15:03 - loss: 79.9325 - accuracy: 0.2699 - f1: 0.484 - ETA: 14:58 - loss: 80.4795 - accuracy: 0.2656 - f1: 0.479 - ETA: 14:54 - loss: 80.1916 - accuracy: 0.2620 - f1: 0.480 - ETA: 14:50 - loss: 80.5942 - accuracy: 0.2522 - f1: 0.479 - ETA: 14:45 - loss: 80.6285 - accuracy: 0.2479 - f1: 0.477 - ETA: 14:41 - loss: 80.4995 - accuracy: 0.2539 - f1: 0.479 - ETA: 14:36 - loss: 81.1386 - accuracy: 0.2610 - f1: 0.474 - ETA: 14:32 - loss: 80.9558 - accuracy: 0.2622 - f1: 0.469 - ETA: 14:28 - loss: 80.5178 - accuracy: 0.2648 - f1: 0.468 - ETA: 14:23 - loss: 80.4426 - accuracy: 0.2641 - f1: 0.464 - ETA: 14:19 - loss: 80.4168 - accuracy: 0.2634 - f1: 0.468 - ETA: 14:15 - loss: 80.4703 - accuracy: 0.2642 - f1: 0.470 - ETA: 14:10 - loss: 79.7689 - accuracy: 0.2677 - f1: 0.473 - ETA: 14:06 - loss: 79.7751 - accuracy: 0.2695 - f1: 0.472 - ETA: 14:01 - loss: 80.0011 - accuracy: 0.2700 - f1: 0.470 - ETA: 13:57 - loss: 80.2740 - accuracy: 0.2680 - f1: 0.471 - ETA: 13:53 - loss: 80.2615 - accuracy: 0.2697 - f1: 0.472 - ETA: 13:48 - loss: 80.2665 - accuracy: 0.2679 - f1: 0.472 - ETA: 13:44 - loss: 80.4296 - accuracy: 0.2640 - f1: 0.469 - ETA: 13:39 - loss: 80.4204 - accuracy: 0.2625 - f1: 0.464 - ETA: 13:35 - loss: 80.1360 - accuracy: 0.2641 - f1: 0.465 - ETA: 13:31 - loss: 80.0355 - accuracy: 0.2666 - f1: 0.469 - ETA: 13:26 - loss: 80.2684 - accuracy: 0.2670 - f1: 0.468 - ETA: 13:22 - loss: 80.6766 - accuracy: 0.2638 - f1: 0.467 - ETA: 13:17 - loss: 80.6136 - accuracy: 0.2625 - f1: 0.468 - ETA: 13:13 - loss: 80.6732 - accuracy: 0.2639 - f1: 0.465 - ETA: 13:09 - loss: 80.8368 - accuracy: 0.2627 - f1: 0.465 - ETA: 13:04 - loss: 80.5619 - accuracy: 0.2648 - f1: 0.466 - ETA: 13:00 - loss: 80.4872 - accuracy: 0.2652 - f1: 0.467 - ETA: 12:56 - loss: 80.5736 - accuracy: 0.2656 - f1: 0.469 - ETA: 12:51 - loss: 80.6676 - accuracy: 0.2622 - f1: 0.470 - ETA: 12:47 - loss: 81.0878 - accuracy: 0.2589 - f1: 0.466 - ETA: 12:42 - loss: 80.9772 - accuracy: 0.2587 - f1: 0.467 - ETA: 12:38 - loss: 80.9129 - accuracy: 0.2585 - f1: 0.468 - ETA: 12:34 - loss: 80.8337 - accuracy: 0.2604 - f1: 0.470 - ETA: 12:29 - loss: 80.8337 - accuracy: 0.2588 - f1: 0.470 - ETA: 12:25 - loss: 81.3776 - accuracy: 0.2573 - f1: 0.470 - ETA: 12:20 - loss: 81.4808 - accuracy: 0.2585 - f1: 0.469 - ETA: 12:16 - loss: 81.7262 - accuracy: 0.2570 - f1: 0.466 - ETA: 12:12 - loss: 81.7151 - accuracy: 0.2562 - f1: 0.465 - ETA: 12:07 - loss: 81.7436 - accuracy: 0.2555 - f1: 0.465 - ETA: 12:03 - loss: 81.8716 - accuracy: 0.2566 - f1: 0.466 - ETA: 11:59 - loss: 81.9576 - accuracy: 0.2547 - f1: 0.466 - ETA: 11:54 - loss: 81.9160 - accuracy: 0.2541 - f1: 0.466 - ETA: 11:50 - loss: 81.8556 - accuracy: 0.2528 - f1: 0.467 - ETA: 11:45 - loss: 81.9284 - accuracy: 0.2522 - f1: 0.465 - ETA: 11:41 - loss: 81.8268 - accuracy: 0.2533 - f1: 0.465 - ETA: 11:37 - loss: 81.7498 - accuracy: 0.2522 - f1: 0.466 - ETA: 11:32 - loss: 81.8812 - accuracy: 0.2505 - f1: 0.467 - ETA: 11:28 - loss: 81.9415 - accuracy: 0.2495 - f1: 0.467 - ETA: 11:23 - loss: 81.9968 - accuracy: 0.2490 - f1: 0.467 - ETA: 11:19 - loss: 81.9123 - accuracy: 0.2510 - f1: 0.469 - ETA: 11:15 - loss: 81.8058 - accuracy: 0.2505 - f1: 0.470 - ETA: 11:10 - loss: 81.6742 - accuracy: 0.2515 - f1: 0.472 - ETA: 11:06 - loss: 81.6878 - accuracy: 0.2510 - f1: 0.472 - ETA: 11:02 - loss: 81.7071 - accuracy: 0.2509 - f1: 0.472 - ETA: 10:57 - loss: 81.7671 - accuracy: 0.2514 - f1: 0.470 - ETA: 10:53 - loss: 81.8675 - accuracy: 0.2514 - f1: 0.470 - ETA: 10:48 - loss: 82.2333 - accuracy: 0.2495 - f1: 0.469 - ETA: 10:44 - loss: 82.2370 - accuracy: 0.2500 - f1: 0.471 - ETA: 10:40 - loss: 82.1452 - accuracy: 0.2522 - f1: 0.473 - ETA: 10:35 - loss: 82.0751 - accuracy: 0.2509 - f1: 0.473 - ETA: 10:31 - loss: 82.0961 - accuracy: 0.2500 - f1: 0.473 - ETA: 10:26 - loss: 82.2059 - accuracy: 0.2496 - f1: 0.473 - ETA: 10:22 - loss: 82.3469 - accuracy: 0.2483 - f1: 0.471 - ETA: 10:18 - loss: 82.3841 - accuracy: 0.2484 - f1: 0.472 - ETA: 10:13 - loss: 82.2720 - accuracy: 0.2492 - f1: 0.472 - ETA: 10:09 - loss: 82.3440 - accuracy: 0.2468 - f1: 0.473 - ETA: 10:04 - loss: 82.3678 - accuracy: 0.2456 - f1: 0.473 - ETA: 10:00 - loss: 82.4070 - accuracy: 0.2453 - f1: 0.473 - ETA: 9:56 - loss: 82.5273 - accuracy: 0.2442 - f1: 0.473 - ETA: 9:51 - loss: 82.4421 - accuracy: 0.2450 - f1: 0.47 - ETA: 9:47 - loss: 82.4018 - accuracy: 0.2447 - f1: 0.47 - ETA: 9:43 - loss: 82.5088 - accuracy: 0.2452 - f1: 0.47 - ETA: 9:38 - loss: 82.6193 - accuracy: 0.2449 - f1: 0.47 - ETA: 9:34 - loss: 82.5931 - accuracy: 0.2456 - f1: 0.47 - ETA: 9:29 - loss: 82.6190 - accuracy: 0.2453 - f1: 0.47 - ETA: 9:25 - loss: 82.8740 - accuracy: 0.2440 - f1: 0.47 - ETA: 9:21 - loss: 82.8682 - accuracy: 0.2444 - f1: 0.47 - ETA: 9:16 - loss: 82.7784 - accuracy: 0.2455 - f1: 0.47 - ETA: 9:12 - loss: 82.8032 - accuracy: 0.2442 - f1: 0.47 - ETA: 9:07 - loss: 82.7678 - accuracy: 0.2432 - f1: 0.47 - ETA: 9:03 - loss: 82.7768 - accuracy: 0.2429 - f1: 0.47 - ETA: 8:59 - loss: 82.7738 - accuracy: 0.2430 - f1: 0.47 - ETA: 8:54 - loss: 82.8022 - accuracy: 0.2431 - f1: 0.47 - ETA: 8:50 - loss: 82.7912 - accuracy: 0.2432 - f1: 0.47 - ETA: 8:46 - loss: 82.7810 - accuracy: 0.2432 - f1: 0.47 - ETA: 8:41 - loss: 82.7702 - accuracy: 0.2443 - f1: 0.47 - ETA: 8:37 - loss: 82.8145 - accuracy: 0.2440 - f1: 0.47 - ETA: 8:32 - loss: 82.8316 - accuracy: 0.2441 - f1: 0.47 - ETA: 8:28 - loss: 82.9501 - accuracy: 0.2426 - f1: 0.47 - ETA: 8:24 - loss: 82.8841 - accuracy: 0.2436 - f1: 0.47 - ETA: 8:19 - loss: 82.9030 - accuracy: 0.2436 - f1: 0.47 - ETA: 8:15 - loss: 83.0145 - accuracy: 0.2440 - f1: 0.47 - ETA: 8:10 - loss: 82.9926 - accuracy: 0.2438 - f1: 0.47 - ETA: 8:06 - loss: 83.0518 - accuracy: 0.2423 - f1: 0.47 - ETA: 8:02 - loss: 83.1917 - accuracy: 0.2418 - f1: 0.47 - ETA: 7:57 - loss: 83.2487 - accuracy: 0.2422 - f1: 0.47 - ETA: 7:53 - loss: 83.2137 - accuracy: 0.2420 - f1: 0.47 - ETA: 7:48 - loss: 83.3014 - accuracy: 0.2426 - f1: 0.47 - ETA: 7:44 - loss: 83.2532 - accuracy: 0.2435 - f1: 0.47 - ETA: 7:40 - loss: 83.2540 - accuracy: 0.2433 - f1: 0.47 - ETA: 7:35 - loss: 83.1772 - accuracy: 0.2442 - f1: 0.47 - ETA: 7:31 - loss: 83.0928 - accuracy: 0.2453 - f1: 0.48 - ETA: 7:27 - loss: 82.9367 - accuracy: 0.2459 - f1: 0.48 - ETA: 7:22 - loss: 82.9527 - accuracy: 0.2454 - f1: 0.48 - ETA: 7:18 - loss: 83.0566 - accuracy: 0.2444 - f1: 0.48 - ETA: 7:13 - loss: 83.0266 - accuracy: 0.2442 - f1: 0.48 - ETA: 7:09 - loss: 82.9566 - accuracy: 0.2437 - f1: 0.48 - ETA: 7:05 - loss: 83.0000 - accuracy: 0.2438 - f1: 0.48 - ETA: 7:00 - loss: 83.0581 - accuracy: 0.2428 - f1: 0.48 - ETA: 6:56 - loss: 83.0161 - accuracy: 0.2431 - f1: 0.48 - ETA: 6:51 - loss: 83.0065 - accuracy: 0.2436 - f1: 0.48 - ETA: 6:47 - loss: 83.0149 - accuracy: 0.2445 - f1: 0.48 - ETA: 6:43 - loss: 83.0538 - accuracy: 0.2438 - f1: 0.48 - ETA: 6:38 - loss: 83.1742 - accuracy: 0.2428 - f1: 0.48 - ETA: 6:34 - loss: 83.0712 - accuracy: 0.2431 - f1: 0.48 - ETA: 6:30 - loss: 83.0090 - accuracy: 0.2427 - f1: 0.48 - ETA: 6:25 - loss: 82.9448 - accuracy: 0.2432 - f1: 0.48 - ETA: 6:21 - loss: 82.9648 - accuracy: 0.2440 - f1: 0.48 - ETA: 6:16 - loss: 82.9061 - accuracy: 0.2455 - f1: 0.48 - ETA: 6:12 - loss: 82.9001 - accuracy: 0.2448 - f1: 0.48 - ETA: 6:08 - loss: 82.8746 - accuracy: 0.2444 - f1: 0.48 - ETA: 6:03 - loss: 82.8764 - accuracy: 0.2449 - f1: 0.48 - ETA: 5:59 - loss: 82.9409 - accuracy: 0.2442 - f1: 0.48 - ETA: 5:54 - loss: 82.8965 - accuracy: 0.2440 - f1: 0.48 - ETA: 5:50 - loss: 82.9331 - accuracy: 0.2436 - f1: 0.48 - ETA: 5:46 - loss: 82.9391 - accuracy: 0.2437 - f1: 0.4842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 5:41 - loss: 82.9369 - accuracy: 0.2439 - f1: 0.48 - ETA: 5:37 - loss: 82.9854 - accuracy: 0.2429 - f1: 0.48 - ETA: 5:32 - loss: 82.9992 - accuracy: 0.2434 - f1: 0.48 - ETA: 5:28 - loss: 83.0550 - accuracy: 0.2438 - f1: 0.48 - ETA: 5:24 - loss: 83.0720 - accuracy: 0.2434 - f1: 0.48 - ETA: 5:19 - loss: 83.1322 - accuracy: 0.2431 - f1: 0.48 - ETA: 5:15 - loss: 83.1365 - accuracy: 0.2435 - f1: 0.48 - ETA: 5:11 - loss: 83.1680 - accuracy: 0.2436 - f1: 0.48 - ETA: 5:06 - loss: 83.1026 - accuracy: 0.2443 - f1: 0.48 - ETA: 5:02 - loss: 83.0872 - accuracy: 0.2441 - f1: 0.48 - ETA: 4:57 - loss: 83.1196 - accuracy: 0.2435 - f1: 0.48 - ETA: 4:53 - loss: 83.1935 - accuracy: 0.2431 - f1: 0.48 - ETA: 4:49 - loss: 83.1798 - accuracy: 0.2440 - f1: 0.48 - ETA: 4:44 - loss: 83.2264 - accuracy: 0.2430 - f1: 0.48 - ETA: 4:40 - loss: 83.1707 - accuracy: 0.2433 - f1: 0.48 - ETA: 4:35 - loss: 83.2419 - accuracy: 0.2425 - f1: 0.48 - ETA: 4:31 - loss: 83.3197 - accuracy: 0.2419 - f1: 0.48 - ETA: 4:27 - loss: 83.3142 - accuracy: 0.2418 - f1: 0.48 - ETA: 4:22 - loss: 83.3042 - accuracy: 0.2410 - f1: 0.48 - ETA: 4:18 - loss: 83.2827 - accuracy: 0.2407 - f1: 0.48 - ETA: 4:13 - loss: 83.2561 - accuracy: 0.2410 - f1: 0.48 - ETA: 4:09 - loss: 83.2420 - accuracy: 0.2412 - f1: 0.48 - ETA: 4:05 - loss: 83.2041 - accuracy: 0.2417 - f1: 0.48 - ETA: 4:00 - loss: 83.2396 - accuracy: 0.2421 - f1: 0.48 - ETA: 3:56 - loss: 83.2257 - accuracy: 0.2418 - f1: 0.48 - ETA: 3:52 - loss: 83.2561 - accuracy: 0.2416 - f1: 0.48 - ETA: 3:47 - loss: 83.2953 - accuracy: 0.2419 - f1: 0.48 - ETA: 3:43 - loss: 83.2928 - accuracy: 0.2428 - f1: 0.48 - ETA: 3:38 - loss: 83.2852 - accuracy: 0.2433 - f1: 0.48 - ETA: 3:34 - loss: 83.2610 - accuracy: 0.2431 - f1: 0.48 - ETA: 3:30 - loss: 83.2579 - accuracy: 0.2428 - f1: 0.48 - ETA: 3:25 - loss: 83.2327 - accuracy: 0.2438 - f1: 0.48 - ETA: 3:21 - loss: 83.2176 - accuracy: 0.2434 - f1: 0.48 - ETA: 3:16 - loss: 83.2752 - accuracy: 0.2427 - f1: 0.48 - ETA: 3:12 - loss: 83.2429 - accuracy: 0.2426 - f1: 0.48 - ETA: 3:08 - loss: 83.2489 - accuracy: 0.2425 - f1: 0.48 - ETA: 3:03 - loss: 83.2733 - accuracy: 0.2423 - f1: 0.48 - ETA: 2:59 - loss: 83.2590 - accuracy: 0.2429 - f1: 0.48 - ETA: 2:54 - loss: 83.2455 - accuracy: 0.2436 - f1: 0.48 - ETA: 2:50 - loss: 83.2172 - accuracy: 0.2440 - f1: 0.48 - ETA: 2:46 - loss: 83.3011 - accuracy: 0.2435 - f1: 0.48 - ETA: 2:41 - loss: 83.4029 - accuracy: 0.2431 - f1: 0.48 - ETA: 2:37 - loss: 83.3885 - accuracy: 0.2427 - f1: 0.48 - ETA: 2:33 - loss: 83.4057 - accuracy: 0.2426 - f1: 0.48 - ETA: 2:28 - loss: 83.4210 - accuracy: 0.2425 - f1: 0.48 - ETA: 2:24 - loss: 83.4066 - accuracy: 0.2425 - f1: 0.48 - ETA: 2:19 - loss: 83.4968 - accuracy: 0.2419 - f1: 0.48 - ETA: 2:15 - loss: 83.5489 - accuracy: 0.2419 - f1: 0.48 - ETA: 2:11 - loss: 83.5137 - accuracy: 0.2421 - f1: 0.48 - ETA: 2:06 - loss: 83.5231 - accuracy: 0.2419 - f1: 0.48 - ETA: 2:02 - loss: 83.5812 - accuracy: 0.2414 - f1: 0.48 - ETA: 1:57 - loss: 83.5569 - accuracy: 0.2418 - f1: 0.48 - ETA: 1:53 - loss: 83.5644 - accuracy: 0.2425 - f1: 0.48 - ETA: 1:49 - loss: 83.5699 - accuracy: 0.2419 - f1: 0.47 - ETA: 1:44 - loss: 83.5578 - accuracy: 0.2416 - f1: 0.47 - ETA: 1:40 - loss: 83.5780 - accuracy: 0.2418 - f1: 0.47 - ETA: 1:36 - loss: 83.5857 - accuracy: 0.2417 - f1: 0.47 - ETA: 1:31 - loss: 83.5800 - accuracy: 0.2419 - f1: 0.47 - ETA: 1:27 - loss: 83.5507 - accuracy: 0.2418 - f1: 0.47 - ETA: 1:22 - loss: 83.5998 - accuracy: 0.2418 - f1: 0.47 - ETA: 1:18 - loss: 83.5854 - accuracy: 0.2420 - f1: 0.47 - ETA: 1:14 - loss: 83.5858 - accuracy: 0.2422 - f1: 0.47 - ETA: 1:09 - loss: 83.5666 - accuracy: 0.2424 - f1: 0.47 - ETA: 1:05 - loss: 83.5523 - accuracy: 0.2424 - f1: 0.47 - ETA: 1:00 - loss: 83.6157 - accuracy: 0.2421 - f1: 0.48 - ETA: 56s - loss: 83.6169 - accuracy: 0.2422 - f1: 0.4798 - ETA: 52s - loss: 83.6512 - accuracy: 0.2418 - f1: 0.479 - ETA: 47s - loss: 83.6430 - accuracy: 0.2414 - f1: 0.479 - ETA: 43s - loss: 83.6304 - accuracy: 0.2414 - f1: 0.479 - ETA: 38s - loss: 83.6038 - accuracy: 0.2413 - f1: 0.479 - ETA: 34s - loss: 83.6358 - accuracy: 0.2410 - f1: 0.479 - ETA: 30s - loss: 83.6886 - accuracy: 0.2406 - f1: 0.479 - ETA: 25s - loss: 83.6902 - accuracy: 0.2402 - f1: 0.479 - ETA: 21s - loss: 83.7092 - accuracy: 0.2401 - f1: 0.479 - ETA: 17s - loss: 83.7176 - accuracy: 0.2393 - f1: 0.479 - ETA: 12s - loss: 83.6943 - accuracy: 0.2395 - f1: 0.478 - ETA: 8s - loss: 83.7013 - accuracy: 0.2395 - f1: 0.479 - ETA: 3s - loss: 83.7473 - accuracy: 0.2391 - f1: 0.47 - 1051s 151ms/sample - loss: 83.7032 - accuracy: 0.2390 - f1: 0.4793 - val_loss: 72.9433 - val_accuracy: 0.2215 - val_f1: 0.5444\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/6940 [==================>...........] - ETA: 15:28 - loss: 84.3201 - accuracy: 0.2188 - f1: 0.588 - ETA: 15:23 - loss: 79.2846 - accuracy: 0.2812 - f1: 0.598 - ETA: 15:18 - loss: 79.4567 - accuracy: 0.3021 - f1: 0.559 - ETA: 15:13 - loss: 78.4653 - accuracy: 0.3438 - f1: 0.555 - ETA: 15:09 - loss: 80.7268 - accuracy: 0.3187 - f1: 0.547 - ETA: 15:05 - loss: 81.7523 - accuracy: 0.3125 - f1: 0.538 - ETA: 15:01 - loss: 81.6632 - accuracy: 0.3080 - f1: 0.529 - ETA: 14:57 - loss: 81.7454 - accuracy: 0.3047 - f1: 0.516 - ETA: 14:53 - loss: 82.7487 - accuracy: 0.2951 - f1: 0.512 - ETA: 14:48 - loss: 83.0026 - accuracy: 0.2750 - f1: 0.505 - ETA: 14:44 - loss: 82.6726 - accuracy: 0.2812 - f1: 0.503 - ETA: 14:40 - loss: 82.1293 - accuracy: 0.2812 - f1: 0.506 - ETA: 14:38 - loss: 82.0234 - accuracy: 0.2812 - f1: 0.507 - ETA: 14:33 - loss: 82.0026 - accuracy: 0.2701 - f1: 0.499 - ETA: 14:29 - loss: 81.9775 - accuracy: 0.2646 - f1: 0.495 - ETA: 14:24 - loss: 82.2269 - accuracy: 0.2617 - f1: 0.490 - ETA: 14:20 - loss: 82.1313 - accuracy: 0.2629 - f1: 0.487 - ETA: 14:15 - loss: 81.9572 - accuracy: 0.2622 - f1: 0.487 - ETA: 14:11 - loss: 82.4928 - accuracy: 0.2632 - f1: 0.486 - ETA: 14:07 - loss: 82.5665 - accuracy: 0.2656 - f1: 0.487 - ETA: 14:02 - loss: 82.0135 - accuracy: 0.2649 - f1: 0.490 - ETA: 13:58 - loss: 82.0606 - accuracy: 0.2642 - f1: 0.487 - ETA: 13:54 - loss: 82.6408 - accuracy: 0.2568 - f1: 0.480 - ETA: 13:50 - loss: 82.6941 - accuracy: 0.2591 - f1: 0.479 - ETA: 13:45 - loss: 82.9730 - accuracy: 0.2575 - f1: 0.480 - ETA: 13:41 - loss: 83.2562 - accuracy: 0.2536 - f1: 0.480 - ETA: 13:36 - loss: 83.2675 - accuracy: 0.2523 - f1: 0.481 - ETA: 13:32 - loss: 83.3611 - accuracy: 0.2522 - f1: 0.479 - ETA: 13:28 - loss: 83.0984 - accuracy: 0.2532 - f1: 0.481 - ETA: 13:24 - loss: 82.8575 - accuracy: 0.2521 - f1: 0.481 - ETA: 13:19 - loss: 83.1195 - accuracy: 0.2500 - f1: 0.477 - ETA: 13:15 - loss: 82.9811 - accuracy: 0.2510 - f1: 0.477 - ETA: 13:11 - loss: 83.0352 - accuracy: 0.2528 - f1: 0.479 - ETA: 13:06 - loss: 83.1404 - accuracy: 0.2500 - f1: 0.479 - ETA: 13:02 - loss: 83.1651 - accuracy: 0.2473 - f1: 0.479 - ETA: 12:58 - loss: 83.2928 - accuracy: 0.2483 - f1: 0.475 - ETA: 12:54 - loss: 83.5464 - accuracy: 0.2483 - f1: 0.470 - ETA: 12:49 - loss: 83.6823 - accuracy: 0.2492 - f1: 0.468 - ETA: 12:45 - loss: 83.6563 - accuracy: 0.2476 - f1: 0.470 - ETA: 12:40 - loss: 83.4234 - accuracy: 0.2500 - f1: 0.472 - ETA: 12:35 - loss: 83.2296 - accuracy: 0.2508 - f1: 0.475 - ETA: 12:30 - loss: 83.0592 - accuracy: 0.2515 - f1: 0.478 - ETA: 12:26 - loss: 82.9770 - accuracy: 0.2500 - f1: 0.476 - ETA: 12:21 - loss: 82.8433 - accuracy: 0.2507 - f1: 0.477 - ETA: 12:16 - loss: 83.0418 - accuracy: 0.2486 - f1: 0.475 - ETA: 12:12 - loss: 83.1059 - accuracy: 0.2514 - f1: 0.475 - ETA: 12:07 - loss: 83.0139 - accuracy: 0.2500 - f1: 0.475 - ETA: 12:02 - loss: 83.0397 - accuracy: 0.2513 - f1: 0.475 - ETA: 11:58 - loss: 82.8697 - accuracy: 0.2526 - f1: 0.476 - ETA: 11:53 - loss: 82.7831 - accuracy: 0.2525 - f1: 0.476 - ETA: 11:48 - loss: 82.7037 - accuracy: 0.2549 - f1: 0.477 - ETA: 11:44 - loss: 82.6508 - accuracy: 0.2530 - f1: 0.478 - ETA: 11:39 - loss: 82.8007 - accuracy: 0.2500 - f1: 0.478 - ETA: 11:35 - loss: 82.7362 - accuracy: 0.2488 - f1: 0.478 - ETA: 11:30 - loss: 82.5729 - accuracy: 0.2489 - f1: 0.480 - ETA: 11:25 - loss: 82.4808 - accuracy: 0.2506 - f1: 0.482 - ETA: 11:21 - loss: 82.6016 - accuracy: 0.2500 - f1: 0.482 - ETA: 11:16 - loss: 82.7384 - accuracy: 0.2500 - f1: 0.483 - ETA: 11:12 - loss: 82.7984 - accuracy: 0.2500 - f1: 0.483 - ETA: 11:08 - loss: 82.9651 - accuracy: 0.2484 - f1: 0.483 - ETA: 11:03 - loss: 82.9269 - accuracy: 0.2464 - f1: 0.484 - ETA: 10:59 - loss: 83.0151 - accuracy: 0.2455 - f1: 0.483 - ETA: 10:54 - loss: 82.7845 - accuracy: 0.2475 - f1: 0.484 - ETA: 10:50 - loss: 82.7731 - accuracy: 0.2471 - f1: 0.484 - ETA: 10:46 - loss: 82.8329 - accuracy: 0.2471 - f1: 0.484 - ETA: 10:41 - loss: 82.9289 - accuracy: 0.2476 - f1: 0.484 - ETA: 10:37 - loss: 83.1398 - accuracy: 0.2486 - f1: 0.483 - ETA: 10:33 - loss: 83.2473 - accuracy: 0.2491 - f1: 0.483 - ETA: 10:28 - loss: 83.2948 - accuracy: 0.2486 - f1: 0.482 - ETA: 10:24 - loss: 83.2868 - accuracy: 0.2478 - f1: 0.482 - ETA: 10:19 - loss: 83.5423 - accuracy: 0.2460 - f1: 0.481 - ETA: 10:15 - loss: 83.4685 - accuracy: 0.2470 - f1: 0.481 - ETA: 10:11 - loss: 83.4236 - accuracy: 0.2474 - f1: 0.483 - ETA: 10:06 - loss: 83.2309 - accuracy: 0.2458 - f1: 0.483 - ETA: 10:02 - loss: 83.1650 - accuracy: 0.2458 - f1: 0.483 - ETA: 9:57 - loss: 83.2144 - accuracy: 0.2451 - f1: 0.482 - ETA: 9:53 - loss: 83.1284 - accuracy: 0.2463 - f1: 0.48 - ETA: 9:49 - loss: 83.0859 - accuracy: 0.2460 - f1: 0.48 - ETA: 9:44 - loss: 83.0258 - accuracy: 0.2464 - f1: 0.48 - ETA: 9:40 - loss: 82.9000 - accuracy: 0.2465 - f1: 0.48 - ETA: 9:36 - loss: 82.8231 - accuracy: 0.2481 - f1: 0.48 - ETA: 9:31 - loss: 82.7816 - accuracy: 0.2477 - f1: 0.48 - ETA: 9:27 - loss: 82.6274 - accuracy: 0.2492 - f1: 0.48 - ETA: 9:23 - loss: 82.7047 - accuracy: 0.2493 - f1: 0.48 - ETA: 9:18 - loss: 82.8758 - accuracy: 0.2482 - f1: 0.48 - ETA: 9:14 - loss: 82.7870 - accuracy: 0.2475 - f1: 0.48 - ETA: 9:10 - loss: 82.7549 - accuracy: 0.2471 - f1: 0.48 - ETA: 9:05 - loss: 82.7585 - accuracy: 0.2479 - f1: 0.48 - ETA: 9:01 - loss: 82.7297 - accuracy: 0.2472 - f1: 0.48 - ETA: 8:57 - loss: 82.7171 - accuracy: 0.2490 - f1: 0.48 - ETA: 8:53 - loss: 82.7041 - accuracy: 0.2476 - f1: 0.48 - ETA: 8:48 - loss: 82.6675 - accuracy: 0.2469 - f1: 0.48 - ETA: 8:44 - loss: 82.5932 - accuracy: 0.2493 - f1: 0.48 - ETA: 8:40 - loss: 82.7015 - accuracy: 0.2493 - f1: 0.48 - ETA: 8:35 - loss: 82.6944 - accuracy: 0.2493 - f1: 0.48 - ETA: 8:31 - loss: 82.6738 - accuracy: 0.2497 - f1: 0.48 - ETA: 8:27 - loss: 82.7575 - accuracy: 0.2494 - f1: 0.48 - ETA: 8:22 - loss: 82.6296 - accuracy: 0.2506 - f1: 0.48 - ETA: 8:18 - loss: 82.5846 - accuracy: 0.2503 - f1: 0.48 - ETA: 8:14 - loss: 82.5843 - accuracy: 0.2503 - f1: 0.49 - ETA: 8:09 - loss: 82.5134 - accuracy: 0.2503 - f1: 0.49 - ETA: 8:05 - loss: 82.5031 - accuracy: 0.2518 - f1: 0.49 - ETA: 8:01 - loss: 82.4989 - accuracy: 0.2512 - f1: 0.49 - ETA: 7:57 - loss: 82.5555 - accuracy: 0.2509 - f1: 0.49 - ETA: 7:52 - loss: 82.5280 - accuracy: 0.2509 - f1: 0.49 - ETA: 7:48 - loss: 82.3733 - accuracy: 0.2512 - f1: 0.49 - ETA: 7:44 - loss: 82.3936 - accuracy: 0.2523 - f1: 0.49 - ETA: 7:40 - loss: 82.4131 - accuracy: 0.2523 - f1: 0.49 - ETA: 7:35 - loss: 82.3744 - accuracy: 0.2529 - f1: 0.49 - ETA: 7:31 - loss: 82.4397 - accuracy: 0.2523 - f1: 0.49 - ETA: 7:27 - loss: 82.4353 - accuracy: 0.2517 - f1: 0.49 - ETA: 7:23 - loss: 82.3813 - accuracy: 0.2522 - f1: 0.49 - ETA: 7:18 - loss: 82.2881 - accuracy: 0.2519 - f1: 0.49 - ETA: 7:14 - loss: 82.3805 - accuracy: 0.2514 - f1: 0.49 - ETA: 7:10 - loss: 82.3338 - accuracy: 0.2516 - f1: 0.49 - ETA: 7:06 - loss: 82.4079 - accuracy: 0.2513 - f1: 0.49 - ETA: 7:01 - loss: 82.3920 - accuracy: 0.2505 - f1: 0.49 - ETA: 6:57 - loss: 82.4114 - accuracy: 0.2497 - f1: 0.49 - ETA: 6:53 - loss: 82.3569 - accuracy: 0.2497 - f1: 0.49 - ETA: 6:49 - loss: 82.4229 - accuracy: 0.2487 - f1: 0.49 - ETA: 6:44 - loss: 82.4884 - accuracy: 0.2495 - f1: 0.49 - ETA: 6:40 - loss: 82.4565 - accuracy: 0.2500 - f1: 0.49 - ETA: 6:36 - loss: 82.4059 - accuracy: 0.2500 - f1: 0.49 - ETA: 6:32 - loss: 82.3306 - accuracy: 0.2497 - f1: 0.49 - ETA: 6:27 - loss: 82.3048 - accuracy: 0.2495 - f1: 0.49 - ETA: 6:23 - loss: 82.3485 - accuracy: 0.2498 - f1: 0.49 - ETA: 6:19 - loss: 82.4295 - accuracy: 0.2495 - f1: 0.49 - ETA: 6:15 - loss: 82.4795 - accuracy: 0.2505 - f1: 0.49 - ETA: 6:10 - loss: 82.4614 - accuracy: 0.2500 - f1: 0.49 - ETA: 6:06 - loss: 82.4816 - accuracy: 0.2500 - f1: 0.49 - ETA: 6:02 - loss: 82.5101 - accuracy: 0.2500 - f1: 0.49 - ETA: 5:58 - loss: 82.6024 - accuracy: 0.2502 - f1: 0.49 - ETA: 5:53 - loss: 82.5642 - accuracy: 0.2502 - f1: 0.49 - ETA: 5:49 - loss: 82.6053 - accuracy: 0.2498 - f1: 0.49 - ETA: 5:45 - loss: 82.5351 - accuracy: 0.2493 - f1: 0.49 - ETA: 5:41 - loss: 82.5265 - accuracy: 0.2486 - f1: 0.49 - ETA: 5:39 - loss: 82.4420 - accuracy: 0.2486 - f1: 0.49 - ETA: 5:38 - loss: 82.4830 - accuracy: 0.2484 - f1: 0.4936"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 5:38 - loss: 82.4523 - accuracy: 0.2489 - f1: 0.49 - ETA: 5:41 - loss: 82.5154 - accuracy: 0.2487 - f1: 0.49 - ETA: 5:42 - loss: 82.5006 - accuracy: 0.2489 - f1: 0.49 - ETA: 5:44 - loss: 82.6053 - accuracy: 0.2487 - f1: 0.49 - ETA: 5:46 - loss: 82.6032 - accuracy: 0.2487 - f1: 0.49 - ETA: 5:47 - loss: 82.5532 - accuracy: 0.2487 - f1: 0.49 - ETA: 5:48 - loss: 82.5636 - accuracy: 0.2483 - f1: 0.49 - ETA: 5:48 - loss: 82.5343 - accuracy: 0.2476 - f1: 0.49 - ETA: 5:42 - loss: 82.4894 - accuracy: 0.2468 - f1: 0.49 - ETA: 5:37 - loss: 82.4867 - accuracy: 0.2468 - f1: 0.49 - ETA: 5:33 - loss: 82.4663 - accuracy: 0.2471 - f1: 0.49 - ETA: 5:33 - loss: 82.3871 - accuracy: 0.2481 - f1: 0.49 - ETA: 5:33 - loss: 82.3876 - accuracy: 0.2490 - f1: 0.49 - ETA: 5:29 - loss: 82.3374 - accuracy: 0.2494 - f1: 0.49 - ETA: 5:28 - loss: 82.2988 - accuracy: 0.2496 - f1: 0.49 - ETA: 5:28 - loss: 82.2842 - accuracy: 0.2490 - f1: 0.49 - ETA: 5:27 - loss: 82.2105 - accuracy: 0.2492 - f1: 0.49 - ETA: 5:26 - loss: 82.1990 - accuracy: 0.2492 - f1: 0.49 - ETA: 5:25 - loss: 82.2609 - accuracy: 0.2486 - f1: 0.49 - ETA: 5:24 - loss: 82.2754 - accuracy: 0.2488 - f1: 0.49 - ETA: 5:22 - loss: 82.3190 - accuracy: 0.2480 - f1: 0.49 - ETA: 5:21 - loss: 82.3401 - accuracy: 0.2477 - f1: 0.49 - ETA: 5:19 - loss: 82.3384 - accuracy: 0.2477 - f1: 0.49 - ETA: 5:17 - loss: 82.3478 - accuracy: 0.2473 - f1: 0.49 - ETA: 5:14 - loss: 82.4657 - accuracy: 0.2467 - f1: 0.49 - ETA: 5:12 - loss: 82.5376 - accuracy: 0.2464 - f1: 0.49 - ETA: 5:09 - loss: 82.5527 - accuracy: 0.2456 - f1: 0.49 - ETA: 5:07 - loss: 82.5587 - accuracy: 0.2453 - f1: 0.49 - ETA: 5:04 - loss: 82.5112 - accuracy: 0.2457 - f1: 0.49 - ETA: 5:00 - loss: 82.5606 - accuracy: 0.2453 - f1: 0.49 - ETA: 4:57 - loss: 82.6225 - accuracy: 0.2446 - f1: 0.49 - ETA: 4:54 - loss: 82.5581 - accuracy: 0.2452 - f1: 0.49 - ETA: 4:50 - loss: 82.5943 - accuracy: 0.2449 - f1: 0.49 - ETA: 4:47 - loss: 82.5132 - accuracy: 0.2458 - f1: 0.49 - ETA: 4:43 - loss: 82.5200 - accuracy: 0.2457 - f1: 0.49 - ETA: 4:39 - loss: 82.5493 - accuracy: 0.2457 - f1: 0.49 - ETA: 4:35 - loss: 82.4786 - accuracy: 0.2455 - f1: 0.49 - ETA: 4:30 - loss: 82.4709 - accuracy: 0.2454 - f1: 0.49 - ETA: 4:26 - loss: 82.4730 - accuracy: 0.2456 - f1: 0.49 - ETA: 4:21 - loss: 82.4500 - accuracy: 0.2460 - f1: 0.49 - ETA: 4:17 - loss: 82.4831 - accuracy: 0.2460 - f1: 0.49 - ETA: 4:12 - loss: 82.4124 - accuracy: 0.2465 - f1: 0.49 - ETA: 4:07 - loss: 82.4012 - accuracy: 0.2471 - f1: 0.49 - ETA: 4:02 - loss: 82.4249 - accuracy: 0.2466 - f1: 0.49 - ETA: 3:56 - loss: 82.4302 - accuracy: 0.2464 - f1: 0.49 - ETA: 3:48 - loss: 82.4236 - accuracy: 0.2459 - f1: 0.49 - ETA: 3:41 - loss: 82.4804 - accuracy: 0.2459 - f1: 0.49 - ETA: 3:33 - loss: 82.5095 - accuracy: 0.2463 - f1: 0.49 - ETA: 3:26 - loss: 82.5219 - accuracy: 0.2468 - f1: 0.49 - ETA: 3:19 - loss: 82.5167 - accuracy: 0.2465 - f1: 0.49 - ETA: 3:13 - loss: 82.6319 - accuracy: 0.2459 - f1: 0.49 - ETA: 3:08 - loss: 82.5620 - accuracy: 0.2464 - f1: 0.49 - ETA: 3:02 - loss: 82.5646 - accuracy: 0.2461 - f1: 0.49 - ETA: 2:56 - loss: 82.5376 - accuracy: 0.2459 - f1: 0.49 - ETA: 2:50 - loss: 82.5329 - accuracy: 0.2460 - f1: 0.49 - ETA: 2:44 - loss: 82.5271 - accuracy: 0.2466 - f1: 0.49 - ETA: 2:38 - loss: 82.5472 - accuracy: 0.2462 - f1: 0.49 - ETA: 2:31 - loss: 82.5380 - accuracy: 0.2457 - f1: 0.49 - ETA: 2:25 - loss: 82.5127 - accuracy: 0.2456 - f1: 0.49 - ETA: 2:19 - loss: 82.5006 - accuracy: 0.2456 - f1: 0.49 - ETA: 2:12 - loss: 82.5637 - accuracy: 0.2454 - f1: 0.49 - ETA: 2:05 - loss: 82.5988 - accuracy: 0.2453 - f1: 0.49 - ETA: 1:59 - loss: 82.5958 - accuracy: 0.2455 - f1: 0.49 - ETA: 1:52 - loss: 82.5653 - accuracy: 0.2463 - f1: 0.49 - ETA: 1:45 - loss: 82.6062 - accuracy: 0.2465 - f1: 0.49 - ETA: 1:37 - loss: 82.5853 - accuracy: 0.2465 - f1: 0.49 - ETA: 1:30 - loss: 82.5963 - accuracy: 0.2460 - f1: 0.49 - ETA: 1:22 - loss: 82.6315 - accuracy: 0.2458 - f1: 0.48 - ETA: 1:15 - loss: 82.6767 - accuracy: 0.2459 - f1: 0.48 - ETA: 1:08 - loss: 82.6910 - accuracy: 0.2465 - f1: 0.48 - ETA: 1:00 - loss: 82.6822 - accuracy: 0.2475 - f1: 0.49 - ETA: 53s - loss: 82.7140 - accuracy: 0.2472 - f1: 0.4896 - ETA: 45s - loss: 82.7554 - accuracy: 0.2470 - f1: 0.489 - ETA: 38s - loss: 82.7050 - accuracy: 0.2473 - f1: 0.489 - ETA: 30s - loss: 82.7713 - accuracy: 0.2471 - f1: 0.488 - ETA: 22s - loss: 82.7642 - accuracy: 0.2469 - f1: 0.488 - ETA: 14s - loss: 82.7392 - accuracy: 0.2472 - f1: 0.488 - ETA: 7s - loss: 82.7109 - accuracy: 0.2473 - f1: 0.488 - 1982s 286ms/sample - loss: 82.6878 - accuracy: 0.2477 - f1: 0.4886 - val_loss: 72.6523 - val_accuracy: 0.2215 - val_f1: 0.5349\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4352/6940 [=================>............] - ETA: 15:20 - loss: 85.2679 - accuracy: 0.3125 - f1: 0.505 - ETA: 14:59 - loss: 88.1350 - accuracy: 0.2031 - f1: 0.464 - ETA: 14:48 - loss: 85.3345 - accuracy: 0.2396 - f1: 0.476 - ETA: 15:57 - loss: 84.7613 - accuracy: 0.2266 - f1: 0.479 - ETA: 18:37 - loss: 84.1836 - accuracy: 0.2375 - f1: 0.487 - ETA: 24:14 - loss: 85.1947 - accuracy: 0.2292 - f1: 0.487 - ETA: 28:53 - loss: 86.8902 - accuracy: 0.2188 - f1: 0.464 - ETA: 32:01 - loss: 86.6070 - accuracy: 0.2266 - f1: 0.458 - ETA: 31:12 - loss: 86.8178 - accuracy: 0.2292 - f1: 0.454 - ETA: 33:23 - loss: 86.8047 - accuracy: 0.2250 - f1: 0.453 - ETA: 35:17 - loss: 87.0259 - accuracy: 0.2301 - f1: 0.463 - ETA: 36:47 - loss: 85.8608 - accuracy: 0.2396 - f1: 0.465 - ETA: 38:00 - loss: 86.8190 - accuracy: 0.2332 - f1: 0.460 - ETA: 38:59 - loss: 87.3249 - accuracy: 0.2232 - f1: 0.453 - ETA: 39:54 - loss: 87.3106 - accuracy: 0.2208 - f1: 0.456 - ETA: 40:35 - loss: 87.1242 - accuracy: 0.2109 - f1: 0.451 - ETA: 41:11 - loss: 86.5899 - accuracy: 0.2206 - f1: 0.455 - ETA: 41:41 - loss: 86.7228 - accuracy: 0.2135 - f1: 0.455 - ETA: 42:10 - loss: 86.6516 - accuracy: 0.2138 - f1: 0.455 - ETA: 42:31 - loss: 86.9499 - accuracy: 0.2141 - f1: 0.451 - ETA: 42:47 - loss: 86.3950 - accuracy: 0.2232 - f1: 0.459 - ETA: 42:59 - loss: 86.3304 - accuracy: 0.2230 - f1: 0.457 - ETA: 43:11 - loss: 86.3538 - accuracy: 0.2269 - f1: 0.461 - ETA: 43:20 - loss: 85.9901 - accuracy: 0.2305 - f1: 0.466 - ETA: 43:26 - loss: 86.1425 - accuracy: 0.2313 - f1: 0.466 - ETA: 43:33 - loss: 86.4718 - accuracy: 0.2332 - f1: 0.461 - ETA: 43:36 - loss: 86.5599 - accuracy: 0.2280 - f1: 0.459 - ETA: 43:38 - loss: 86.4152 - accuracy: 0.2288 - f1: 0.454 - ETA: 43:39 - loss: 86.2046 - accuracy: 0.2317 - f1: 0.456 - ETA: 43:38 - loss: 85.6912 - accuracy: 0.2313 - f1: 0.458 - ETA: 43:37 - loss: 85.3844 - accuracy: 0.2288 - f1: 0.458 - ETA: 43:35 - loss: 85.1969 - accuracy: 0.2305 - f1: 0.460 - ETA: 43:34 - loss: 84.8063 - accuracy: 0.2311 - f1: 0.463 - ETA: 43:30 - loss: 84.5075 - accuracy: 0.2325 - f1: 0.465 - ETA: 43:25 - loss: 84.1124 - accuracy: 0.2313 - f1: 0.467 - ETA: 43:20 - loss: 83.9457 - accuracy: 0.2326 - f1: 0.469 - ETA: 43:15 - loss: 84.3977 - accuracy: 0.2314 - f1: 0.468 - ETA: 43:09 - loss: 84.4442 - accuracy: 0.2311 - f1: 0.468 - ETA: 43:01 - loss: 84.6673 - accuracy: 0.2316 - f1: 0.468 - ETA: 42:53 - loss: 84.5714 - accuracy: 0.2328 - f1: 0.468 - ETA: 42:45 - loss: 84.6362 - accuracy: 0.2302 - f1: 0.467 - ETA: 42:37 - loss: 84.4663 - accuracy: 0.2307 - f1: 0.466 - ETA: 42:21 - loss: 84.2460 - accuracy: 0.2340 - f1: 0.468 - ETA: 41:38 - loss: 84.2223 - accuracy: 0.2322 - f1: 0.466 - ETA: 41:30 - loss: 84.0591 - accuracy: 0.2313 - f1: 0.466 - ETA: 40:41 - loss: 84.0488 - accuracy: 0.2303 - f1: 0.464 - ETA: 40:32 - loss: 84.3071 - accuracy: 0.2320 - f1: 0.464 - ETA: 40:24 - loss: 84.2551 - accuracy: 0.2298 - f1: 0.466 - ETA: 40:19 - loss: 84.4191 - accuracy: 0.2283 - f1: 0.464 - ETA: 40:11 - loss: 84.5711 - accuracy: 0.2281 - f1: 0.462 - ETA: 40:04 - loss: 84.4042 - accuracy: 0.2267 - f1: 0.462 - ETA: 39:57 - loss: 84.3775 - accuracy: 0.2296 - f1: 0.461 - ETA: 39:49 - loss: 84.5464 - accuracy: 0.2305 - f1: 0.459 - ETA: 39:40 - loss: 84.5457 - accuracy: 0.2309 - f1: 0.458 - ETA: 39:30 - loss: 84.4737 - accuracy: 0.2301 - f1: 0.459 - ETA: 39:21 - loss: 84.2528 - accuracy: 0.2310 - f1: 0.461 - ETA: 39:12 - loss: 84.2227 - accuracy: 0.2325 - f1: 0.461 - ETA: 39:03 - loss: 84.1132 - accuracy: 0.2328 - f1: 0.462 - ETA: 38:54 - loss: 83.9628 - accuracy: 0.2336 - f1: 0.461 - ETA: 38:44 - loss: 83.9237 - accuracy: 0.2328 - f1: 0.462 - ETA: 38:34 - loss: 83.8763 - accuracy: 0.2331 - f1: 0.462 - ETA: 38:24 - loss: 83.8597 - accuracy: 0.2344 - f1: 0.463 - ETA: 38:14 - loss: 83.6910 - accuracy: 0.2346 - f1: 0.464 - ETA: 38:03 - loss: 83.5957 - accuracy: 0.2363 - f1: 0.465 - ETA: 37:51 - loss: 83.5805 - accuracy: 0.2375 - f1: 0.464 - ETA: 37:40 - loss: 83.4500 - accuracy: 0.2391 - f1: 0.465 - ETA: 37:28 - loss: 83.4710 - accuracy: 0.2379 - f1: 0.465 - ETA: 37:14 - loss: 83.5140 - accuracy: 0.2367 - f1: 0.465 - ETA: 37:02 - loss: 83.3211 - accuracy: 0.2396 - f1: 0.467 - ETA: 36:36 - loss: 83.2941 - accuracy: 0.2397 - f1: 0.467 - ETA: 36:14 - loss: 83.2910 - accuracy: 0.2390 - f1: 0.466 - ETA: 35:45 - loss: 83.1269 - accuracy: 0.2405 - f1: 0.468 - ETA: 35:34 - loss: 83.2185 - accuracy: 0.2406 - f1: 0.469 - ETA: 35:24 - loss: 83.1542 - accuracy: 0.2399 - f1: 0.469 - ETA: 35:13 - loss: 83.0010 - accuracy: 0.2412 - f1: 0.469 - ETA: 35:03 - loss: 82.9580 - accuracy: 0.2405 - f1: 0.469 - ETA: 34:51 - loss: 83.0342 - accuracy: 0.2411 - f1: 0.468 - ETA: 34:40 - loss: 82.9775 - accuracy: 0.2424 - f1: 0.468 - ETA: 34:28 - loss: 82.9632 - accuracy: 0.2433 - f1: 0.468 - ETA: 34:17 - loss: 82.8849 - accuracy: 0.2441 - f1: 0.468 - ETA: 34:05 - loss: 82.8875 - accuracy: 0.2446 - f1: 0.470 - ETA: 33:53 - loss: 82.8752 - accuracy: 0.2458 - f1: 0.469 - ETA: 33:41 - loss: 82.8567 - accuracy: 0.2444 - f1: 0.468 - ETA: 33:29 - loss: 82.8078 - accuracy: 0.2429 - f1: 0.469 - ETA: 33:16 - loss: 82.7975 - accuracy: 0.2415 - f1: 0.469 - ETA: 33:04 - loss: 82.7843 - accuracy: 0.2416 - f1: 0.469 - ETA: 32:51 - loss: 82.8065 - accuracy: 0.2428 - f1: 0.469 - ETA: 32:38 - loss: 82.8500 - accuracy: 0.2422 - f1: 0.468 - ETA: 32:25 - loss: 82.8881 - accuracy: 0.2430 - f1: 0.468 - ETA: 32:12 - loss: 82.8113 - accuracy: 0.2431 - f1: 0.467 - ETA: 31:59 - loss: 82.7736 - accuracy: 0.2438 - f1: 0.468 - ETA: 31:47 - loss: 82.7918 - accuracy: 0.2446 - f1: 0.468 - ETA: 31:33 - loss: 82.7732 - accuracy: 0.2446 - f1: 0.468 - ETA: 31:20 - loss: 82.8374 - accuracy: 0.2457 - f1: 0.468 - ETA: 31:06 - loss: 82.8549 - accuracy: 0.2454 - f1: 0.468 - ETA: 30:53 - loss: 82.8813 - accuracy: 0.2454 - f1: 0.469 - ETA: 30:40 - loss: 82.8431 - accuracy: 0.2461 - f1: 0.468 - ETA: 30:26 - loss: 82.8054 - accuracy: 0.2465 - f1: 0.469 - ETA: 30:13 - loss: 82.8107 - accuracy: 0.2453 - f1: 0.469 - ETA: 29:59 - loss: 82.8934 - accuracy: 0.2456 - f1: 0.469 - ETA: 29:46 - loss: 82.9139 - accuracy: 0.2450 - f1: 0.469 - ETA: 29:32 - loss: 82.9296 - accuracy: 0.2448 - f1: 0.469 - ETA: 29:18 - loss: 83.0331 - accuracy: 0.2436 - f1: 0.468 - ETA: 29:05 - loss: 82.8213 - accuracy: 0.2449 - f1: 0.470 - ETA: 28:51 - loss: 82.8181 - accuracy: 0.2449 - f1: 0.470 - ETA: 28:37 - loss: 82.8273 - accuracy: 0.2453 - f1: 0.471 - ETA: 28:18 - loss: 82.7524 - accuracy: 0.2468 - f1: 0.471 - ETA: 27:57 - loss: 82.7926 - accuracy: 0.2474 - f1: 0.472 - ETA: 27:42 - loss: 82.9554 - accuracy: 0.2457 - f1: 0.471 - ETA: 27:25 - loss: 82.9361 - accuracy: 0.2455 - f1: 0.471 - ETA: 27:02 - loss: 82.9075 - accuracy: 0.2452 - f1: 0.470 - ETA: 26:47 - loss: 82.9835 - accuracy: 0.2444 - f1: 0.468 - ETA: 26:34 - loss: 82.8926 - accuracy: 0.2447 - f1: 0.469 - ETA: 26:19 - loss: 82.8427 - accuracy: 0.2456 - f1: 0.469 - ETA: 26:04 - loss: 82.8141 - accuracy: 0.2462 - f1: 0.469 - ETA: 25:50 - loss: 82.7595 - accuracy: 0.2473 - f1: 0.469 - ETA: 25:36 - loss: 82.7359 - accuracy: 0.2476 - f1: 0.469 - ETA: 25:22 - loss: 82.7338 - accuracy: 0.2471 - f1: 0.470 - ETA: 25:08 - loss: 82.8644 - accuracy: 0.2471 - f1: 0.470 - ETA: 24:53 - loss: 82.8549 - accuracy: 0.2471 - f1: 0.470 - ETA: 24:39 - loss: 82.8756 - accuracy: 0.2466 - f1: 0.471 - ETA: 24:25 - loss: 82.9051 - accuracy: 0.2467 - f1: 0.471 - ETA: 24:10 - loss: 82.8757 - accuracy: 0.2467 - f1: 0.472 - ETA: 23:56 - loss: 82.8481 - accuracy: 0.2470 - f1: 0.472 - ETA: 23:41 - loss: 82.7766 - accuracy: 0.2480 - f1: 0.473 - ETA: 23:26 - loss: 82.7322 - accuracy: 0.2483 - f1: 0.473 - ETA: 23:12 - loss: 82.7077 - accuracy: 0.2478 - f1: 0.473 - ETA: 22:57 - loss: 82.8050 - accuracy: 0.2468 - f1: 0.472 - ETA: 22:42 - loss: 82.8696 - accuracy: 0.2461 - f1: 0.472 - ETA: 22:27 - loss: 82.8023 - accuracy: 0.2469 - f1: 0.473 - ETA: 22:12 - loss: 82.7866 - accuracy: 0.2469 - f1: 0.474 - ETA: 21:58 - loss: 82.7099 - accuracy: 0.2469 - f1: 0.474 - ETA: 21:43 - loss: 82.7525 - accuracy: 0.2469 - f1: 0.474 - ETA: 21:28 - loss: 82.6801 - accuracy: 0.2479 - f1: 0.475 - ETA: 21:13 - loss: 82.6219 - accuracy: 0.2484 - f1: 0.476 - ETA: 20:58 - loss: 82.5590 - accuracy: 0.2489 - f1: 0.4772"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 20:43 - loss: 82.6372 - accuracy: 0.2489 - f1: 0.476 - ETA: 20:28 - loss: 82.7094 - accuracy: 0.2480 - f1: 0.477 - ETA: 20:08 - loss: 82.6705 - accuracy: 0.2473 - f1: 0.477 - ETA: 19:50 - loss: 82.7180 - accuracy: 0.2471 - f1: 0.477 - ETA: 19:32 - loss: 82.7191 - accuracy: 0.2471 - f1: 0.477 - ETA: 19:16 - loss: 82.7180 - accuracy: 0.2467 - f1: 0.478 - ETA: 19:01 - loss: 82.7524 - accuracy: 0.2467 - f1: 0.478 - ETA: 18:47 - loss: 82.6860 - accuracy: 0.2463 - f1: 0.478 - ETA: 18:32 - loss: 82.6693 - accuracy: 0.2461 - f1: 0.478 - ETA: 18:17 - loss: 82.5980 - accuracy: 0.2466 - f1: 0.479 - ETA: 18:02 - loss: 82.5685 - accuracy: 0.2464 - f1: 0.478 - ETA: 17:47 - loss: 82.5170 - accuracy: 0.2468 - f1: 0.479 - ETA: 17:32 - loss: 82.4661 - accuracy: 0.2469 - f1: 0.479 - ETA: 17:17 - loss: 82.5866 - accuracy: 0.2458 - f1: 0.478 - ETA: 17:02 - loss: 82.5743 - accuracy: 0.2459 - f1: 0.478 - ETA: 16:47 - loss: 82.5845 - accuracy: 0.2461 - f1: 0.478 - ETA: 16:32 - loss: 82.6664 - accuracy: 0.2455 - f1: 0.478 - ETA: 16:17 - loss: 82.6899 - accuracy: 0.2459 - f1: 0.478 - ETA: 16:02 - loss: 82.6324 - accuracy: 0.2462 - f1: 0.478 - ETA: 15:47 - loss: 82.7714 - accuracy: 0.2456 - f1: 0.479 - ETA: 15:32 - loss: 82.7255 - accuracy: 0.2454 - f1: 0.479 - ETA: 15:12 - loss: 82.6495 - accuracy: 0.2456 - f1: 0.481 - ETA: 14:55 - loss: 82.5880 - accuracy: 0.2451 - f1: 0.481 - ETA: 14:40 - loss: 82.5965 - accuracy: 0.2447 - f1: 0.481 - ETA: 14:25 - loss: 82.5533 - accuracy: 0.2450 - f1: 0.481 - ETA: 14:10 - loss: 82.5108 - accuracy: 0.2454 - f1: 0.482 - ETA: 13:54 - loss: 82.5391 - accuracy: 0.2450 - f1: 0.482 - ETA: 13:39 - loss: 82.5782 - accuracy: 0.2447 - f1: 0.482 - ETA: 13:24 - loss: 82.5937 - accuracy: 0.2441 - f1: 0.482 - ETA: 13:09 - loss: 82.6303 - accuracy: 0.2440 - f1: 0.481 - ETA: 12:54 - loss: 82.6252 - accuracy: 0.2436 - f1: 0.481 - ETA: 12:39 - loss: 82.6977 - accuracy: 0.2439 - f1: 0.481 - ETA: 12:24 - loss: 82.6764 - accuracy: 0.2437 - f1: 0.481 - ETA: 12:09 - loss: 82.6506 - accuracy: 0.2439 - f1: 0.482 - ETA: 11:53 - loss: 82.6455 - accuracy: 0.2438 - f1: 0.482 - ETA: 11:37 - loss: 82.5974 - accuracy: 0.2442 - f1: 0.482 - ETA: 11:20 - loss: 82.5774 - accuracy: 0.2444 - f1: 0.482 - ETA: 11:02 - loss: 82.6087 - accuracy: 0.2443 - f1: 0.482 - ETA: 10:47 - loss: 82.5812 - accuracy: 0.2448 - f1: 0.483 - ETA: 10:32 - loss: 82.5697 - accuracy: 0.2452 - f1: 0.483 - ETA: 10:17 - loss: 82.6028 - accuracy: 0.2449 - f1: 0.483 - ETA: 10:02 - loss: 82.6147 - accuracy: 0.2451 - f1: 0.483 - ETA: 9:46 - loss: 82.5779 - accuracy: 0.2458 - f1: 0.483 - ETA: 9:31 - loss: 82.6064 - accuracy: 0.2458 - f1: 0.48 - ETA: 9:16 - loss: 82.7080 - accuracy: 0.2457 - f1: 0.48 - ETA: 9:00 - loss: 82.6853 - accuracy: 0.2459 - f1: 0.48 - ETA: 8:45 - loss: 82.6696 - accuracy: 0.2459 - f1: 0.48 - ETA: 8:30 - loss: 82.6166 - accuracy: 0.2464 - f1: 0.48 - ETA: 8:14 - loss: 82.6336 - accuracy: 0.2461 - f1: 0.48 - ETA: 7:59 - loss: 82.6087 - accuracy: 0.2463 - f1: 0.48 - ETA: 7:44 - loss: 82.6912 - accuracy: 0.2453 - f1: 0.48 - ETA: 7:28 - loss: 82.6567 - accuracy: 0.2457 - f1: 0.48 - ETA: 7:13 - loss: 82.6790 - accuracy: 0.2464 - f1: 0.48 - ETA: 6:58 - loss: 82.6697 - accuracy: 0.2462 - f1: 0.48 - ETA: 6:42 - loss: 82.7083 - accuracy: 0.2459 - f1: 0.48 - ETA: 6:27 - loss: 82.6518 - accuracy: 0.2459 - f1: 0.48 - ETA: 6:11 - loss: 82.6555 - accuracy: 0.2453 - f1: 0.48 - ETA: 5:56 - loss: 82.6746 - accuracy: 0.2452 - f1: 0.48 - ETA: 5:40 - loss: 82.7221 - accuracy: 0.2446 - f1: 0.48 - ETA: 5:25 - loss: 82.6868 - accuracy: 0.2443 - f1: 0.48 - ETA: 5:09 - loss: 82.7412 - accuracy: 0.2438 - f1: 0.48 - ETA: 4:54 - loss: 82.7494 - accuracy: 0.2437 - f1: 0.48 - ETA: 4:38 - loss: 82.7534 - accuracy: 0.2434 - f1: 0.48 - ETA: 4:23 - loss: 82.7862 - accuracy: 0.2431 - f1: 0.48 - ETA: 4:07 - loss: 82.7452 - accuracy: 0.2439 - f1: 0.48 - ETA: 3:52 - loss: 82.8067 - accuracy: 0.2433 - f1: 0.48 - ETA: 3:36 - loss: 82.7663 - accuracy: 0.2429 - f1: 0.48 - ETA: 3:21 - loss: 82.8020 - accuracy: 0.2428 - f1: 0.48 - ETA: 3:05 - loss: 82.7669 - accuracy: 0.2431 - f1: 0.48 - ETA: 2:49 - loss: 82.7454 - accuracy: 0.2433 - f1: 0.48 - ETA: 2:33 - loss: 82.6895 - accuracy: 0.2438 - f1: 0.48 - ETA: 2:17 - loss: 82.6970 - accuracy: 0.2437 - f1: 0.48 - ETA: 2:01 - loss: 82.6902 - accuracy: 0.2439 - f1: 0.48 - ETA: 1:45 - loss: 82.6837 - accuracy: 0.2439 - f1: 0.48 - ETA: 1:30 - loss: 82.6657 - accuracy: 0.2441 - f1: 0.48 - ETA: 1:15 - loss: 82.6420 - accuracy: 0.2443 - f1: 0.48 - ETA: 59s - loss: 82.6895 - accuracy: 0.2437 - f1: 0.4871 - ETA: 44s - loss: 82.6785 - accuracy: 0.2442 - f1: 0.487 - ETA: 28s - loss: 82.6543 - accuracy: 0.2445 - f1: 0.487 - ETA: 13s - loss: 82.6739 - accuracy: 0.2448 - f1: 0.486 - 3692s 532ms/sample - loss: 82.6022 - accuracy: 0.2452 - f1: 0.4872 - val_loss: 72.2515 - val_accuracy: 0.2261 - val_f1: 0.5658\n"
     ]
    }
   ],
   "source": [
    "# Defining hyperparameters\n",
    "EPOCHS = 5\n",
    "BS = 32\n",
    "LR=1e-5\n",
    "# Compiling and training\n",
    "bertModel.compile(\n",
    "  optimizer=keras.optimizers.Adam(LR),\n",
    "  loss=multi_loss,\n",
    "  metrics=[\"accuracy\",f1]\n",
    ")\n",
    "(X_train, X_temp, Y_train, Y_temp) = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "(X_valid, X_test, Y_valid, Y_test) = train_test_split(X_temp,Y_temp,test_size=0.5, random_state=42)\n",
    "history = bertModel.fit(\n",
    "  X_train, \n",
    "  Y_train,\n",
    "  validation_data=(X_valid,Y_valid),\n",
    "  batch_size=BS,\n",
    "  shuffle=True,\n",
    "  epochs=EPOCHS,\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868/868 [==============================] - ETA: 5:58 - loss: 76.8420 - accuracy: 0.2188 - f1: 0.50 - ETA: 5:46 - loss: 77.5710 - accuracy: 0.1719 - f1: 0.53 - ETA: 5:31 - loss: 72.9894 - accuracy: 0.2188 - f1: 0.57 - ETA: 5:19 - loss: 72.4802 - accuracy: 0.2031 - f1: 0.57 - ETA: 5:04 - loss: 72.2237 - accuracy: 0.2188 - f1: 0.56 - ETA: 4:50 - loss: 72.0151 - accuracy: 0.2188 - f1: 0.57 - ETA: 4:37 - loss: 71.9796 - accuracy: 0.2232 - f1: 0.57 - ETA: 4:23 - loss: 71.7409 - accuracy: 0.2148 - f1: 0.57 - ETA: 4:09 - loss: 70.8928 - accuracy: 0.2222 - f1: 0.57 - ETA: 3:55 - loss: 71.4149 - accuracy: 0.2250 - f1: 0.56 - ETA: 3:42 - loss: 71.0784 - accuracy: 0.2216 - f1: 0.56 - ETA: 3:28 - loss: 72.1517 - accuracy: 0.2135 - f1: 0.55 - ETA: 3:14 - loss: 72.8379 - accuracy: 0.2067 - f1: 0.54 - ETA: 3:00 - loss: 72.7704 - accuracy: 0.2098 - f1: 0.54 - ETA: 2:47 - loss: 73.0311 - accuracy: 0.2104 - f1: 0.55 - ETA: 2:33 - loss: 73.0639 - accuracy: 0.2090 - f1: 0.55 - ETA: 2:19 - loss: 73.4343 - accuracy: 0.2096 - f1: 0.55 - ETA: 2:05 - loss: 73.8929 - accuracy: 0.2049 - f1: 0.54 - ETA: 1:51 - loss: 73.9814 - accuracy: 0.2039 - f1: 0.54 - ETA: 1:38 - loss: 73.7702 - accuracy: 0.2078 - f1: 0.54 - ETA: 1:24 - loss: 73.4933 - accuracy: 0.2113 - f1: 0.54 - ETA: 1:09 - loss: 73.7530 - accuracy: 0.2074 - f1: 0.54 - ETA: 55s - loss: 73.9588 - accuracy: 0.2079 - f1: 0.5473 - ETA: 41s - loss: 73.9764 - accuracy: 0.2109 - f1: 0.546 - ETA: 28s - loss: 73.9782 - accuracy: 0.2062 - f1: 0.544 - ETA: 14s - loss: 73.8950 - accuracy: 0.2067 - f1: 0.543 - ETA: 1s - loss: 73.4266 - accuracy: 0.2095 - f1: 0.545 - 363s 418ms/sample - loss: 73.1390 - accuracy: 0.2085 - f1: 0.5335\n",
      "Accuracy :  0.20852534\n",
      "F1 Score : 0.5334728\n",
      "Loss : 73.13904793910717\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 1. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 1. 0. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 1. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 1. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 1. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 0. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 1.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [0. 0. 1. 0.]\n",
      "Prediction : [0. 0. 0. 1.]  Actual Value : [1. 0. 1. 0.]\n",
      "Total accuracy :  0.23387096774193547\n",
      "I/E accuracy :  0.7672811059907834\n",
      "N/S accuracy :  0.8605990783410138\n",
      "F/T accuracy :  0.5990783410138248\n",
      "J/P accuracy :  0.6013824884792627\n",
      "Binary accuracy : 0.7070852534562212\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc,test_f1 = bertModel.evaluate(X_test,Y_test)\n",
    "print(\"Accuracy : \",test_acc)\n",
    "print(\"F1 Score :\",test_f1)\n",
    "print(\"Loss :\",test_loss)\n",
    "Yhat=bertModel.predict(X_test)\n",
    "correct=0\n",
    "correctlabels=[0,0,0,0]\n",
    "for i,prediction in enumerate(Yhat):\n",
    "    for j,value in enumerate(prediction):\n",
    "        if (value<0.5):\n",
    "            prediction[j]=0\n",
    "        else:\n",
    "            prediction[j]=1\n",
    "        if(prediction[j]==Y_test[i][j]):\n",
    "            correctlabels[j]+=1\n",
    "    if (np.array_equal(prediction,Y_test[i])):\n",
    "        correct+=1\n",
    "    print(\"Prediction :\",prediction,\" Actual Value :\",Y_test[i] )\n",
    "print(\"Total accuracy : \",correct/len(X_test))\n",
    "print(\"I/E accuracy : \",correctlabels[0]/len(X_test))\n",
    "print(\"N/S accuracy : \",correctlabels[1]/len(X_test))\n",
    "print(\"F/T accuracy : \",correctlabels[2]/len(X_test))\n",
    "print(\"J/P accuracy : \",correctlabels[3]/len(X_test))\n",
    "print(\"Binary accuracy :\",np.sum(correctlabels)/(4*len(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
