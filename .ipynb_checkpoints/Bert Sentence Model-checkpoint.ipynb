{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: py-params>=0.9.6 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.8.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData=[]\n",
    "labels=[]\n",
    "for i,element in enumerate(data.posts):\n",
    "    temp = element.split(\"|||\")\n",
    "    for post in temp:\n",
    "        newData.append(post)\n",
    "        labels.append(data.type[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422845\n",
      "422845\n",
      "Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.\n",
      "INFJ\n"
     ]
    }
   ],
   "source": [
    "print(len(newData))\n",
    "print(len(labels))\n",
    "print(newData[25])\n",
    "print(labels[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>422840</th>\n",
       "      <td>I was going to close my facebook a few months ...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422841</th>\n",
       "      <td>30 Seconds to Mars - All of my collections. It...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422842</th>\n",
       "      <td>I have seen it, and i agree. I did actually th...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422843</th>\n",
       "      <td>Ok so i have just watched Underworld 4 (Awaken...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422844</th>\n",
       "      <td>I would never want to turn off my emotions. so...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  type\n",
       "422840  I was going to close my facebook a few months ...  INFP\n",
       "422841  30 Seconds to Mars - All of my collections. It...  INFP\n",
       "422842  I have seen it, and i agree. I did actually th...  INFP\n",
       "422843  Ok so i have just watched Underworld 4 (Awaken...  INFP\n",
       "422844  I would never want to turn off my emotions. so...  INFP"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "list_of_tuples = list(zip(newData, labels)) \n",
    "df = pd.DataFrame(list_of_tuples, columns = ['sentence', 'type'])  \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart = sns.countplot(data.type)\n",
    "plt.title(\"Number of users per personality\")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "  \n",
    "unique_type_list = unique_type_list + [x.lower() for x in unique_type_list]\n",
    "# Preprocess data\n",
    "def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments using regular expressions\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp)\n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t,\"\")\n",
    "        type_labelized = translate_personality(row[1].type)\n",
    "        if (temp):\n",
    "            list_personality.append(type_labelized)\n",
    "            list_posts.append(temp)\n",
    "\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |   and  moments sportscenter not top ten plays pranks |  What has been the most life changing experience in your life |  On repeat for most of today |  May the PerC Experience immerse you |  The last thing my  friend posted on his facebook before committing suicide the next day Rest in peace |  Hello  Sorry to hear of your distress It s only natural for a relationship to not be perfection all the time in every moment of existence Try to figure the hard times as times of growth as |  Welcome and stuff |  Game Set Match |  Prozac wellbrutin at least thirty minutes of moving your legs and I don t mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternative |  Basically come up with three items you ve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left by |  All things in moderation Sims is indeed a video game and a good one at that Note a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim |  Dear  What were your favorite video games growing up and what are your now current favorite video games cool \n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X=list_posts\n",
    "Y=list_personality\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', \"'\", 'm', 'here', 'to', 'stay', ',', 'hello', 'nice', 'to', 'meet', 'you', 'sir', '[SEP]']\n",
      "[101, 1045, 1005, 1049, 2182, 2000, 2994, 1010, 7592, 3835, 2000, 3113, 2017, 2909, 102]\n"
     ]
    }
   ],
   "source": [
    "# importing the tokenizer and testing it\n",
    "tokenizer = FullTokenizer(\n",
    "  vocab_file=\"./dataset/bert/vocab.txt\"\n",
    ")\n",
    "tokens=tokenizer.tokenize(\"I'm here to stay , hello nice to meet you sir\")\n",
    "tokens=[\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "print(tokens)\n",
    "indices=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 8675 rows\n",
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the features\n",
    "def tokenize_features(X):\n",
    "    X_tokenized=[]\n",
    "    for i,entry in enumerate(X):\n",
    "        if (i % 500 == 0 or i == 1 or i == len(X)):\n",
    "            print(\"%s of %s rows\" % (i, len(X)))\n",
    "        temp=[]\n",
    "        temp=tokenizer.tokenize(entry)\n",
    "        temp = [w.replace('|', '[SEP]') for w in temp]\n",
    "        if temp[0] == \"[SEP]\":\n",
    "            temp.pop(0)\n",
    "        temp.insert(0,\"[CLS]\")\n",
    "        temp.append(\"[SEP]\")\n",
    "        X_tokenized.append(temp)\n",
    "    return(X_tokenized)\n",
    "\n",
    "X_tokenized=tokenize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1998, 5312, 2998, 13013, 2121, 2025, 2327, 2702, 3248, 26418, 2015, 102, 2054, 2038, 2042, 1996, 2087, 2166, 5278, 3325, 1999, 2115, 2166, 102, 2006, 9377, 2005, 2087, 1997, 2651, 102, 2089, 1996, 2566, 2278, 3325, 10047, 16862, 2063, 2017, 102, 1996, 2197, 2518, 2026, 2767, 6866, 2006, 2010, 9130, 2077, 16873, 5920, 1996, 2279, 2154, 2717, 1999, 3521, 102, 7592, 3374, 2000, 2963, 1997, 2115, 12893, 2009, 1055, 2069, 3019, 2005, 1037, 3276, 2000, 2025, 2022, 15401, 2035, 1996, 2051, 1999, 2296, 2617, 1997, 4598, 3046, 2000, 3275, 1996, 2524, 2335, 2004, 2335, 1997, 3930, 2004, 102, 6160, 1998, 4933, 102, 2208, 2275, 2674, 102, 4013, 4143, 2278, 2092, 19892, 21823, 2078, 2012, 2560, 4228, 2781, 1997, 3048, 2115, 3456, 1998, 1045, 2123, 1056, 2812, 3048, 2068, 2096, 3564, 1999, 2115, 2168, 4624, 3242, 17901, 1999, 5549, 8156, 2672, 3046, 21006, 2015, 2004, 1037, 2740, 3771, 4522, 102, 10468, 2272, 2039, 2007, 2093, 5167, 2017, 2310, 4340, 2008, 2169, 2828, 2030, 29221, 4127, 2017, 2215, 2000, 2079, 2052, 2062, 2084, 3497, 2224, 2445, 2169, 4127, 10699, 4972, 1998, 2054, 17048, 2043, 2187, 2011, 102, 2035, 2477, 1999, 5549, 8156, 18135, 2003, 5262, 1037, 2678, 2208, 1998, 1037, 2204, 2028, 2012, 2008, 3602, 1037, 2204, 2028, 2012, 2008, 2003, 5399, 20714, 1999, 2008, 1045, 2572, 2025, 3294, 7694, 1996, 2331, 1997, 2151, 2445, 21934, 102, 6203, 2054, 2020, 2115, 5440, 2678, 2399, 3652, 2039, 1998, 2054, 2024, 2115, 2085, 2783, 5440, 2678, 2399, 4658, 102]\n"
     ]
    }
   ],
   "source": [
    "# indexing the tokens\n",
    "def tokens_to_indices(tokens):\n",
    "    result = []\n",
    "    for element in tokens:\n",
    "        indices =[]\n",
    "        indices=tokenizer.convert_tokens_to_ids(element)\n",
    "        result.append(indices)\n",
    "    return result\n",
    "X_indexed=tokens_to_indices(X_tokenized)\n",
    "print(X_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "# getting the longest indexed array\n",
    "def max_post_words(posts):\n",
    "    maxLen=0\n",
    "    averageLen=0\n",
    "    for post in posts:\n",
    "        averageLen=averageLen+len(post)\n",
    "        if maxLen < len(post):\n",
    "            maxLen=len(post)\n",
    "    averageLen=int(averageLen/len(posts))\n",
    "    return maxLen,averageLen\n",
    "maxLen,avgLen=max_post_words(X_indexed)\n",
    "print(maxLen)\n",
    "print(avgLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding padding \n",
    "def padding(indices):\n",
    "    for i,element in enumerate(indices):\n",
    "        pad=maxLen-len(element)\n",
    "        l = [0] * pad\n",
    "        indices[i]=indices[i] + l\n",
    "    return(indices)\n",
    "X=np.array(padding(X_indexed))\n",
    "Y=(np.array(list_personality[:,0])).astype(np.float32)\n",
    "#Y=(np.array(list_personality)).astype(np.float32)\n",
    "# Not enough memory to train model with maxLen input size so i'll take the first 512\n",
    "#X=X[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    with tf.io.gfile.GFile(\"./dataset/bert/bert_config.json\", \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "    input_ids = keras.layers.Input(shape=(maxLen,), dtype='int16', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    X1 = keras.layers.Dense(units=1,name=\"I/E_classifier\", activation=\"sigmoid\")(logits)\n",
    "    '''X2 = keras.layers.Dense(units=1,name=\"N/S_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X3 = keras.layers.Dense(units=1,name=\"F/T_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X4 = keras.layers.Dense(units=1,name=\"J/P_classifier\", activation=\"sigmoid\")(logits)\n",
    "    finalOutput=keras.layers.concatenate(\n",
    "    inputs=[X1,X2,X3,X4],\n",
    "    name='final_output')'''\n",
    "    model = keras.Model(inputs=input_ids, outputs=X1)\n",
    "    model.build(input_shape=(None, maxLen))\n",
    "    load_stock_weights(bert, \"./dataset/bert/bert_model.ckpt\")\n",
    "    model.layers[1].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 643, 768)\n",
      "loader: Skipping weight:[bert_1/embeddings/position_embeddings/embeddings:0] as the weight shape:[(1083, 768)] is not compatible with the checkpoint:[bert/embeddings/position_embeddings] shape:(512, 768)\n",
      "Done loading 195 BERT weights from: ./dataset/bert/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000002DE50AD1E10> (prefix:bert_1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [1]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/position_embeddings\n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 643)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 643, 768)          109328640 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "I/E_classifier (Dense)       (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 109,920,001\n",
      "Trainable params: 591,361\n",
      "Non-trainable params: 109,328,640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bertModel = create_model()\n",
    "bertModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss(y_true,y_pred):\n",
    "    return K.mean(K.sum(K.binary_crossentropy(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 643)\n",
      "(8675,)\n",
      "Train on 6940 samples, validate on 867 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/6940 [==================>...........] - ETA: 49:30 - loss: 0.6765 - accuracy: 0.5938 - f1: 0.23 - ETA: 34:45 - loss: 0.6210 - accuracy: 0.6719 - f1: 0.21 - ETA: 29:44 - loss: 0.5984 - accuracy: 0.6771 - f1: 0.14 - ETA: 27:11 - loss: 0.5980 - accuracy: 0.6797 - f1: 0.18 - ETA: 25:37 - loss: 0.5999 - accuracy: 0.6750 - f1: 0.17 - ETA: 24:32 - loss: 0.6277 - accuracy: 0.6615 - f1: 0.14 - ETA: 23:45 - loss: 0.6165 - accuracy: 0.6830 - f1: 0.19 - ETA: 23:08 - loss: 0.6152 - accuracy: 0.6758 - f1: 0.20 - ETA: 22:38 - loss: 0.6051 - accuracy: 0.6806 - f1: 0.20 - ETA: 22:13 - loss: 0.6060 - accuracy: 0.6750 - f1: 0.19 - ETA: 21:52 - loss: 0.6066 - accuracy: 0.6761 - f1: 0.17 - ETA: 21:33 - loss: 0.6035 - accuracy: 0.6719 - f1: 0.19 - ETA: 21:16 - loss: 0.6131 - accuracy: 0.6731 - f1: 0.19 - ETA: 21:01 - loss: 0.6200 - accuracy: 0.6652 - f1: 0.17 - ETA: 20:47 - loss: 0.6159 - accuracy: 0.6687 - f1: 0.17 - ETA: 20:34 - loss: 0.6128 - accuracy: 0.6680 - f1: 0.16 - ETA: 20:22 - loss: 0.6011 - accuracy: 0.6801 - f1: 0.17 - ETA: 20:11 - loss: 0.5960 - accuracy: 0.6840 - f1: 0.17 - ETA: 20:00 - loss: 0.5886 - accuracy: 0.6842 - f1: 0.17 - ETA: 19:50 - loss: 0.5987 - accuracy: 0.6812 - f1: 0.16 - ETA: 19:40 - loss: 0.6069 - accuracy: 0.6786 - f1: 0.16 - ETA: 19:31 - loss: 0.6013 - accuracy: 0.6832 - f1: 0.15 - ETA: 19:22 - loss: 0.5952 - accuracy: 0.6902 - f1: 0.14 - ETA: 19:13 - loss: 0.5947 - accuracy: 0.6927 - f1: 0.14 - ETA: 19:05 - loss: 0.5886 - accuracy: 0.6950 - f1: 0.13 - ETA: 18:57 - loss: 0.5970 - accuracy: 0.6947 - f1: 0.13 - ETA: 18:49 - loss: 0.5976 - accuracy: 0.6979 - f1: 0.13 - ETA: 18:41 - loss: 0.6054 - accuracy: 0.6964 - f1: 0.13 - ETA: 18:33 - loss: 0.6095 - accuracy: 0.6950 - f1: 0.12 - ETA: 18:26 - loss: 0.6068 - accuracy: 0.6990 - f1: 0.13 - ETA: 18:18 - loss: 0.6110 - accuracy: 0.6996 - f1: 0.13 - ETA: 18:11 - loss: 0.6122 - accuracy: 0.7002 - f1: 0.12 - ETA: 18:04 - loss: 0.6111 - accuracy: 0.6998 - f1: 0.13 - ETA: 17:56 - loss: 0.6168 - accuracy: 0.6976 - f1: 0.13 - ETA: 17:49 - loss: 0.6194 - accuracy: 0.6973 - f1: 0.13 - ETA: 17:42 - loss: 0.6182 - accuracy: 0.6979 - f1: 0.12 - ETA: 17:36 - loss: 0.6154 - accuracy: 0.7002 - f1: 0.12 - ETA: 17:29 - loss: 0.6129 - accuracy: 0.7015 - f1: 0.12 - ETA: 17:22 - loss: 0.6160 - accuracy: 0.7011 - f1: 0.12 - ETA: 17:15 - loss: 0.6153 - accuracy: 0.7039 - f1: 0.12 - ETA: 17:09 - loss: 0.6145 - accuracy: 0.7058 - f1: 0.11 - ETA: 17:02 - loss: 0.6111 - accuracy: 0.7091 - f1: 0.11 - ETA: 16:56 - loss: 0.6123 - accuracy: 0.7100 - f1: 0.11 - ETA: 16:49 - loss: 0.6103 - accuracy: 0.7116 - f1: 0.11 - ETA: 16:43 - loss: 0.6064 - accuracy: 0.7132 - f1: 0.11 - ETA: 16:36 - loss: 0.6016 - accuracy: 0.7174 - f1: 0.12 - ETA: 16:30 - loss: 0.6002 - accuracy: 0.7201 - f1: 0.12 - ETA: 16:23 - loss: 0.5973 - accuracy: 0.7220 - f1: 0.12 - ETA: 16:17 - loss: 0.5985 - accuracy: 0.7226 - f1: 0.12 - ETA: 16:11 - loss: 0.5961 - accuracy: 0.7237 - f1: 0.11 - ETA: 16:04 - loss: 0.5970 - accuracy: 0.7243 - f1: 0.11 - ETA: 15:58 - loss: 0.5948 - accuracy: 0.7254 - f1: 0.11 - ETA: 15:52 - loss: 0.5935 - accuracy: 0.7264 - f1: 0.11 - ETA: 15:46 - loss: 0.5924 - accuracy: 0.7274 - f1: 0.11 - ETA: 15:39 - loss: 0.5941 - accuracy: 0.7284 - f1: 0.11 - ETA: 15:33 - loss: 0.5977 - accuracy: 0.7266 - f1: 0.11 - ETA: 15:27 - loss: 0.6001 - accuracy: 0.7248 - f1: 0.10 - ETA: 15:21 - loss: 0.5992 - accuracy: 0.7263 - f1: 0.10 - ETA: 15:15 - loss: 0.5982 - accuracy: 0.7262 - f1: 0.10 - ETA: 15:09 - loss: 0.5968 - accuracy: 0.7266 - f1: 0.10 - ETA: 15:03 - loss: 0.6013 - accuracy: 0.7239 - f1: 0.10 - ETA: 14:57 - loss: 0.5999 - accuracy: 0.7253 - f1: 0.10 - ETA: 14:50 - loss: 0.5992 - accuracy: 0.7267 - f1: 0.10 - ETA: 14:44 - loss: 0.5970 - accuracy: 0.7290 - f1: 0.10 - ETA: 14:38 - loss: 0.5952 - accuracy: 0.7308 - f1: 0.10 - ETA: 14:32 - loss: 0.5961 - accuracy: 0.7306 - f1: 0.10 - ETA: 14:26 - loss: 0.5973 - accuracy: 0.7309 - f1: 0.10 - ETA: 14:20 - loss: 0.5949 - accuracy: 0.7312 - f1: 0.10 - ETA: 14:14 - loss: 0.5918 - accuracy: 0.7341 - f1: 0.10 - ETA: 14:08 - loss: 0.5906 - accuracy: 0.7357 - f1: 0.11 - ETA: 14:02 - loss: 0.5912 - accuracy: 0.7364 - f1: 0.11 - ETA: 13:56 - loss: 0.5925 - accuracy: 0.7348 - f1: 0.11 - ETA: 13:50 - loss: 0.5935 - accuracy: 0.7342 - f1: 0.11 - ETA: 13:44 - loss: 0.5914 - accuracy: 0.7348 - f1: 0.10 - ETA: 13:38 - loss: 0.5932 - accuracy: 0.7337 - f1: 0.10 - ETA: 13:32 - loss: 0.5933 - accuracy: 0.7344 - f1: 0.10 - ETA: 13:26 - loss: 0.5947 - accuracy: 0.7338 - f1: 0.10 - ETA: 13:20 - loss: 0.5947 - accuracy: 0.7336 - f1: 0.10 - ETA: 13:14 - loss: 0.5932 - accuracy: 0.7354 - f1: 0.10 - ETA: 13:08 - loss: 0.5930 - accuracy: 0.7359 - f1: 0.10 - ETA: 13:02 - loss: 0.5937 - accuracy: 0.7357 - f1: 0.10 - ETA: 12:57 - loss: 0.5922 - accuracy: 0.7367 - f1: 0.10 - ETA: 12:51 - loss: 0.5922 - accuracy: 0.7376 - f1: 0.10 - ETA: 12:45 - loss: 0.5933 - accuracy: 0.7366 - f1: 0.10 - ETA: 12:39 - loss: 0.5948 - accuracy: 0.7360 - f1: 0.10 - ETA: 12:33 - loss: 0.5948 - accuracy: 0.7355 - f1: 0.10 - ETA: 12:27 - loss: 0.5943 - accuracy: 0.7364 - f1: 0.10 - ETA: 12:21 - loss: 0.5935 - accuracy: 0.7372 - f1: 0.10 - ETA: 12:15 - loss: 0.5942 - accuracy: 0.7363 - f1: 0.10 - ETA: 12:09 - loss: 0.5927 - accuracy: 0.7375 - f1: 0.10 - ETA: 12:03 - loss: 0.5939 - accuracy: 0.7370 - f1: 0.10 - ETA: 11:58 - loss: 0.5928 - accuracy: 0.7378 - f1: 0.10 - ETA: 11:52 - loss: 0.5932 - accuracy: 0.7376 - f1: 0.10 - ETA: 11:46 - loss: 0.5938 - accuracy: 0.7377 - f1: 0.10 - ETA: 11:40 - loss: 0.5948 - accuracy: 0.7359 - f1: 0.09 - ETA: 11:34 - loss: 0.5938 - accuracy: 0.7367 - f1: 0.09 - ETA: 11:28 - loss: 0.5929 - accuracy: 0.7381 - f1: 0.09 - ETA: 11:22 - loss: 0.5934 - accuracy: 0.7379 - f1: 0.09 - ETA: 11:17 - loss: 0.5939 - accuracy: 0.7380 - f1: 0.09 - ETA: 11:11 - loss: 0.5918 - accuracy: 0.7394 - f1: 0.09 - ETA: 11:05 - loss: 0.5915 - accuracy: 0.7395 - f1: 0.10 - ETA: 10:59 - loss: 0.5913 - accuracy: 0.7402 - f1: 0.10 - ETA: 10:53 - loss: 0.5896 - accuracy: 0.7412 - f1: 0.10 - ETA: 10:48 - loss: 0.5885 - accuracy: 0.7416 - f1: 0.10 - ETA: 10:42 - loss: 0.5885 - accuracy: 0.7411 - f1: 0.10 - ETA: 10:36 - loss: 0.5893 - accuracy: 0.7403 - f1: 0.09 - ETA: 10:30 - loss: 0.5887 - accuracy: 0.7415 - f1: 0.09 - ETA: 10:24 - loss: 0.5895 - accuracy: 0.7413 - f1: 0.09 - ETA: 10:19 - loss: 0.5898 - accuracy: 0.7414 - f1: 0.09 - ETA: 10:13 - loss: 0.5881 - accuracy: 0.7429 - f1: 0.09 - ETA: 10:07 - loss: 0.5874 - accuracy: 0.7432 - f1: 0.09 - ETA: 10:01 - loss: 0.5881 - accuracy: 0.7425 - f1: 0.09 - ETA: 9:55 - loss: 0.5890 - accuracy: 0.7423 - f1: 0.0998 - ETA: 9:50 - loss: 0.5896 - accuracy: 0.7426 - f1: 0.098 - ETA: 9:44 - loss: 0.5886 - accuracy: 0.7432 - f1: 0.098 - ETA: 9:38 - loss: 0.5882 - accuracy: 0.7433 - f1: 0.097 - ETA: 9:32 - loss: 0.5876 - accuracy: 0.7433 - f1: 0.096 - ETA: 9:26 - loss: 0.5882 - accuracy: 0.7431 - f1: 0.095 - ETA: 9:21 - loss: 0.5891 - accuracy: 0.7429 - f1: 0.094 - ETA: 9:15 - loss: 0.5894 - accuracy: 0.7430 - f1: 0.093 - ETA: 9:09 - loss: 0.5901 - accuracy: 0.7425 - f1: 0.093 - ETA: 9:03 - loss: 0.5912 - accuracy: 0.7423 - f1: 0.092 - ETA: 8:58 - loss: 0.5918 - accuracy: 0.7419 - f1: 0.091 - ETA: 8:52 - loss: 0.5916 - accuracy: 0.7424 - f1: 0.090 - ETA: 8:46 - loss: 0.5904 - accuracy: 0.7427 - f1: 0.090 - ETA: 8:40 - loss: 0.5902 - accuracy: 0.7428 - f1: 0.089 - ETA: 8:34 - loss: 0.5902 - accuracy: 0.7426 - f1: 0.090 - ETA: 8:29 - loss: 0.5905 - accuracy: 0.7422 - f1: 0.091 - ETA: 8:23 - loss: 0.5906 - accuracy: 0.7422 - f1: 0.092 - ETA: 8:17 - loss: 0.5913 - accuracy: 0.7423 - f1: 0.091 - ETA: 8:11 - loss: 0.5936 - accuracy: 0.7412 - f1: 0.091 - ETA: 8:06 - loss: 0.5915 - accuracy: 0.7420 - f1: 0.090 - ETA: 8:00 - loss: 0.5903 - accuracy: 0.7427 - f1: 0.089 - ETA: 7:54 - loss: 0.5915 - accuracy: 0.7423 - f1: 0.089 - ETA: 7:48 - loss: 0.5910 - accuracy: 0.7426 - f1: 0.091 - ETA: 7:43 - loss: 0.5910 - accuracy: 0.7424 - f1: 0.090 - ETA: 7:37 - loss: 0.5921 - accuracy: 0.7422 - f1: 0.089 - ETA: 7:31 - loss: 0.5907 - accuracy: 0.7432 - f1: 0.092 - ETA: 7:25 - loss: 0.5904 - accuracy: 0.7435 - f1: 0.092 - ETA: 7:20 - loss: 0.5914 - accuracy: 0.7429 - f1: 0.0916940/6940 [==============================] - ETA: 7:14 - loss: 0.5925 - accuracy: 0.7425 - f1: 0.092 - ETA: 7:08 - loss: 0.5922 - accuracy: 0.7430 - f1: 0.094 - ETA: 7:02 - loss: 0.5919 - accuracy: 0.7432 - f1: 0.095 - ETA: 6:57 - loss: 0.5932 - accuracy: 0.7424 - f1: 0.094 - ETA: 6:51 - loss: 0.5933 - accuracy: 0.7425 - f1: 0.093 - ETA: 6:45 - loss: 0.5926 - accuracy: 0.7429 - f1: 0.093 - ETA: 6:39 - loss: 0.5914 - accuracy: 0.7436 - f1: 0.092 - ETA: 6:34 - loss: 0.5914 - accuracy: 0.7435 - f1: 0.091 - ETA: 6:28 - loss: 0.5911 - accuracy: 0.7435 - f1: 0.091 - ETA: 6:22 - loss: 0.5920 - accuracy: 0.7431 - f1: 0.090 - ETA: 6:16 - loss: 0.5927 - accuracy: 0.7425 - f1: 0.090 - ETA: 6:11 - loss: 0.5925 - accuracy: 0.7426 - f1: 0.089 - ETA: 6:05 - loss: 0.5915 - accuracy: 0.7431 - f1: 0.088 - ETA: 5:59 - loss: 0.5908 - accuracy: 0.7435 - f1: 0.089 - ETA: 5:53 - loss: 0.5896 - accuracy: 0.7444 - f1: 0.089 - ETA: 5:48 - loss: 0.5892 - accuracy: 0.7442 - f1: 0.088 - ETA: 5:42 - loss: 0.5900 - accuracy: 0.7438 - f1: 0.088 - ETA: 5:36 - loss: 0.5902 - accuracy: 0.7437 - f1: 0.087 - ETA: 5:31 - loss: 0.5901 - accuracy: 0.7435 - f1: 0.087 - ETA: 5:25 - loss: 0.5897 - accuracy: 0.7439 - f1: 0.088 - ETA: 5:19 - loss: 0.5899 - accuracy: 0.7440 - f1: 0.087 - ETA: 5:13 - loss: 0.5894 - accuracy: 0.7444 - f1: 0.088 - ETA: 5:08 - loss: 0.5878 - accuracy: 0.7450 - f1: 0.089 - ETA: 5:02 - loss: 0.5881 - accuracy: 0.7449 - f1: 0.090 - ETA: 4:56 - loss: 0.5889 - accuracy: 0.7443 - f1: 0.089 - ETA: 4:50 - loss: 0.5895 - accuracy: 0.7438 - f1: 0.090 - ETA: 4:45 - loss: 0.5905 - accuracy: 0.7438 - f1: 0.090 - ETA: 4:39 - loss: 0.5914 - accuracy: 0.7427 - f1: 0.090 - ETA: 4:33 - loss: 0.5906 - accuracy: 0.7432 - f1: 0.089 - ETA: 4:27 - loss: 0.5907 - accuracy: 0.7426 - f1: 0.090 - ETA: 4:22 - loss: 0.5904 - accuracy: 0.7427 - f1: 0.089 - ETA: 4:16 - loss: 0.5920 - accuracy: 0.7418 - f1: 0.089 - ETA: 4:10 - loss: 0.5905 - accuracy: 0.7428 - f1: 0.089 - ETA: 4:05 - loss: 0.5912 - accuracy: 0.7423 - f1: 0.089 - ETA: 3:59 - loss: 0.5919 - accuracy: 0.7420 - f1: 0.089 - ETA: 3:53 - loss: 0.5924 - accuracy: 0.7411 - f1: 0.088 - ETA: 3:47 - loss: 0.5922 - accuracy: 0.7412 - f1: 0.088 - ETA: 3:42 - loss: 0.5914 - accuracy: 0.7421 - f1: 0.091 - ETA: 3:36 - loss: 0.5913 - accuracy: 0.7421 - f1: 0.090 - ETA: 3:30 - loss: 0.5916 - accuracy: 0.7418 - f1: 0.090 - ETA: 3:25 - loss: 0.5924 - accuracy: 0.7415 - f1: 0.089 - ETA: 3:19 - loss: 0.5925 - accuracy: 0.7409 - f1: 0.089 - ETA: 3:13 - loss: 0.5922 - accuracy: 0.7411 - f1: 0.088 - ETA: 3:07 - loss: 0.5921 - accuracy: 0.7417 - f1: 0.090 - ETA: 3:02 - loss: 0.5923 - accuracy: 0.7416 - f1: 0.089 - ETA: 2:56 - loss: 0.5922 - accuracy: 0.7414 - f1: 0.090 - ETA: 2:50 - loss: 0.5927 - accuracy: 0.7410 - f1: 0.089 - ETA: 2:44 - loss: 0.5921 - accuracy: 0.7410 - f1: 0.089 - ETA: 2:39 - loss: 0.5917 - accuracy: 0.7406 - f1: 0.088 - ETA: 2:33 - loss: 0.5917 - accuracy: 0.7406 - f1: 0.089 - ETA: 2:27 - loss: 0.5912 - accuracy: 0.7412 - f1: 0.090 - ETA: 2:22 - loss: 0.5911 - accuracy: 0.7412 - f1: 0.090 - ETA: 2:16 - loss: 0.5920 - accuracy: 0.7409 - f1: 0.089 - ETA: 2:10 - loss: 0.5929 - accuracy: 0.7405 - f1: 0.089 - ETA: 2:04 - loss: 0.5933 - accuracy: 0.7402 - f1: 0.089 - ETA: 1:59 - loss: 0.5936 - accuracy: 0.7396 - f1: 0.089 - ETA: 1:53 - loss: 0.5933 - accuracy: 0.7400 - f1: 0.090 - ETA: 1:47 - loss: 0.5936 - accuracy: 0.7397 - f1: 0.089 - ETA: 1:42 - loss: 0.5937 - accuracy: 0.7396 - f1: 0.090 - ETA: 1:36 - loss: 0.5938 - accuracy: 0.7394 - f1: 0.089 - ETA: 1:30 - loss: 0.5941 - accuracy: 0.7394 - f1: 0.089 - ETA: 1:24 - loss: 0.5948 - accuracy: 0.7390 - f1: 0.089 - ETA: 1:19 - loss: 0.5944 - accuracy: 0.7392 - f1: 0.089 - ETA: 1:13 - loss: 0.5948 - accuracy: 0.7390 - f1: 0.088 - ETA: 1:07 - loss: 0.5958 - accuracy: 0.7386 - f1: 0.088 - ETA: 1:02 - loss: 0.5972 - accuracy: 0.7379 - f1: 0.087 - ETA: 56s - loss: 0.5974 - accuracy: 0.7378 - f1: 0.088 - ETA: 50s - loss: 0.5976 - accuracy: 0.7375 - f1: 0.08 - ETA: 44s - loss: 0.5973 - accuracy: 0.7376 - f1: 0.08 - ETA: 39s - loss: 0.5969 - accuracy: 0.7379 - f1: 0.08 - ETA: 33s - loss: 0.5962 - accuracy: 0.7386 - f1: 0.08 - ETA: 27s - loss: 0.5966 - accuracy: 0.7385 - f1: 0.08 - ETA: 22s - loss: 0.5965 - accuracy: 0.7388 - f1: 0.08 - ETA: 16s - loss: 0.5972 - accuracy: 0.7386 - f1: 0.08 - ETA: 10s - loss: 0.5973 - accuracy: 0.7381 - f1: 0.08 - ETA: 4s - loss: 0.5969 - accuracy: 0.7383 - f1: 0.0882 - 1371s 198ms/sample - loss: 0.5967 - accuracy: 0.7383 - f1: 0.0878 - val_loss: 0.5103 - val_accuracy: 0.7924 - val_f1: 0.0000e+00\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/6940 [==================>...........] - ETA: 20:22 - loss: 0.7487 - accuracy: 0.6250 - f1: 0.0000e+ - ETA: 20:20 - loss: 0.7128 - accuracy: 0.6406 - f1: 0.0000e+ - ETA: 20:14 - loss: 0.6800 - accuracy: 0.6875 - f1: 0.0741   - ETA: 20:07 - loss: 0.6779 - accuracy: 0.6875 - f1: 0.05 - ETA: 20:01 - loss: 0.6335 - accuracy: 0.7063 - f1: 0.08 - ETA: 19:55 - loss: 0.6309 - accuracy: 0.7240 - f1: 0.15 - ETA: 19:49 - loss: 0.6070 - accuracy: 0.7411 - f1: 0.13 - ETA: 19:44 - loss: 0.5775 - accuracy: 0.7578 - f1: 0.11 - ETA: 19:38 - loss: 0.6034 - accuracy: 0.7361 - f1: 0.10 - ETA: 19:32 - loss: 0.6100 - accuracy: 0.7312 - f1: 0.12 - ETA: 19:26 - loss: 0.6023 - accuracy: 0.7358 - f1: 0.11 - ETA: 19:21 - loss: 0.6016 - accuracy: 0.7344 - f1: 0.10 - ETA: 19:15 - loss: 0.5993 - accuracy: 0.7332 - f1: 0.09 - ETA: 19:09 - loss: 0.5935 - accuracy: 0.7433 - f1: 0.08 - ETA: 19:04 - loss: 0.5884 - accuracy: 0.7479 - f1: 0.10 - ETA: 18:58 - loss: 0.5888 - accuracy: 0.7461 - f1: 0.10 - ETA: 18:52 - loss: 0.5928 - accuracy: 0.7445 - f1: 0.09 - ETA: 18:47 - loss: 0.5969 - accuracy: 0.7431 - f1: 0.10 - ETA: 18:41 - loss: 0.6026 - accuracy: 0.7418 - f1: 0.09 - ETA: 18:35 - loss: 0.5965 - accuracy: 0.7453 - f1: 0.10 - ETA: 18:29 - loss: 0.5990 - accuracy: 0.7426 - f1: 0.09 - ETA: 18:24 - loss: 0.5937 - accuracy: 0.7457 - f1: 0.10 - ETA: 18:18 - loss: 0.5894 - accuracy: 0.7459 - f1: 0.10 - ETA: 18:12 - loss: 0.5839 - accuracy: 0.7487 - f1: 0.10 - ETA: 18:07 - loss: 0.5806 - accuracy: 0.7487 - f1: 0.10 - ETA: 18:01 - loss: 0.5760 - accuracy: 0.7512 - f1: 0.09 - ETA: 17:55 - loss: 0.5767 - accuracy: 0.7535 - f1: 0.09 - ETA: 17:50 - loss: 0.5800 - accuracy: 0.7500 - f1: 0.09 - ETA: 17:44 - loss: 0.5787 - accuracy: 0.7489 - f1: 0.09 - ETA: 17:38 - loss: 0.5738 - accuracy: 0.7521 - f1: 0.08 - ETA: 17:33 - loss: 0.5751 - accuracy: 0.7500 - f1: 0.08 - ETA: 17:27 - loss: 0.5715 - accuracy: 0.7529 - f1: 0.09 - ETA: 17:21 - loss: 0.5798 - accuracy: 0.7481 - f1: 0.09 - ETA: 17:16 - loss: 0.5856 - accuracy: 0.7445 - f1: 0.09 - ETA: 17:10 - loss: 0.5885 - accuracy: 0.7429 - f1: 0.08 - ETA: 17:04 - loss: 0.5875 - accuracy: 0.7439 - f1: 0.08 - ETA: 16:59 - loss: 0.5897 - accuracy: 0.7432 - f1: 0.08 - ETA: 16:53 - loss: 0.5963 - accuracy: 0.7410 - f1: 0.08 - ETA: 16:47 - loss: 0.5925 - accuracy: 0.7436 - f1: 0.07 - ETA: 16:42 - loss: 0.5986 - accuracy: 0.7398 - f1: 0.07 - ETA: 16:36 - loss: 0.5998 - accuracy: 0.7378 - f1: 0.07 - ETA: 16:30 - loss: 0.5932 - accuracy: 0.7426 - f1: 0.07 - ETA: 16:25 - loss: 0.5947 - accuracy: 0.7435 - f1: 0.07 - ETA: 16:19 - loss: 0.5966 - accuracy: 0.7429 - f1: 0.08 - ETA: 16:13 - loss: 0.5957 - accuracy: 0.7437 - f1: 0.08 - ETA: 16:08 - loss: 0.5972 - accuracy: 0.7432 - f1: 0.08 - ETA: 16:02 - loss: 0.6003 - accuracy: 0.7407 - f1: 0.08 - ETA: 15:56 - loss: 0.5975 - accuracy: 0.7428 - f1: 0.09 - ETA: 15:50 - loss: 0.5994 - accuracy: 0.7430 - f1: 0.08 - ETA: 15:45 - loss: 0.5995 - accuracy: 0.7437 - f1: 0.09 - ETA: 15:39 - loss: 0.6024 - accuracy: 0.7414 - f1: 0.09 - ETA: 15:33 - loss: 0.6014 - accuracy: 0.7410 - f1: 0.09 - ETA: 15:28 - loss: 0.6025 - accuracy: 0.7400 - f1: 0.08 - ETA: 15:22 - loss: 0.6002 - accuracy: 0.7413 - f1: 0.08 - ETA: 15:16 - loss: 0.6011 - accuracy: 0.7398 - f1: 0.08 - ETA: 15:11 - loss: 0.5983 - accuracy: 0.7416 - f1: 0.08 - ETA: 15:05 - loss: 0.5973 - accuracy: 0.7412 - f1: 0.08 - ETA: 14:59 - loss: 0.5965 - accuracy: 0.7408 - f1: 0.08 - ETA: 14:54 - loss: 0.5957 - accuracy: 0.7421 - f1: 0.08 - ETA: 14:48 - loss: 0.5956 - accuracy: 0.7417 - f1: 0.08 - ETA: 14:42 - loss: 0.5957 - accuracy: 0.7418 - f1: 0.08 - ETA: 14:37 - loss: 0.5956 - accuracy: 0.7414 - f1: 0.08 - ETA: 14:31 - loss: 0.5926 - accuracy: 0.7431 - f1: 0.09 - ETA: 14:25 - loss: 0.5898 - accuracy: 0.7451 - f1: 0.09 - ETA: 14:20 - loss: 0.5893 - accuracy: 0.7442 - f1: 0.09 - ETA: 14:14 - loss: 0.5884 - accuracy: 0.7448 - f1: 0.09 - ETA: 14:08 - loss: 0.5888 - accuracy: 0.7439 - f1: 0.09 - ETA: 14:03 - loss: 0.5903 - accuracy: 0.7426 - f1: 0.09 - ETA: 13:57 - loss: 0.5869 - accuracy: 0.7455 - f1: 0.09 - ETA: 13:51 - loss: 0.5877 - accuracy: 0.7455 - f1: 0.09 - ETA: 13:46 - loss: 0.5870 - accuracy: 0.7460 - f1: 0.10 - ETA: 13:40 - loss: 0.5874 - accuracy: 0.7452 - f1: 0.10 - ETA: 13:34 - loss: 0.5900 - accuracy: 0.7432 - f1: 0.10 - ETA: 13:29 - loss: 0.5888 - accuracy: 0.7432 - f1: 0.09 - ETA: 13:23 - loss: 0.5905 - accuracy: 0.7425 - f1: 0.09 - ETA: 13:17 - loss: 0.5909 - accuracy: 0.7422 - f1: 0.09 - ETA: 13:12 - loss: 0.5907 - accuracy: 0.7419 - f1: 0.09 - ETA: 13:06 - loss: 0.5894 - accuracy: 0.7428 - f1: 0.09 - ETA: 13:00 - loss: 0.5912 - accuracy: 0.7405 - f1: 0.09 - ETA: 12:55 - loss: 0.5894 - accuracy: 0.7418 - f1: 0.09 - ETA: 12:49 - loss: 0.5890 - accuracy: 0.7427 - f1: 0.09 - ETA: 12:43 - loss: 0.5904 - accuracy: 0.7420 - f1: 0.09 - ETA: 12:38 - loss: 0.5886 - accuracy: 0.7432 - f1: 0.09 - ETA: 12:32 - loss: 0.5875 - accuracy: 0.7437 - f1: 0.09 - ETA: 12:26 - loss: 0.5862 - accuracy: 0.7445 - f1: 0.09 - ETA: 12:21 - loss: 0.5872 - accuracy: 0.7445 - f1: 0.09 - ETA: 12:15 - loss: 0.5870 - accuracy: 0.7446 - f1: 0.09 - ETA: 12:09 - loss: 0.5847 - accuracy: 0.7457 - f1: 0.09 - ETA: 12:04 - loss: 0.5872 - accuracy: 0.7447 - f1: 0.09 - ETA: 11:58 - loss: 0.5878 - accuracy: 0.7444 - f1: 0.09 - ETA: 11:52 - loss: 0.5884 - accuracy: 0.7445 - f1: 0.09 - ETA: 11:47 - loss: 0.5883 - accuracy: 0.7442 - f1: 0.09 - ETA: 11:41 - loss: 0.5907 - accuracy: 0.7433 - f1: 0.10 - ETA: 11:35 - loss: 0.5922 - accuracy: 0.7427 - f1: 0.09 - ETA: 11:30 - loss: 0.5924 - accuracy: 0.7428 - f1: 0.09 - ETA: 11:24 - loss: 0.5921 - accuracy: 0.7438 - f1: 0.10 - ETA: 11:18 - loss: 0.5937 - accuracy: 0.7426 - f1: 0.09 - ETA: 11:13 - loss: 0.5929 - accuracy: 0.7436 - f1: 0.09 - ETA: 11:07 - loss: 0.5915 - accuracy: 0.7449 - f1: 0.10 - ETA: 11:01 - loss: 0.5911 - accuracy: 0.7450 - f1: 0.10 - ETA: 10:56 - loss: 0.5908 - accuracy: 0.7450 - f1: 0.10 - ETA: 10:50 - loss: 0.5914 - accuracy: 0.7442 - f1: 0.10 - ETA: 10:44 - loss: 0.5917 - accuracy: 0.7442 - f1: 0.10 - ETA: 10:39 - loss: 0.5917 - accuracy: 0.7437 - f1: 0.10 - ETA: 10:33 - loss: 0.5912 - accuracy: 0.7443 - f1: 0.10 - ETA: 10:27 - loss: 0.5897 - accuracy: 0.7456 - f1: 0.10 - ETA: 10:22 - loss: 0.5899 - accuracy: 0.7459 - f1: 0.10 - ETA: 10:16 - loss: 0.5903 - accuracy: 0.7457 - f1: 0.10 - ETA: 10:10 - loss: 0.5912 - accuracy: 0.7451 - f1: 0.10 - ETA: 10:05 - loss: 0.5908 - accuracy: 0.7446 - f1: 0.10 - ETA: 9:59 - loss: 0.5892 - accuracy: 0.7452 - f1: 0.1046 - ETA: 9:53 - loss: 0.5882 - accuracy: 0.7458 - f1: 0.103 - ETA: 9:48 - loss: 0.5871 - accuracy: 0.7467 - f1: 0.102 - ETA: 9:42 - loss: 0.5868 - accuracy: 0.7467 - f1: 0.101 - ETA: 9:36 - loss: 0.5889 - accuracy: 0.7457 - f1: 0.101 - ETA: 9:31 - loss: 0.5890 - accuracy: 0.7454 - f1: 0.101 - ETA: 9:25 - loss: 0.5890 - accuracy: 0.7457 - f1: 0.100 - ETA: 9:19 - loss: 0.5887 - accuracy: 0.7458 - f1: 0.099 - ETA: 9:14 - loss: 0.5879 - accuracy: 0.7463 - f1: 0.101 - ETA: 9:08 - loss: 0.5887 - accuracy: 0.7464 - f1: 0.102 - ETA: 9:02 - loss: 0.5908 - accuracy: 0.7446 - f1: 0.101 - ETA: 8:57 - loss: 0.5905 - accuracy: 0.7446 - f1: 0.100 - ETA: 8:51 - loss: 0.5896 - accuracy: 0.7449 - f1: 0.099 - ETA: 8:46 - loss: 0.5899 - accuracy: 0.7442 - f1: 0.100 - ETA: 8:40 - loss: 0.5897 - accuracy: 0.7445 - f1: 0.101 - ETA: 8:34 - loss: 0.5901 - accuracy: 0.7445 - f1: 0.101 - ETA: 8:29 - loss: 0.5905 - accuracy: 0.7443 - f1: 0.100 - ETA: 8:23 - loss: 0.5904 - accuracy: 0.7441 - f1: 0.100 - ETA: 8:18 - loss: 0.5903 - accuracy: 0.7439 - f1: 0.099 - ETA: 8:12 - loss: 0.5899 - accuracy: 0.7440 - f1: 0.098 - ETA: 8:06 - loss: 0.5893 - accuracy: 0.7443 - f1: 0.097 - ETA: 8:01 - loss: 0.5890 - accuracy: 0.7446 - f1: 0.097 - ETA: 7:55 - loss: 0.5886 - accuracy: 0.7448 - f1: 0.098 - ETA: 7:49 - loss: 0.5890 - accuracy: 0.7446 - f1: 0.097 - ETA: 7:44 - loss: 0.5890 - accuracy: 0.7440 - f1: 0.096 - ETA: 7:38 - loss: 0.5880 - accuracy: 0.7445 - f1: 0.095 - ETA: 7:33 - loss: 0.5877 - accuracy: 0.7450 - f1: 0.095 - ETA: 7:27 - loss: 0.5883 - accuracy: 0.7446 - f1: 0.094 - ETA: 7:21 - loss: 0.5885 - accuracy: 0.7444 - f1: 0.093 - ETA: 7:16 - loss: 0.5891 - accuracy: 0.7446 - f1: 0.093 - ETA: 7:10 - loss: 0.5893 - accuracy: 0.7447 - f1: 0.0925"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - ETA: 7:04 - loss: 0.5896 - accuracy: 0.7449 - f1: 0.091 - ETA: 6:59 - loss: 0.5902 - accuracy: 0.7445 - f1: 0.092 - ETA: 6:53 - loss: 0.5902 - accuracy: 0.7448 - f1: 0.091 - ETA: 6:47 - loss: 0.5906 - accuracy: 0.7442 - f1: 0.091 - ETA: 6:42 - loss: 0.5904 - accuracy: 0.7444 - f1: 0.090 - ETA: 6:36 - loss: 0.5903 - accuracy: 0.7440 - f1: 0.091 - ETA: 6:30 - loss: 0.5895 - accuracy: 0.7443 - f1: 0.090 - ETA: 6:25 - loss: 0.5888 - accuracy: 0.7445 - f1: 0.091 - ETA: 6:19 - loss: 0.5883 - accuracy: 0.7448 - f1: 0.090 - ETA: 6:13 - loss: 0.5882 - accuracy: 0.7442 - f1: 0.091 - ETA: 6:08 - loss: 0.5884 - accuracy: 0.7440 - f1: 0.090 - ETA: 6:02 - loss: 0.5880 - accuracy: 0.7445 - f1: 0.089 - ETA: 5:56 - loss: 0.5880 - accuracy: 0.7447 - f1: 0.089 - ETA: 5:51 - loss: 0.5879 - accuracy: 0.7450 - f1: 0.088 - ETA: 5:45 - loss: 0.5879 - accuracy: 0.7446 - f1: 0.088 - ETA: 5:39 - loss: 0.5888 - accuracy: 0.7446 - f1: 0.087 - ETA: 5:34 - loss: 0.5890 - accuracy: 0.7443 - f1: 0.087 - ETA: 5:28 - loss: 0.5906 - accuracy: 0.7433 - f1: 0.086 - ETA: 5:22 - loss: 0.5900 - accuracy: 0.7439 - f1: 0.088 - ETA: 5:17 - loss: 0.5895 - accuracy: 0.7444 - f1: 0.089 - ETA: 5:11 - loss: 0.5900 - accuracy: 0.7438 - f1: 0.089 - ETA: 5:05 - loss: 0.5896 - accuracy: 0.7439 - f1: 0.088 - ETA: 5:00 - loss: 0.5903 - accuracy: 0.7431 - f1: 0.089 - ETA: 4:54 - loss: 0.5901 - accuracy: 0.7432 - f1: 0.088 - ETA: 4:48 - loss: 0.5911 - accuracy: 0.7430 - f1: 0.087 - ETA: 4:43 - loss: 0.5919 - accuracy: 0.7427 - f1: 0.087 - ETA: 4:37 - loss: 0.5919 - accuracy: 0.7429 - f1: 0.086 - ETA: 4:31 - loss: 0.5916 - accuracy: 0.7435 - f1: 0.086 - ETA: 4:26 - loss: 0.5911 - accuracy: 0.7437 - f1: 0.085 - ETA: 4:20 - loss: 0.5909 - accuracy: 0.7443 - f1: 0.087 - ETA: 4:14 - loss: 0.5923 - accuracy: 0.7436 - f1: 0.086 - ETA: 4:09 - loss: 0.5935 - accuracy: 0.7431 - f1: 0.086 - ETA: 4:03 - loss: 0.5950 - accuracy: 0.7421 - f1: 0.085 - ETA: 3:57 - loss: 0.5954 - accuracy: 0.7414 - f1: 0.085 - ETA: 3:51 - loss: 0.5957 - accuracy: 0.7408 - f1: 0.084 - ETA: 3:46 - loss: 0.5956 - accuracy: 0.7410 - f1: 0.084 - ETA: 3:40 - loss: 0.5946 - accuracy: 0.7417 - f1: 0.083 - ETA: 3:34 - loss: 0.5937 - accuracy: 0.7425 - f1: 0.085 - ETA: 3:29 - loss: 0.5924 - accuracy: 0.7436 - f1: 0.088 - ETA: 3:23 - loss: 0.5923 - accuracy: 0.7434 - f1: 0.087 - ETA: 3:17 - loss: 0.5930 - accuracy: 0.7430 - f1: 0.087 - ETA: 3:12 - loss: 0.5935 - accuracy: 0.7423 - f1: 0.086 - ETA: 3:06 - loss: 0.5935 - accuracy: 0.7425 - f1: 0.086 - ETA: 3:00 - loss: 0.5924 - accuracy: 0.7434 - f1: 0.085 - ETA: 2:55 - loss: 0.5913 - accuracy: 0.7440 - f1: 0.085 - ETA: 2:49 - loss: 0.5913 - accuracy: 0.7438 - f1: 0.084 - ETA: 2:43 - loss: 0.5912 - accuracy: 0.7434 - f1: 0.084 - ETA: 2:38 - loss: 0.5910 - accuracy: 0.7432 - f1: 0.084 - ETA: 2:32 - loss: 0.5917 - accuracy: 0.7428 - f1: 0.083 - ETA: 2:26 - loss: 0.5913 - accuracy: 0.7426 - f1: 0.083 - ETA: 2:21 - loss: 0.5917 - accuracy: 0.7424 - f1: 0.082 - ETA: 2:15 - loss: 0.5919 - accuracy: 0.7422 - f1: 0.083 - ETA: 2:09 - loss: 0.5922 - accuracy: 0.7418 - f1: 0.082 - ETA: 2:04 - loss: 0.5923 - accuracy: 0.7417 - f1: 0.083 - ETA: 1:58 - loss: 0.5930 - accuracy: 0.7412 - f1: 0.082 - ETA: 1:52 - loss: 0.5928 - accuracy: 0.7413 - f1: 0.082 - ETA: 1:47 - loss: 0.5929 - accuracy: 0.7412 - f1: 0.082 - ETA: 1:41 - loss: 0.5925 - accuracy: 0.7417 - f1: 0.081 - ETA: 1:35 - loss: 0.5938 - accuracy: 0.7408 - f1: 0.081 - ETA: 1:30 - loss: 0.5936 - accuracy: 0.7411 - f1: 0.080 - ETA: 1:24 - loss: 0.5946 - accuracy: 0.7404 - f1: 0.081 - ETA: 1:18 - loss: 0.5946 - accuracy: 0.7405 - f1: 0.081 - ETA: 1:13 - loss: 0.5940 - accuracy: 0.7407 - f1: 0.082 - ETA: 1:07 - loss: 0.5932 - accuracy: 0.7410 - f1: 0.081 - ETA: 1:01 - loss: 0.5938 - accuracy: 0.7409 - f1: 0.081 - ETA: 56s - loss: 0.5942 - accuracy: 0.7408 - f1: 0.082 - ETA: 50s - loss: 0.5949 - accuracy: 0.7404 - f1: 0.08 - ETA: 44s - loss: 0.5942 - accuracy: 0.7409 - f1: 0.08 - ETA: 39s - loss: 0.5946 - accuracy: 0.7409 - f1: 0.08 - ETA: 33s - loss: 0.5943 - accuracy: 0.7413 - f1: 0.08 - ETA: 27s - loss: 0.5945 - accuracy: 0.7410 - f1: 0.08 - ETA: 21s - loss: 0.5942 - accuracy: 0.7412 - f1: 0.08 - ETA: 16s - loss: 0.5945 - accuracy: 0.7409 - f1: 0.08 - ETA: 10s - loss: 0.5946 - accuracy: 0.7408 - f1: 0.08 - ETA: 4s - loss: 0.5942 - accuracy: 0.7415 - f1: 0.0811 - 1361s 196ms/sample - loss: 0.5944 - accuracy: 0.7415 - f1: 0.0817 - val_loss: 0.5102 - val_accuracy: 0.7924 - val_f1: 0.0000e+00\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/6940 [==================>...........] - ETA: 20:43 - loss: 0.4527 - accuracy: 0.8438 - f1: 0.28 - ETA: 20:38 - loss: 0.4684 - accuracy: 0.8594 - f1: 0.14 - ETA: 20:33 - loss: 0.5409 - accuracy: 0.8021 - f1: 0.15 - ETA: 20:28 - loss: 0.5648 - accuracy: 0.7812 - f1: 0.11 - ETA: 20:22 - loss: 0.5912 - accuracy: 0.7563 - f1: 0.12 - ETA: 20:16 - loss: 0.6021 - accuracy: 0.7500 - f1: 0.10 - ETA: 20:09 - loss: 0.6332 - accuracy: 0.7366 - f1: 0.10 - ETA: 20:02 - loss: 0.6094 - accuracy: 0.7461 - f1: 0.09 - ETA: 19:55 - loss: 0.5876 - accuracy: 0.7569 - f1: 0.08 - ETA: 19:48 - loss: 0.6003 - accuracy: 0.7500 - f1: 0.07 - ETA: 19:42 - loss: 0.6053 - accuracy: 0.7472 - f1: 0.06 - ETA: 19:36 - loss: 0.6102 - accuracy: 0.7474 - f1: 0.06 - ETA: 19:29 - loss: 0.5993 - accuracy: 0.7500 - f1: 0.05 - ETA: 19:23 - loss: 0.5984 - accuracy: 0.7455 - f1: 0.05 - ETA: 19:17 - loss: 0.5952 - accuracy: 0.7500 - f1: 0.05 - ETA: 19:11 - loss: 0.5864 - accuracy: 0.7559 - f1: 0.04 - ETA: 19:05 - loss: 0.6045 - accuracy: 0.7463 - f1: 0.04 - ETA: 18:59 - loss: 0.5985 - accuracy: 0.7500 - f1: 0.04 - ETA: 18:53 - loss: 0.6035 - accuracy: 0.7500 - f1: 0.04 - ETA: 18:47 - loss: 0.5980 - accuracy: 0.7547 - f1: 0.03 - ETA: 18:42 - loss: 0.6019 - accuracy: 0.7530 - f1: 0.04 - ETA: 18:36 - loss: 0.5997 - accuracy: 0.7557 - f1: 0.04 - ETA: 18:30 - loss: 0.5924 - accuracy: 0.7609 - f1: 0.05 - ETA: 18:24 - loss: 0.5935 - accuracy: 0.7591 - f1: 0.05 - ETA: 18:18 - loss: 0.5892 - accuracy: 0.7588 - f1: 0.05 - ETA: 18:12 - loss: 0.5902 - accuracy: 0.7572 - f1: 0.04 - ETA: 18:06 - loss: 0.5898 - accuracy: 0.7569 - f1: 0.05 - ETA: 18:01 - loss: 0.5861 - accuracy: 0.7589 - f1: 0.05 - ETA: 17:55 - loss: 0.5888 - accuracy: 0.7575 - f1: 0.05 - ETA: 17:49 - loss: 0.5849 - accuracy: 0.7594 - f1: 0.05 - ETA: 17:43 - loss: 0.5839 - accuracy: 0.7601 - f1: 0.05 - ETA: 17:37 - loss: 0.5843 - accuracy: 0.7598 - f1: 0.05 - ETA: 17:32 - loss: 0.5842 - accuracy: 0.7614 - f1: 0.05 - ETA: 17:26 - loss: 0.5889 - accuracy: 0.7592 - f1: 0.06 - ETA: 17:20 - loss: 0.5864 - accuracy: 0.7598 - f1: 0.05 - ETA: 17:15 - loss: 0.5863 - accuracy: 0.7604 - f1: 0.05 - ETA: 17:09 - loss: 0.5823 - accuracy: 0.7635 - f1: 0.06 - ETA: 17:03 - loss: 0.5820 - accuracy: 0.7640 - f1: 0.06 - ETA: 16:58 - loss: 0.5824 - accuracy: 0.7628 - f1: 0.06 - ETA: 16:52 - loss: 0.5840 - accuracy: 0.7625 - f1: 0.06 - ETA: 16:46 - loss: 0.5884 - accuracy: 0.7591 - f1: 0.07 - ETA: 16:41 - loss: 0.5940 - accuracy: 0.7560 - f1: 0.06 - ETA: 16:35 - loss: 0.5933 - accuracy: 0.7573 - f1: 0.06 - ETA: 16:29 - loss: 0.5921 - accuracy: 0.7585 - f1: 0.06 - ETA: 16:23 - loss: 0.5872 - accuracy: 0.7618 - f1: 0.06 - ETA: 16:18 - loss: 0.5869 - accuracy: 0.7615 - f1: 0.06 - ETA: 16:12 - loss: 0.5880 - accuracy: 0.7613 - f1: 0.06 - ETA: 16:06 - loss: 0.5889 - accuracy: 0.7604 - f1: 0.06 - ETA: 16:00 - loss: 0.5880 - accuracy: 0.7608 - f1: 0.05 - ETA: 15:55 - loss: 0.5876 - accuracy: 0.7606 - f1: 0.06 - ETA: 15:49 - loss: 0.5871 - accuracy: 0.7616 - f1: 0.06 - ETA: 15:43 - loss: 0.5898 - accuracy: 0.7590 - f1: 0.05 - ETA: 15:37 - loss: 0.5894 - accuracy: 0.7588 - f1: 0.05 - ETA: 15:31 - loss: 0.5874 - accuracy: 0.7604 - f1: 0.06 - ETA: 15:26 - loss: 0.5870 - accuracy: 0.7602 - f1: 0.06 - ETA: 15:20 - loss: 0.5879 - accuracy: 0.7595 - f1: 0.06 - ETA: 15:14 - loss: 0.5844 - accuracy: 0.7615 - f1: 0.07 - ETA: 15:09 - loss: 0.5825 - accuracy: 0.7629 - f1: 0.07 - ETA: 15:03 - loss: 0.5851 - accuracy: 0.7606 - f1: 0.06 - ETA: 14:57 - loss: 0.5841 - accuracy: 0.7604 - f1: 0.07 - ETA: 14:51 - loss: 0.5832 - accuracy: 0.7597 - f1: 0.07 - ETA: 14:46 - loss: 0.5831 - accuracy: 0.7601 - f1: 0.07 - ETA: 14:40 - loss: 0.5795 - accuracy: 0.7629 - f1: 0.07 - ETA: 14:34 - loss: 0.5794 - accuracy: 0.7627 - f1: 0.06 - ETA: 14:29 - loss: 0.5795 - accuracy: 0.7615 - f1: 0.06 - ETA: 14:23 - loss: 0.5787 - accuracy: 0.7614 - f1: 0.07 - ETA: 14:17 - loss: 0.5813 - accuracy: 0.7603 - f1: 0.06 - ETA: 14:11 - loss: 0.5821 - accuracy: 0.7601 - f1: 0.06 - ETA: 14:06 - loss: 0.5846 - accuracy: 0.7582 - f1: 0.06 - ETA: 14:00 - loss: 0.5853 - accuracy: 0.7585 - f1: 0.06 - ETA: 13:54 - loss: 0.5876 - accuracy: 0.7575 - f1: 0.07 - ETA: 13:49 - loss: 0.5880 - accuracy: 0.7578 - f1: 0.07 - ETA: 13:43 - loss: 0.5902 - accuracy: 0.7564 - f1: 0.07 - ETA: 13:37 - loss: 0.5892 - accuracy: 0.7568 - f1: 0.07 - ETA: 13:31 - loss: 0.5862 - accuracy: 0.7583 - f1: 0.07 - ETA: 13:26 - loss: 0.5846 - accuracy: 0.7599 - f1: 0.07 - ETA: 13:20 - loss: 0.5835 - accuracy: 0.7606 - f1: 0.07 - ETA: 13:14 - loss: 0.5871 - accuracy: 0.7580 - f1: 0.07 - ETA: 13:09 - loss: 0.5907 - accuracy: 0.7555 - f1: 0.07 - ETA: 13:03 - loss: 0.5909 - accuracy: 0.7555 - f1: 0.07 - ETA: 12:57 - loss: 0.5911 - accuracy: 0.7554 - f1: 0.07 - ETA: 12:51 - loss: 0.5888 - accuracy: 0.7565 - f1: 0.07 - ETA: 12:46 - loss: 0.5885 - accuracy: 0.7564 - f1: 0.07 - ETA: 12:40 - loss: 0.5888 - accuracy: 0.7563 - f1: 0.07 - ETA: 12:34 - loss: 0.5900 - accuracy: 0.7548 - f1: 0.07 - ETA: 12:29 - loss: 0.5901 - accuracy: 0.7544 - f1: 0.07 - ETA: 12:23 - loss: 0.5891 - accuracy: 0.7547 - f1: 0.07 - ETA: 12:17 - loss: 0.5913 - accuracy: 0.7532 - f1: 0.07 - ETA: 12:11 - loss: 0.5918 - accuracy: 0.7532 - f1: 0.07 - ETA: 12:06 - loss: 0.5899 - accuracy: 0.7545 - f1: 0.07 - ETA: 12:00 - loss: 0.5897 - accuracy: 0.7545 - f1: 0.07 - ETA: 11:54 - loss: 0.5919 - accuracy: 0.7537 - f1: 0.07 - ETA: 11:49 - loss: 0.5915 - accuracy: 0.7537 - f1: 0.07 - ETA: 11:43 - loss: 0.5922 - accuracy: 0.7533 - f1: 0.07 - ETA: 11:37 - loss: 0.5941 - accuracy: 0.7523 - f1: 0.07 - ETA: 11:31 - loss: 0.5935 - accuracy: 0.7526 - f1: 0.07 - ETA: 11:26 - loss: 0.5922 - accuracy: 0.7532 - f1: 0.07 - ETA: 11:20 - loss: 0.5922 - accuracy: 0.7535 - f1: 0.07 - ETA: 11:14 - loss: 0.5935 - accuracy: 0.7532 - f1: 0.07 - ETA: 11:08 - loss: 0.5922 - accuracy: 0.7541 - f1: 0.07 - ETA: 11:03 - loss: 0.5930 - accuracy: 0.7534 - f1: 0.07 - ETA: 10:57 - loss: 0.5924 - accuracy: 0.7537 - f1: 0.08 - ETA: 10:51 - loss: 0.5931 - accuracy: 0.7533 - f1: 0.08 - ETA: 10:46 - loss: 0.5935 - accuracy: 0.7533 - f1: 0.08 - ETA: 10:40 - loss: 0.5921 - accuracy: 0.7542 - f1: 0.08 - ETA: 10:34 - loss: 0.5924 - accuracy: 0.7541 - f1: 0.08 - ETA: 10:28 - loss: 0.5917 - accuracy: 0.7544 - f1: 0.08 - ETA: 10:23 - loss: 0.5927 - accuracy: 0.7538 - f1: 0.08 - ETA: 10:17 - loss: 0.5906 - accuracy: 0.7554 - f1: 0.08 - ETA: 10:11 - loss: 0.5906 - accuracy: 0.7560 - f1: 0.08 - ETA: 10:06 - loss: 0.5910 - accuracy: 0.7556 - f1: 0.08 - ETA: 10:00 - loss: 0.5912 - accuracy: 0.7556 - f1: 0.08 - ETA: 9:54 - loss: 0.5901 - accuracy: 0.7566 - f1: 0.0895 - ETA: 9:48 - loss: 0.5883 - accuracy: 0.7574 - f1: 0.091 - ETA: 9:43 - loss: 0.5885 - accuracy: 0.7571 - f1: 0.090 - ETA: 9:37 - loss: 0.5874 - accuracy: 0.7578 - f1: 0.092 - ETA: 9:31 - loss: 0.5878 - accuracy: 0.7569 - f1: 0.091 - ETA: 9:26 - loss: 0.5887 - accuracy: 0.7561 - f1: 0.090 - ETA: 9:20 - loss: 0.5885 - accuracy: 0.7560 - f1: 0.089 - ETA: 9:14 - loss: 0.5876 - accuracy: 0.7568 - f1: 0.091 - ETA: 9:08 - loss: 0.5877 - accuracy: 0.7565 - f1: 0.090 - ETA: 9:03 - loss: 0.5877 - accuracy: 0.7567 - f1: 0.091 - ETA: 8:57 - loss: 0.5868 - accuracy: 0.7571 - f1: 0.093 - ETA: 8:51 - loss: 0.5858 - accuracy: 0.7583 - f1: 0.092 - ETA: 8:45 - loss: 0.5861 - accuracy: 0.7580 - f1: 0.091 - ETA: 8:40 - loss: 0.5870 - accuracy: 0.7574 - f1: 0.092 - ETA: 8:34 - loss: 0.5871 - accuracy: 0.7569 - f1: 0.091 - ETA: 8:28 - loss: 0.5874 - accuracy: 0.7571 - f1: 0.090 - ETA: 8:23 - loss: 0.5874 - accuracy: 0.7575 - f1: 0.091 - ETA: 8:17 - loss: 0.5876 - accuracy: 0.7572 - f1: 0.091 - ETA: 8:11 - loss: 0.5887 - accuracy: 0.7564 - f1: 0.090 - ETA: 8:05 - loss: 0.5887 - accuracy: 0.7566 - f1: 0.089 - ETA: 8:00 - loss: 0.5902 - accuracy: 0.7554 - f1: 0.090 - ETA: 7:54 - loss: 0.5895 - accuracy: 0.7556 - f1: 0.089 - ETA: 7:48 - loss: 0.5879 - accuracy: 0.7567 - f1: 0.088 - ETA: 7:42 - loss: 0.5877 - accuracy: 0.7567 - f1: 0.088 - ETA: 7:37 - loss: 0.5877 - accuracy: 0.7566 - f1: 0.087 - ETA: 7:31 - loss: 0.5886 - accuracy: 0.7559 - f1: 0.086 - ETA: 7:25 - loss: 0.5890 - accuracy: 0.7556 - f1: 0.086 - ETA: 7:20 - loss: 0.5879 - accuracy: 0.7565 - f1: 0.088 - ETA: 7:14 - loss: 0.5877 - accuracy: 0.7569 - f1: 0.08746940/6940 [==============================] - ETA: 7:08 - loss: 0.5866 - accuracy: 0.7573 - f1: 0.088 - ETA: 7:02 - loss: 0.5865 - accuracy: 0.7572 - f1: 0.089 - ETA: 6:57 - loss: 0.5878 - accuracy: 0.7563 - f1: 0.088 - ETA: 6:51 - loss: 0.5884 - accuracy: 0.7554 - f1: 0.088 - ETA: 6:45 - loss: 0.5890 - accuracy: 0.7549 - f1: 0.087 - ETA: 6:40 - loss: 0.5891 - accuracy: 0.7545 - f1: 0.086 - ETA: 6:34 - loss: 0.5900 - accuracy: 0.7536 - f1: 0.087 - ETA: 6:28 - loss: 0.5905 - accuracy: 0.7536 - f1: 0.088 - ETA: 6:22 - loss: 0.5906 - accuracy: 0.7533 - f1: 0.087 - ETA: 6:17 - loss: 0.5893 - accuracy: 0.7537 - f1: 0.089 - ETA: 6:11 - loss: 0.5901 - accuracy: 0.7535 - f1: 0.088 - ETA: 6:05 - loss: 0.5890 - accuracy: 0.7541 - f1: 0.088 - ETA: 5:59 - loss: 0.5907 - accuracy: 0.7526 - f1: 0.087 - ETA: 5:54 - loss: 0.5916 - accuracy: 0.7516 - f1: 0.087 - ETA: 5:48 - loss: 0.5914 - accuracy: 0.7516 - f1: 0.086 - ETA: 5:42 - loss: 0.5910 - accuracy: 0.7520 - f1: 0.087 - ETA: 5:37 - loss: 0.5900 - accuracy: 0.7526 - f1: 0.087 - ETA: 5:31 - loss: 0.5908 - accuracy: 0.7516 - f1: 0.087 - ETA: 5:25 - loss: 0.5903 - accuracy: 0.7518 - f1: 0.086 - ETA: 5:19 - loss: 0.5902 - accuracy: 0.7517 - f1: 0.086 - ETA: 5:14 - loss: 0.5911 - accuracy: 0.7515 - f1: 0.085 - ETA: 5:08 - loss: 0.5914 - accuracy: 0.7512 - f1: 0.085 - ETA: 5:02 - loss: 0.5911 - accuracy: 0.7511 - f1: 0.086 - ETA: 4:57 - loss: 0.5909 - accuracy: 0.7509 - f1: 0.086 - ETA: 4:51 - loss: 0.5901 - accuracy: 0.7513 - f1: 0.087 - ETA: 4:45 - loss: 0.5896 - accuracy: 0.7517 - f1: 0.087 - ETA: 4:39 - loss: 0.5903 - accuracy: 0.7511 - f1: 0.086 - ETA: 4:34 - loss: 0.5909 - accuracy: 0.7509 - f1: 0.087 - ETA: 4:28 - loss: 0.5905 - accuracy: 0.7513 - f1: 0.086 - ETA: 4:22 - loss: 0.5911 - accuracy: 0.7507 - f1: 0.086 - ETA: 4:16 - loss: 0.5913 - accuracy: 0.7507 - f1: 0.087 - ETA: 4:11 - loss: 0.5914 - accuracy: 0.7500 - f1: 0.087 - ETA: 4:05 - loss: 0.5912 - accuracy: 0.7502 - f1: 0.086 - ETA: 3:59 - loss: 0.5924 - accuracy: 0.7493 - f1: 0.086 - ETA: 3:54 - loss: 0.5909 - accuracy: 0.7498 - f1: 0.085 - ETA: 3:48 - loss: 0.5915 - accuracy: 0.7496 - f1: 0.085 - ETA: 3:42 - loss: 0.5913 - accuracy: 0.7498 - f1: 0.084 - ETA: 3:36 - loss: 0.5924 - accuracy: 0.7488 - f1: 0.084 - ETA: 3:31 - loss: 0.5934 - accuracy: 0.7486 - f1: 0.086 - ETA: 3:25 - loss: 0.5923 - accuracy: 0.7491 - f1: 0.085 - ETA: 3:19 - loss: 0.5923 - accuracy: 0.7490 - f1: 0.086 - ETA: 3:13 - loss: 0.5929 - accuracy: 0.7485 - f1: 0.086 - ETA: 3:08 - loss: 0.5935 - accuracy: 0.7481 - f1: 0.086 - ETA: 3:02 - loss: 0.5942 - accuracy: 0.7471 - f1: 0.085 - ETA: 2:56 - loss: 0.5937 - accuracy: 0.7471 - f1: 0.086 - ETA: 2:51 - loss: 0.5937 - accuracy: 0.7472 - f1: 0.085 - ETA: 2:45 - loss: 0.5932 - accuracy: 0.7473 - f1: 0.086 - ETA: 2:39 - loss: 0.5936 - accuracy: 0.7470 - f1: 0.086 - ETA: 2:33 - loss: 0.5932 - accuracy: 0.7474 - f1: 0.087 - ETA: 2:28 - loss: 0.5940 - accuracy: 0.7471 - f1: 0.087 - ETA: 2:22 - loss: 0.5942 - accuracy: 0.7471 - f1: 0.086 - ETA: 2:16 - loss: 0.5945 - accuracy: 0.7468 - f1: 0.086 - ETA: 2:10 - loss: 0.5947 - accuracy: 0.7465 - f1: 0.086 - ETA: 2:05 - loss: 0.5940 - accuracy: 0.7468 - f1: 0.088 - ETA: 1:59 - loss: 0.5940 - accuracy: 0.7468 - f1: 0.089 - ETA: 1:53 - loss: 0.5934 - accuracy: 0.7471 - f1: 0.089 - ETA: 1:48 - loss: 0.5945 - accuracy: 0.7462 - f1: 0.088 - ETA: 1:42 - loss: 0.5946 - accuracy: 0.7465 - f1: 0.088 - ETA: 1:36 - loss: 0.5943 - accuracy: 0.7469 - f1: 0.089 - ETA: 1:30 - loss: 0.5949 - accuracy: 0.7464 - f1: 0.090 - ETA: 1:25 - loss: 0.5942 - accuracy: 0.7469 - f1: 0.089 - ETA: 1:19 - loss: 0.5947 - accuracy: 0.7466 - f1: 0.089 - ETA: 1:13 - loss: 0.5940 - accuracy: 0.7466 - f1: 0.088 - ETA: 1:07 - loss: 0.5938 - accuracy: 0.7470 - f1: 0.088 - ETA: 1:02 - loss: 0.5931 - accuracy: 0.7474 - f1: 0.087 - ETA: 56s - loss: 0.5929 - accuracy: 0.7476 - f1: 0.087 - ETA: 50s - loss: 0.5943 - accuracy: 0.7468 - f1: 0.08 - ETA: 45s - loss: 0.5940 - accuracy: 0.7467 - f1: 0.08 - ETA: 39s - loss: 0.5940 - accuracy: 0.7469 - f1: 0.08 - ETA: 33s - loss: 0.5941 - accuracy: 0.7467 - f1: 0.08 - ETA: 27s - loss: 0.5938 - accuracy: 0.7468 - f1: 0.08 - ETA: 22s - loss: 0.5937 - accuracy: 0.7468 - f1: 0.08 - ETA: 16s - loss: 0.5934 - accuracy: 0.7468 - f1: 0.08 - ETA: 10s - loss: 0.5932 - accuracy: 0.7468 - f1: 0.08 - ETA: 5s - loss: 0.5938 - accuracy: 0.7464 - f1: 0.0849 - 1372s 198ms/sample - loss: 0.5937 - accuracy: 0.7465 - f1: 0.0845 - val_loss: 0.5100 - val_accuracy: 0.7924 - val_f1: 0.0000e+00\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/6940 [==================>...........] - ETA: 20:38 - loss: 0.7203 - accuracy: 0.6562 - f1: 0.15 - ETA: 20:32 - loss: 0.7198 - accuracy: 0.6250 - f1: 0.07 - ETA: 20:27 - loss: 0.6829 - accuracy: 0.6562 - f1: 0.05 - ETA: 20:21 - loss: 0.6392 - accuracy: 0.6953 - f1: 0.10 - ETA: 20:15 - loss: 0.6741 - accuracy: 0.6938 - f1: 0.11 - ETA: 20:09 - loss: 0.6378 - accuracy: 0.7188 - f1: 0.14 - ETA: 20:03 - loss: 0.6289 - accuracy: 0.7232 - f1: 0.16 - ETA: 19:57 - loss: 0.6360 - accuracy: 0.7148 - f1: 0.14 - ETA: 19:52 - loss: 0.6230 - accuracy: 0.7222 - f1: 0.15 - ETA: 19:46 - loss: 0.6449 - accuracy: 0.7125 - f1: 0.14 - ETA: 19:40 - loss: 0.6491 - accuracy: 0.7102 - f1: 0.12 - ETA: 19:35 - loss: 0.6395 - accuracy: 0.7161 - f1: 0.14 - ETA: 19:29 - loss: 0.6373 - accuracy: 0.7212 - f1: 0.15 - ETA: 19:23 - loss: 0.6398 - accuracy: 0.7188 - f1: 0.14 - ETA: 19:17 - loss: 0.6359 - accuracy: 0.7188 - f1: 0.13 - ETA: 19:11 - loss: 0.6381 - accuracy: 0.7168 - f1: 0.12 - ETA: 19:05 - loss: 0.6312 - accuracy: 0.7206 - f1: 0.11 - ETA: 18:59 - loss: 0.6263 - accuracy: 0.7205 - f1: 0.11 - ETA: 18:54 - loss: 0.6213 - accuracy: 0.7253 - f1: 0.11 - ETA: 18:48 - loss: 0.6231 - accuracy: 0.7266 - f1: 0.11 - ETA: 18:42 - loss: 0.6140 - accuracy: 0.7321 - f1: 0.10 - ETA: 18:36 - loss: 0.6178 - accuracy: 0.7315 - f1: 0.10 - ETA: 18:31 - loss: 0.6224 - accuracy: 0.7255 - f1: 0.09 - ETA: 18:25 - loss: 0.6124 - accuracy: 0.7292 - f1: 0.09 - ETA: 18:19 - loss: 0.6087 - accuracy: 0.7312 - f1: 0.08 - ETA: 18:14 - loss: 0.6068 - accuracy: 0.7296 - f1: 0.08 - ETA: 18:08 - loss: 0.6022 - accuracy: 0.7303 - f1: 0.08 - ETA: 18:02 - loss: 0.6038 - accuracy: 0.7321 - f1: 0.08 - ETA: 17:56 - loss: 0.6026 - accuracy: 0.7349 - f1: 0.08 - ETA: 17:51 - loss: 0.6005 - accuracy: 0.7375 - f1: 0.09 - ETA: 17:45 - loss: 0.5995 - accuracy: 0.7379 - f1: 0.08 - ETA: 17:39 - loss: 0.5916 - accuracy: 0.7432 - f1: 0.08 - ETA: 17:33 - loss: 0.5956 - accuracy: 0.7405 - f1: 0.08 - ETA: 17:27 - loss: 0.5957 - accuracy: 0.7408 - f1: 0.08 - ETA: 17:22 - loss: 0.5930 - accuracy: 0.7402 - f1: 0.08 - ETA: 17:16 - loss: 0.5911 - accuracy: 0.7431 - f1: 0.08 - ETA: 17:10 - loss: 0.5864 - accuracy: 0.7458 - f1: 0.08 - ETA: 17:04 - loss: 0.5852 - accuracy: 0.7459 - f1: 0.08 - ETA: 16:59 - loss: 0.5845 - accuracy: 0.7444 - f1: 0.09 - ETA: 16:53 - loss: 0.5797 - accuracy: 0.7484 - f1: 0.08 - ETA: 16:47 - loss: 0.5783 - accuracy: 0.7500 - f1: 0.08 - ETA: 16:41 - loss: 0.5768 - accuracy: 0.7507 - f1: 0.08 - ETA: 16:36 - loss: 0.5756 - accuracy: 0.7515 - f1: 0.08 - ETA: 16:30 - loss: 0.5709 - accuracy: 0.7543 - f1: 0.08 - ETA: 16:24 - loss: 0.5683 - accuracy: 0.7569 - f1: 0.08 - ETA: 16:18 - loss: 0.5657 - accuracy: 0.7582 - f1: 0.08 - ETA: 16:13 - loss: 0.5627 - accuracy: 0.7606 - f1: 0.08 - ETA: 16:07 - loss: 0.5652 - accuracy: 0.7591 - f1: 0.08 - ETA: 16:01 - loss: 0.5663 - accuracy: 0.7589 - f1: 0.07 - ETA: 15:55 - loss: 0.5651 - accuracy: 0.7600 - f1: 0.07 - ETA: 15:50 - loss: 0.5653 - accuracy: 0.7604 - f1: 0.07 - ETA: 15:44 - loss: 0.5650 - accuracy: 0.7620 - f1: 0.07 - ETA: 15:38 - loss: 0.5700 - accuracy: 0.7612 - f1: 0.07 - ETA: 15:32 - loss: 0.5728 - accuracy: 0.7598 - f1: 0.07 - ETA: 15:27 - loss: 0.5734 - accuracy: 0.7597 - f1: 0.07 - ETA: 15:21 - loss: 0.5718 - accuracy: 0.7606 - f1: 0.07 - ETA: 15:15 - loss: 0.5701 - accuracy: 0.7621 - f1: 0.07 - ETA: 15:10 - loss: 0.5737 - accuracy: 0.7602 - f1: 0.07 - ETA: 15:04 - loss: 0.5724 - accuracy: 0.7606 - f1: 0.06 - ETA: 14:58 - loss: 0.5729 - accuracy: 0.7599 - f1: 0.07 - ETA: 14:52 - loss: 0.5745 - accuracy: 0.7582 - f1: 0.07 - ETA: 14:47 - loss: 0.5732 - accuracy: 0.7591 - f1: 0.07 - ETA: 14:41 - loss: 0.5727 - accuracy: 0.7594 - f1: 0.07 - ETA: 14:35 - loss: 0.5737 - accuracy: 0.7583 - f1: 0.07 - ETA: 14:29 - loss: 0.5756 - accuracy: 0.7577 - f1: 0.07 - ETA: 14:24 - loss: 0.5753 - accuracy: 0.7580 - f1: 0.07 - ETA: 14:18 - loss: 0.5765 - accuracy: 0.7579 - f1: 0.07 - ETA: 14:12 - loss: 0.5759 - accuracy: 0.7578 - f1: 0.07 - ETA: 14:06 - loss: 0.5739 - accuracy: 0.7591 - f1: 0.07 - ETA: 14:01 - loss: 0.5738 - accuracy: 0.7585 - f1: 0.07 - ETA: 13:55 - loss: 0.5791 - accuracy: 0.7548 - f1: 0.06 - ETA: 13:49 - loss: 0.5796 - accuracy: 0.7548 - f1: 0.06 - ETA: 13:44 - loss: 0.5817 - accuracy: 0.7530 - f1: 0.06 - ETA: 13:38 - loss: 0.5779 - accuracy: 0.7559 - f1: 0.06 - ETA: 13:32 - loss: 0.5762 - accuracy: 0.7575 - f1: 0.06 - ETA: 13:27 - loss: 0.5759 - accuracy: 0.7586 - f1: 0.06 - ETA: 13:21 - loss: 0.5746 - accuracy: 0.7589 - f1: 0.06 - ETA: 13:15 - loss: 0.5767 - accuracy: 0.7588 - f1: 0.06 - ETA: 13:09 - loss: 0.5756 - accuracy: 0.7595 - f1: 0.06 - ETA: 13:04 - loss: 0.5765 - accuracy: 0.7594 - f1: 0.06 - ETA: 12:58 - loss: 0.5759 - accuracy: 0.7589 - f1: 0.06 - ETA: 12:52 - loss: 0.5762 - accuracy: 0.7588 - f1: 0.06 - ETA: 12:47 - loss: 0.5741 - accuracy: 0.7598 - f1: 0.06 - ETA: 12:41 - loss: 0.5749 - accuracy: 0.7593 - f1: 0.06 - ETA: 12:35 - loss: 0.5752 - accuracy: 0.7592 - f1: 0.06 - ETA: 12:29 - loss: 0.5749 - accuracy: 0.7594 - f1: 0.06 - ETA: 12:24 - loss: 0.5750 - accuracy: 0.7597 - f1: 0.06 - ETA: 12:18 - loss: 0.5748 - accuracy: 0.7603 - f1: 0.06 - ETA: 12:12 - loss: 0.5746 - accuracy: 0.7602 - f1: 0.06 - ETA: 12:07 - loss: 0.5747 - accuracy: 0.7608 - f1: 0.06 - ETA: 12:01 - loss: 0.5740 - accuracy: 0.7617 - f1: 0.06 - ETA: 11:55 - loss: 0.5728 - accuracy: 0.7622 - f1: 0.06 - ETA: 11:49 - loss: 0.5712 - accuracy: 0.7624 - f1: 0.06 - ETA: 11:44 - loss: 0.5714 - accuracy: 0.7623 - f1: 0.06 - ETA: 11:38 - loss: 0.5728 - accuracy: 0.7609 - f1: 0.06 - ETA: 11:32 - loss: 0.5749 - accuracy: 0.7598 - f1: 0.06 - ETA: 11:27 - loss: 0.5753 - accuracy: 0.7590 - f1: 0.06 - ETA: 11:21 - loss: 0.5741 - accuracy: 0.7589 - f1: 0.06 - ETA: 11:15 - loss: 0.5732 - accuracy: 0.7595 - f1: 0.06 - ETA: 11:09 - loss: 0.5737 - accuracy: 0.7594 - f1: 0.06 - ETA: 11:04 - loss: 0.5746 - accuracy: 0.7593 - f1: 0.06 - ETA: 10:58 - loss: 0.5743 - accuracy: 0.7595 - f1: 0.06 - ETA: 10:52 - loss: 0.5733 - accuracy: 0.7600 - f1: 0.06 - ETA: 10:46 - loss: 0.5738 - accuracy: 0.7599 - f1: 0.06 - ETA: 10:41 - loss: 0.5731 - accuracy: 0.7598 - f1: 0.06 - ETA: 10:35 - loss: 0.5724 - accuracy: 0.7606 - f1: 0.05 - ETA: 10:29 - loss: 0.5717 - accuracy: 0.7611 - f1: 0.05 - ETA: 10:23 - loss: 0.5715 - accuracy: 0.7607 - f1: 0.06 - ETA: 10:18 - loss: 0.5715 - accuracy: 0.7606 - f1: 0.05 - ETA: 10:12 - loss: 0.5704 - accuracy: 0.7611 - f1: 0.05 - ETA: 10:06 - loss: 0.5690 - accuracy: 0.7621 - f1: 0.05 - ETA: 10:00 - loss: 0.5702 - accuracy: 0.7612 - f1: 0.06 - ETA: 9:55 - loss: 0.5704 - accuracy: 0.7611 - f1: 0.0598 - ETA: 9:49 - loss: 0.5708 - accuracy: 0.7607 - f1: 0.059 - ETA: 9:43 - loss: 0.5704 - accuracy: 0.7606 - f1: 0.058 - ETA: 9:37 - loss: 0.5705 - accuracy: 0.7602 - f1: 0.059 - ETA: 9:32 - loss: 0.5712 - accuracy: 0.7596 - f1: 0.059 - ETA: 9:26 - loss: 0.5714 - accuracy: 0.7595 - f1: 0.058 - ETA: 9:20 - loss: 0.5695 - accuracy: 0.7605 - f1: 0.058 - ETA: 9:14 - loss: 0.5701 - accuracy: 0.7604 - f1: 0.059 - ETA: 9:09 - loss: 0.5698 - accuracy: 0.7608 - f1: 0.062 - ETA: 9:03 - loss: 0.5671 - accuracy: 0.7626 - f1: 0.061 - ETA: 8:57 - loss: 0.5665 - accuracy: 0.7632 - f1: 0.061 - ETA: 8:51 - loss: 0.5645 - accuracy: 0.7646 - f1: 0.064 - ETA: 8:46 - loss: 0.5638 - accuracy: 0.7653 - f1: 0.064 - ETA: 8:40 - loss: 0.5635 - accuracy: 0.7654 - f1: 0.063 - ETA: 8:34 - loss: 0.5654 - accuracy: 0.7645 - f1: 0.063 - ETA: 8:29 - loss: 0.5668 - accuracy: 0.7639 - f1: 0.062 - ETA: 8:23 - loss: 0.5661 - accuracy: 0.7645 - f1: 0.062 - ETA: 8:17 - loss: 0.5650 - accuracy: 0.7654 - f1: 0.064 - ETA: 8:11 - loss: 0.5670 - accuracy: 0.7646 - f1: 0.065 - ETA: 8:06 - loss: 0.5671 - accuracy: 0.7644 - f1: 0.066 - ETA: 8:00 - loss: 0.5677 - accuracy: 0.7643 - f1: 0.065 - ETA: 7:54 - loss: 0.5697 - accuracy: 0.7631 - f1: 0.065 - ETA: 7:48 - loss: 0.5694 - accuracy: 0.7630 - f1: 0.066 - ETA: 7:43 - loss: 0.5700 - accuracy: 0.7626 - f1: 0.065 - ETA: 7:37 - loss: 0.5697 - accuracy: 0.7625 - f1: 0.065 - ETA: 7:31 - loss: 0.5720 - accuracy: 0.7615 - f1: 0.064 - ETA: 7:25 - loss: 0.5725 - accuracy: 0.7615 - f1: 0.064 - ETA: 7:20 - loss: 0.5735 - accuracy: 0.7612 - f1: 0.063 - ETA: 7:14 - loss: 0.5744 - accuracy: 0.7606 - f1: 0.0645"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5024/6940 [====================>.........] - ETA: 7:08 - loss: 0.5747 - accuracy: 0.7606 - f1: 0.066 - ETA: 7:02 - loss: 0.5759 - accuracy: 0.7601 - f1: 0.065 - ETA: 6:57 - loss: 0.5758 - accuracy: 0.7604 - f1: 0.065 - ETA: 6:51 - loss: 0.5766 - accuracy: 0.7597 - f1: 0.065 - ETA: 6:45 - loss: 0.5761 - accuracy: 0.7596 - f1: 0.064 - ETA: 6:40 - loss: 0.5754 - accuracy: 0.7598 - f1: 0.064 - ETA: 6:34 - loss: 0.5756 - accuracy: 0.7597 - f1: 0.063 - ETA: 6:28 - loss: 0.5766 - accuracy: 0.7592 - f1: 0.064 - ETA: 6:22 - loss: 0.5750 - accuracy: 0.7604 - f1: 0.067 - ETA: 6:17 - loss: 0.5758 - accuracy: 0.7597 - f1: 0.069 - ETA: 6:11 - loss: 0.5754 - accuracy: 0.7597 - f1: 0.070 - ETA: 6:05 - loss: 0.5750 - accuracy: 0.7598 - f1: 0.071 - ETA: 5:59 - loss: 0.5764 - accuracy: 0.7589 - f1: 0.070 - ETA: 5:54 - loss: 0.5753 - accuracy: 0.7595 - f1: 0.070 - ETA: 5:48 - loss: 0.5755 - accuracy: 0.7600 - f1: 0.069 - ETA: 5:42 - loss: 0.5755 - accuracy: 0.7600 - f1: 0.0697"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-dfe662dd6a31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m )\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Users\\Shintaki\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Defining hyperparameters\n",
    "EPOCHS = 5\n",
    "BS = 32\n",
    "LR=1e-5\n",
    "# Compiling and training\n",
    "bertModel.compile(\n",
    "  optimizer=keras.optimizers.Adam(LR),\n",
    "  loss=\"binary_crossentropy\",\n",
    "  metrics=[\"accuracy\",f1]\n",
    ")\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "(X_train, X_temp, Y_train, Y_temp) = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "(X_valid, X_test, Y_valid, Y_test) = train_test_split(X_temp,Y_temp,test_size=0.5, random_state=42)\n",
    "history = bertModel.fit(\n",
    "  X_train, \n",
    "  Y_train,\n",
    "  validation_data=(X_valid,Y_valid),\n",
    "  batch_size=BS,\n",
    "  shuffle=True,\n",
    "  epochs=EPOCHS,\n",
    "  verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc,test_f1 = bertModel.evaluate(X_test,Y_test)\n",
    "print(\"Accuracy : \",test_acc)\n",
    "print(\"F1 Score :\",test_f1)\n",
    "print(\"Loss :\",test_loss)\n",
    "Yhat=bertModel.predict(X_test)\n",
    "correct=0\n",
    "correctlabels=[0,0,0,0]\n",
    "for i,prediction in enumerate(Yhat):\n",
    "    for j,value in enumerate(prediction):\n",
    "        if (value<0.5):\n",
    "            prediction[j]=0\n",
    "        else:\n",
    "            prediction[j]=1\n",
    "        if(prediction[j]==Y_test[i][j]):\n",
    "            correctlabels[j]+=1\n",
    "    if (np.array_equal(prediction,Y_test[i])):\n",
    "        correct+=1\n",
    "    print(\"Prediction :\",prediction,\" Actual Value :\",Y_test[i] )\n",
    "print(\"Total accuracy : \",correct/len(X_test))\n",
    "print(\"I/E accuracy : \",correctlabels[0]/len(X_test))\n",
    "print(\"N/S accuracy : \",correctlabels[1]/len(X_test))\n",
    "print(\"F/T accuracy : \",correctlabels[2]/len(X_test))\n",
    "print(\"J/P accuracy : \",correctlabels[3]/len(X_test))\n",
    "print(\"Binary accuracy :\",np.sum(correctlabels)/(4*len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
