{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: py-params>=0.9.6 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.31.1)\n",
      "Requirement already satisfied: numpy in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(strng, sep, pos):\n",
    "    strng = strng.split(sep)\n",
    "    return sep.join(strng[:pos]), sep.join(strng[pos:])\n",
    "for i,element in enumerate(data.posts):\n",
    "    data.posts[i],_=split(element,\"|||\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(data.type)\n",
    "plt.title(\"Number of users per personality\")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "  \n",
    "unique_type_list = unique_type_list + [x.lower() for x in unique_type_list]\n",
    "# Preprocess data\n",
    "def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments using regular expressions\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "        temp = re.sub('\\|\\|\\|',\"[SEP] \",temp)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp)\n",
    "        temp = re.sub(\"(SEP )+\",\"[SEP] \",temp) \n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t,\"\")\n",
    "        '''temp= \"[CLS]\" + temp\n",
    "        temp = re.sub(\"(\\[CLS\\] *\\[SEP\\])\",\"[CLS] \",temp)'''\n",
    "        temp = re.sub(\"\\[SEP\\]+\",\"| \",temp) \n",
    "        type_labelized = translate_personality(row[1].type)\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=list_posts\n",
    "Y=list_personality\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the tokenizer and testing it\n",
    "tokenizer = FullTokenizer(\n",
    "  vocab_file=\"./dataset/bert/vocab.txt\"\n",
    ")\n",
    "tokens=tokenizer.tokenize(\"I'm here to stay , hello nice to meet you sir\")\n",
    "tokens=[\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "print(tokens)\n",
    "indices=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the features\n",
    "def tokenize_features(X):\n",
    "    X_tokenized=[]\n",
    "    for i,entry in enumerate(X):\n",
    "        if (i % 500 == 0 or i == 1 or i == len(X)):\n",
    "            print(\"%s of %s rows\" % (i, len(X)))\n",
    "        temp=[]\n",
    "        temp=tokenizer.tokenize(entry)\n",
    "        temp = [w.replace('|', '[SEP]') for w in temp]\n",
    "        if temp[0] == \"[SEP]\":\n",
    "            temp.pop(0)\n",
    "        temp.insert(0,\"[CLS]\")\n",
    "        temp.append(\"[SEP]\")\n",
    "        X_tokenized.append(temp)\n",
    "    return(X_tokenized)\n",
    "\n",
    "X_tokenized=tokenize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the tokens\n",
    "def tokens_to_indices(tokens):\n",
    "    result = []\n",
    "    for element in tokens:\n",
    "        indices =[]\n",
    "        indices=tokenizer.convert_tokens_to_ids(element)\n",
    "        result.append(indices)\n",
    "    return result\n",
    "X_indexed=tokens_to_indices(X_tokenized)\n",
    "print(X_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the longest indexed array\n",
    "def max_post_words(posts):\n",
    "    maxLen=0\n",
    "    averageLen=0\n",
    "    for post in posts:\n",
    "        averageLen=averageLen+len(post)\n",
    "        if maxLen < len(post):\n",
    "            maxLen=len(post)\n",
    "    averageLen=int(averageLen/len(posts))\n",
    "    return maxLen,averageLen\n",
    "maxLen,avgLen=max_post_words(X_indexed)\n",
    "print(maxLen)\n",
    "print(avgLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding padding \n",
    "def padding(indices):\n",
    "    for i,element in enumerate(indices):\n",
    "        pad=maxLen-len(element)\n",
    "        l = [0] * pad\n",
    "        indices[i]=indices[i] + l\n",
    "    return(indices)\n",
    "X=np.array(padding(X_indexed))\n",
    "Y=(np.array(list_personality[:,0])).astype(np.float32)\n",
    "#Y=(np.array(list_personality)).astype(np.float32)\n",
    "# Not enough memory to train model with maxLen input size so i'll take the first 512\n",
    "#X=X[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    with tf.io.gfile.GFile(\"./dataset/bert/bert_config.json\", \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "    input_ids = keras.layers.Input(shape=(maxLen,), dtype='int16', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    X1 = keras.layers.Dense(units=1,name=\"I/E_classifier\", activation=\"sigmoid\")(logits)\n",
    "    '''X2 = keras.layers.Dense(units=1,name=\"N/S_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X3 = keras.layers.Dense(units=1,name=\"F/T_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X4 = keras.layers.Dense(units=1,name=\"J/P_classifier\", activation=\"sigmoid\")(logits)\n",
    "    finalOutput=keras.layers.concatenate(\n",
    "    inputs=[X1,X2,X3,X4],\n",
    "    name='final_output')'''\n",
    "    model = keras.Model(inputs=input_ids, outputs=X1)\n",
    "    model.build(input_shape=(None, maxLen))\n",
    "    load_stock_weights(bert, \"./dataset/bert/bert_model.ckpt\")\n",
    "    model.layers[1].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 643, 768)\n",
      "loader: Skipping weight:[bert/embeddings/position_embeddings/embeddings:0] as the weight shape:[(1083, 768)] is not compatible with the checkpoint:[bert/embeddings/position_embeddings] shape:(512, 768)\n",
      "Done loading 195 BERT weights from: ./dataset/bert/bert_model.ckpt into <bert.model.BertModelLayer object at 0x00000201ADC3F438> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [1]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/position_embeddings\n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 643)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 643, 768)          109328640 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "I/E_classifier (Dense)       (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 109,920,001\n",
      "Trainable params: 591,361\n",
      "Non-trainable params: 109,328,640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bertModel = create_model()\n",
    "bertModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss(y_true,y_pred):\n",
    "    return K.mean(K.sum(K.binary_crossentropy(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 643)\n",
      "(8675,)\n",
      "Train on 6940 samples, validate on 867 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/6940 [==================>...........] - ETA: 1:56:25 - loss: 0.5621 - accuracy: 0.7500 - f1: 0.0000e+ - ETA: 1:38:48 - loss: 0.5362 - accuracy: 0.7656 - f1: 0.1111   - ETA: 1:33:00 - loss: 0.5714 - accuracy: 0.7604 - f1: 0.07 - ETA: 1:28:30 - loss: 0.5846 - accuracy: 0.7500 - f1: 0.05 - ETA: 1:21:32 - loss: 0.5935 - accuracy: 0.7312 - f1: 0.07 - ETA: 1:20:41 - loss: 0.6157 - accuracy: 0.7292 - f1: 0.06 - ETA: 1:19:48 - loss: 0.6014 - accuracy: 0.7366 - f1: 0.08 - ETA: 1:19:09 - loss: 0.6131 - accuracy: 0.7227 - f1: 0.07 - ETA: 1:18:35 - loss: 0.5930 - accuracy: 0.7361 - f1: 0.09 - ETA: 1:18:12 - loss: 0.5893 - accuracy: 0.7312 - f1: 0.08 - ETA: 1:17:33 - loss: 0.5796 - accuracy: 0.7358 - f1: 0.08 - ETA: 1:17:09 - loss: 0.5816 - accuracy: 0.7370 - f1: 0.10 - ETA: 1:16:44 - loss: 0.5999 - accuracy: 0.7284 - f1: 0.09 - ETA: 1:16:17 - loss: 0.6015 - accuracy: 0.7277 - f1: 0.08 - ETA: 1:15:45 - loss: 0.5960 - accuracy: 0.7312 - f1: 0.10 - ETA: 1:14:14 - loss: 0.5944 - accuracy: 0.7305 - f1: 0.09 - ETA: 1:13:00 - loss: 0.5830 - accuracy: 0.7371 - f1: 0.09 - ETA: 1:11:20 - loss: 0.5823 - accuracy: 0.7326 - f1: 0.08 - ETA: 1:11:14 - loss: 0.5796 - accuracy: 0.7352 - f1: 0.08 - ETA: 1:11:05 - loss: 0.5788 - accuracy: 0.7359 - f1: 0.10 - ETA: 1:10:52 - loss: 0.5829 - accuracy: 0.7351 - f1: 0.10 - ETA: 1:10:40 - loss: 0.5763 - accuracy: 0.7401 - f1: 0.09 - ETA: 1:10:25 - loss: 0.5673 - accuracy: 0.7473 - f1: 0.11 - ETA: 1:10:11 - loss: 0.5632 - accuracy: 0.7487 - f1: 0.10 - ETA: 1:09:59 - loss: 0.5609 - accuracy: 0.7513 - f1: 0.10 - ETA: 1:09:47 - loss: 0.5670 - accuracy: 0.7476 - f1: 0.10 - ETA: 1:09:31 - loss: 0.5681 - accuracy: 0.7465 - f1: 0.10 - ETA: 1:09:13 - loss: 0.5738 - accuracy: 0.7433 - f1: 0.10 - ETA: 1:08:57 - loss: 0.5754 - accuracy: 0.7435 - f1: 0.10 - ETA: 1:08:39 - loss: 0.5748 - accuracy: 0.7417 - f1: 0.10 - ETA: 1:08:23 - loss: 0.5817 - accuracy: 0.7369 - f1: 0.09 - ETA: 1:08:07 - loss: 0.5843 - accuracy: 0.7363 - f1: 0.09 - ETA: 1:07:48 - loss: 0.5879 - accuracy: 0.7348 - f1: 0.09 - ETA: 1:07:32 - loss: 0.5890 - accuracy: 0.7316 - f1: 0.09 - ETA: 1:07:13 - loss: 0.5923 - accuracy: 0.7321 - f1: 0.09 - ETA: 1:06:45 - loss: 0.5919 - accuracy: 0.7326 - f1: 0.09 - ETA: 1:05:39 - loss: 0.5930 - accuracy: 0.7331 - f1: 0.09 - ETA: 1:04:36 - loss: 0.5917 - accuracy: 0.7344 - f1: 0.09 - ETA: 1:03:14 - loss: 0.5920 - accuracy: 0.7332 - f1: 0.09 - ETA: 1:02:59 - loss: 0.5911 - accuracy: 0.7328 - f1: 0.09 - ETA: 1:02:42 - loss: 0.5896 - accuracy: 0.7332 - f1: 0.09 - ETA: 1:02:27 - loss: 0.5846 - accuracy: 0.7366 - f1: 0.09 - ETA: 1:02:12 - loss: 0.5846 - accuracy: 0.7362 - f1: 0.09 - ETA: 1:01:56 - loss: 0.5830 - accuracy: 0.7386 - f1: 0.09 - ETA: 1:01:40 - loss: 0.5824 - accuracy: 0.7396 - f1: 0.09 - ETA: 1:01:25 - loss: 0.5816 - accuracy: 0.7398 - f1: 0.09 - ETA: 1:01:07 - loss: 0.5796 - accuracy: 0.7407 - f1: 0.09 - ETA: 1:00:50 - loss: 0.5812 - accuracy: 0.7415 - f1: 0.09 - ETA: 1:00:31 - loss: 0.5787 - accuracy: 0.7430 - f1: 0.09 - ETA: 1:00:14 - loss: 0.5765 - accuracy: 0.7444 - f1: 0.09 - ETA: 59:56 - loss: 0.5801 - accuracy: 0.7433 - f1: 0.0904 - ETA: 59:39 - loss: 0.5807 - accuracy: 0.7434 - f1: 0.08 - ETA: 59:19 - loss: 0.5787 - accuracy: 0.7441 - f1: 0.08 - ETA: 59:00 - loss: 0.5767 - accuracy: 0.7454 - f1: 0.08 - ETA: 58:11 - loss: 0.5782 - accuracy: 0.7449 - f1: 0.08 - ETA: 57:38 - loss: 0.5818 - accuracy: 0.7433 - f1: 0.08 - ETA: 56:32 - loss: 0.5853 - accuracy: 0.7407 - f1: 0.08 - ETA: 56:01 - loss: 0.5853 - accuracy: 0.7414 - f1: 0.08 - ETA: 55:44 - loss: 0.5847 - accuracy: 0.7426 - f1: 0.08 - ETA: 55:25 - loss: 0.5829 - accuracy: 0.7422 - f1: 0.07 - ETA: 55:07 - loss: 0.5856 - accuracy: 0.7403 - f1: 0.08 - ETA: 54:49 - loss: 0.5849 - accuracy: 0.7404 - f1: 0.08 - ETA: 54:30 - loss: 0.5851 - accuracy: 0.7411 - f1: 0.08 - ETA: 54:11 - loss: 0.5819 - accuracy: 0.7432 - f1: 0.08 - ETA: 53:52 - loss: 0.5816 - accuracy: 0.7433 - f1: 0.08 - ETA: 53:34 - loss: 0.5808 - accuracy: 0.7424 - f1: 0.08 - ETA: 53:16 - loss: 0.5821 - accuracy: 0.7425 - f1: 0.09 - ETA: 52:57 - loss: 0.5810 - accuracy: 0.7436 - f1: 0.09 - ETA: 52:38 - loss: 0.5780 - accuracy: 0.7455 - f1: 0.09 - ETA: 52:19 - loss: 0.5780 - accuracy: 0.7464 - f1: 0.09 - ETA: 51:59 - loss: 0.5772 - accuracy: 0.7474 - f1: 0.09 - ETA: 51:40 - loss: 0.5786 - accuracy: 0.7465 - f1: 0.09 - ETA: 51:20 - loss: 0.5795 - accuracy: 0.7461 - f1: 0.09 - ETA: 51:01 - loss: 0.5769 - accuracy: 0.7479 - f1: 0.09 - ETA: 50:40 - loss: 0.5802 - accuracy: 0.7467 - f1: 0.09 - ETA: 50:06 - loss: 0.5801 - accuracy: 0.7451 - f1: 0.09 - ETA: 49:41 - loss: 0.5819 - accuracy: 0.7447 - f1: 0.09 - ETA: 48:58 - loss: 0.5837 - accuracy: 0.7444 - f1: 0.09 - ETA: 48:39 - loss: 0.5838 - accuracy: 0.7449 - f1: 0.09 - ETA: 48:21 - loss: 0.5834 - accuracy: 0.7445 - f1: 0.09 - ETA: 48:02 - loss: 0.5830 - accuracy: 0.7446 - f1: 0.09 - ETA: 47:43 - loss: 0.5815 - accuracy: 0.7454 - f1: 0.09 - ETA: 47:24 - loss: 0.5809 - accuracy: 0.7451 - f1: 0.10 - ETA: 47:06 - loss: 0.5823 - accuracy: 0.7440 - f1: 0.09 - ETA: 46:47 - loss: 0.5831 - accuracy: 0.7430 - f1: 0.09 - ETA: 46:28 - loss: 0.5836 - accuracy: 0.7420 - f1: 0.09 - ETA: 46:08 - loss: 0.5824 - accuracy: 0.7432 - f1: 0.09 - ETA: 45:48 - loss: 0.5820 - accuracy: 0.7433 - f1: 0.09 - ETA: 45:28 - loss: 0.5828 - accuracy: 0.7433 - f1: 0.09 - ETA: 45:09 - loss: 0.5822 - accuracy: 0.7441 - f1: 0.09 - ETA: 44:49 - loss: 0.5827 - accuracy: 0.7428 - f1: 0.09 - ETA: 44:30 - loss: 0.5832 - accuracy: 0.7425 - f1: 0.09 - ETA: 44:10 - loss: 0.5839 - accuracy: 0.7426 - f1: 0.09 - ETA: 43:51 - loss: 0.5844 - accuracy: 0.7427 - f1: 0.09 - ETA: 43:31 - loss: 0.5839 - accuracy: 0.7431 - f1: 0.09 - ETA: 43:11 - loss: 0.5838 - accuracy: 0.7428 - f1: 0.09 - ETA: 42:51 - loss: 0.5825 - accuracy: 0.7436 - f1: 0.09 - ETA: 42:30 - loss: 0.5822 - accuracy: 0.7436 - f1: 0.09 - ETA: 42:10 - loss: 0.5835 - accuracy: 0.7437 - f1: 0.09 - ETA: 41:38 - loss: 0.5820 - accuracy: 0.7444 - f1: 0.09 - ETA: 41:13 - loss: 0.5828 - accuracy: 0.7435 - f1: 0.09 - ETA: 40:47 - loss: 0.5826 - accuracy: 0.7442 - f1: 0.09 - ETA: 40:28 - loss: 0.5812 - accuracy: 0.7451 - f1: 0.09 - ETA: 40:08 - loss: 0.5798 - accuracy: 0.7458 - f1: 0.09 - ETA: 39:49 - loss: 0.5794 - accuracy: 0.7455 - f1: 0.09 - ETA: 39:29 - loss: 0.5800 - accuracy: 0.7450 - f1: 0.09 - ETA: 39:10 - loss: 0.5783 - accuracy: 0.7462 - f1: 0.09 - ETA: 38:50 - loss: 0.5791 - accuracy: 0.7459 - f1: 0.09 - ETA: 38:31 - loss: 0.5790 - accuracy: 0.7457 - f1: 0.09 - ETA: 38:11 - loss: 0.5768 - accuracy: 0.7472 - f1: 0.09 - ETA: 37:51 - loss: 0.5774 - accuracy: 0.7475 - f1: 0.09 - ETA: 37:31 - loss: 0.5794 - accuracy: 0.7464 - f1: 0.09 - ETA: 37:10 - loss: 0.5806 - accuracy: 0.7453 - f1: 0.09 - ETA: 36:50 - loss: 0.5804 - accuracy: 0.7456 - f1: 0.09 - ETA: 36:30 - loss: 0.5797 - accuracy: 0.7462 - f1: 0.09 - ETA: 36:10 - loss: 0.5785 - accuracy: 0.7470 - f1: 0.09 - ETA: 35:50 - loss: 0.5775 - accuracy: 0.7479 - f1: 0.09 - ETA: 35:30 - loss: 0.5791 - accuracy: 0.7468 - f1: 0.09 - ETA: 35:08 - loss: 0.5806 - accuracy: 0.7466 - f1: 0.09 - ETA: 34:40 - loss: 0.5811 - accuracy: 0.7466 - f1: 0.09 - ETA: 34:11 - loss: 0.5817 - accuracy: 0.7464 - f1: 0.09 - ETA: 33:50 - loss: 0.5825 - accuracy: 0.7462 - f1: 0.09 - ETA: 33:29 - loss: 0.5841 - accuracy: 0.7459 - f1: 0.09 - ETA: 33:08 - loss: 0.5839 - accuracy: 0.7462 - f1: 0.09 - ETA: 32:47 - loss: 0.5838 - accuracy: 0.7465 - f1: 0.09 - ETA: 32:26 - loss: 0.5845 - accuracy: 0.7463 - f1: 0.09 - ETA: 32:05 - loss: 0.5854 - accuracy: 0.7456 - f1: 0.09 - ETA: 31:44 - loss: 0.5866 - accuracy: 0.7449 - f1: 0.09 - ETA: 31:23 - loss: 0.5871 - accuracy: 0.7447 - f1: 0.09 - ETA: 31:02 - loss: 0.5866 - accuracy: 0.7450 - f1: 0.09 - ETA: 30:41 - loss: 0.5873 - accuracy: 0.7438 - f1: 0.09 - ETA: 30:20 - loss: 0.5861 - accuracy: 0.7446 - f1: 0.09 - ETA: 29:59 - loss: 0.5858 - accuracy: 0.7451 - f1: 0.08 - ETA: 29:37 - loss: 0.5853 - accuracy: 0.7453 - f1: 0.09 - ETA: 29:16 - loss: 0.5856 - accuracy: 0.7451 - f1: 0.09 - ETA: 28:55 - loss: 0.5858 - accuracy: 0.7454 - f1: 0.09 - ETA: 28:34 - loss: 0.5864 - accuracy: 0.7452 - f1: 0.09 - ETA: 28:13 - loss: 0.5863 - accuracy: 0.7452 - f1: 0.096940/6940 [==============================] - ETA: 27:47 - loss: 0.5859 - accuracy: 0.7460 - f1: 0.09 - ETA: 27:17 - loss: 0.5872 - accuracy: 0.7451 - f1: 0.09 - ETA: 26:48 - loss: 0.5876 - accuracy: 0.7445 - f1: 0.09 - ETA: 26:21 - loss: 0.5878 - accuracy: 0.7445 - f1: 0.09 - ETA: 26:01 - loss: 0.5871 - accuracy: 0.7450 - f1: 0.09 - ETA: 25:40 - loss: 0.5871 - accuracy: 0.7452 - f1: 0.09 - ETA: 25:20 - loss: 0.5864 - accuracy: 0.7453 - f1: 0.09 - ETA: 25:00 - loss: 0.5863 - accuracy: 0.7457 - f1: 0.09 - ETA: 24:36 - loss: 0.5856 - accuracy: 0.7464 - f1: 0.09 - ETA: 24:16 - loss: 0.5858 - accuracy: 0.7464 - f1: 0.09 - ETA: 23:55 - loss: 0.5867 - accuracy: 0.7458 - f1: 0.09 - ETA: 23:35 - loss: 0.5862 - accuracy: 0.7462 - f1: 0.09 - ETA: 23:15 - loss: 0.5869 - accuracy: 0.7457 - f1: 0.09 - ETA: 22:54 - loss: 0.5869 - accuracy: 0.7461 - f1: 0.09 - ETA: 22:34 - loss: 0.5865 - accuracy: 0.7461 - f1: 0.09 - ETA: 22:14 - loss: 0.5862 - accuracy: 0.7463 - f1: 0.09 - ETA: 21:53 - loss: 0.5845 - accuracy: 0.7472 - f1: 0.09 - ETA: 21:33 - loss: 0.5840 - accuracy: 0.7476 - f1: 0.09 - ETA: 21:12 - loss: 0.5847 - accuracy: 0.7472 - f1: 0.09 - ETA: 20:51 - loss: 0.5850 - accuracy: 0.7474 - f1: 0.09 - ETA: 20:31 - loss: 0.5853 - accuracy: 0.7471 - f1: 0.09 - ETA: 20:10 - loss: 0.5851 - accuracy: 0.7467 - f1: 0.09 - ETA: 19:50 - loss: 0.5852 - accuracy: 0.7465 - f1: 0.09 - ETA: 19:29 - loss: 0.5850 - accuracy: 0.7465 - f1: 0.09 - ETA: 19:08 - loss: 0.5842 - accuracy: 0.7467 - f1: 0.09 - ETA: 18:48 - loss: 0.5839 - accuracy: 0.7470 - f1: 0.09 - ETA: 18:25 - loss: 0.5839 - accuracy: 0.7470 - f1: 0.09 - ETA: 18:03 - loss: 0.5855 - accuracy: 0.7460 - f1: 0.09 - ETA: 17:40 - loss: 0.5850 - accuracy: 0.7461 - f1: 0.09 - ETA: 17:19 - loss: 0.5860 - accuracy: 0.7453 - f1: 0.09 - ETA: 16:58 - loss: 0.5861 - accuracy: 0.7457 - f1: 0.09 - ETA: 16:37 - loss: 0.5870 - accuracy: 0.7452 - f1: 0.09 - ETA: 16:16 - loss: 0.5861 - accuracy: 0.7454 - f1: 0.09 - ETA: 15:55 - loss: 0.5874 - accuracy: 0.7447 - f1: 0.09 - ETA: 15:34 - loss: 0.5864 - accuracy: 0.7458 - f1: 0.09 - ETA: 15:13 - loss: 0.5871 - accuracy: 0.7452 - f1: 0.09 - ETA: 14:52 - loss: 0.5873 - accuracy: 0.7450 - f1: 0.09 - ETA: 14:31 - loss: 0.5880 - accuracy: 0.7445 - f1: 0.09 - ETA: 14:09 - loss: 0.5878 - accuracy: 0.7445 - f1: 0.09 - ETA: 13:48 - loss: 0.5868 - accuracy: 0.7451 - f1: 0.09 - ETA: 13:27 - loss: 0.5872 - accuracy: 0.7453 - f1: 0.09 - ETA: 13:06 - loss: 0.5868 - accuracy: 0.7457 - f1: 0.09 - ETA: 12:45 - loss: 0.5870 - accuracy: 0.7452 - f1: 0.09 - ETA: 12:24 - loss: 0.5876 - accuracy: 0.7450 - f1: 0.09 - ETA: 12:03 - loss: 0.5871 - accuracy: 0.7454 - f1: 0.09 - ETA: 11:41 - loss: 0.5866 - accuracy: 0.7456 - f1: 0.09 - ETA: 11:20 - loss: 0.5868 - accuracy: 0.7456 - f1: 0.09 - ETA: 10:59 - loss: 0.5871 - accuracy: 0.7455 - f1: 0.09 - ETA: 10:36 - loss: 0.5873 - accuracy: 0.7448 - f1: 0.09 - ETA: 10:14 - loss: 0.5871 - accuracy: 0.7448 - f1: 0.09 - ETA: 9:52 - loss: 0.5868 - accuracy: 0.7452 - f1: 0.0988 - ETA: 9:31 - loss: 0.5869 - accuracy: 0.7446 - f1: 0.099 - ETA: 9:10 - loss: 0.5868 - accuracy: 0.7448 - f1: 0.098 - ETA: 8:49 - loss: 0.5870 - accuracy: 0.7448 - f1: 0.098 - ETA: 8:28 - loss: 0.5880 - accuracy: 0.7443 - f1: 0.097 - ETA: 8:07 - loss: 0.5879 - accuracy: 0.7444 - f1: 0.098 - ETA: 7:46 - loss: 0.5884 - accuracy: 0.7441 - f1: 0.098 - ETA: 7:25 - loss: 0.5884 - accuracy: 0.7438 - f1: 0.098 - ETA: 7:04 - loss: 0.5882 - accuracy: 0.7438 - f1: 0.098 - ETA: 6:43 - loss: 0.5879 - accuracy: 0.7440 - f1: 0.099 - ETA: 6:21 - loss: 0.5878 - accuracy: 0.7437 - f1: 0.100 - ETA: 6:00 - loss: 0.5872 - accuracy: 0.7439 - f1: 0.101 - ETA: 5:39 - loss: 0.5872 - accuracy: 0.7438 - f1: 0.100 - ETA: 5:18 - loss: 0.5877 - accuracy: 0.7433 - f1: 0.100 - ETA: 4:57 - loss: 0.5873 - accuracy: 0.7437 - f1: 0.102 - ETA: 4:35 - loss: 0.5874 - accuracy: 0.7436 - f1: 0.101 - ETA: 4:14 - loss: 0.5881 - accuracy: 0.7428 - f1: 0.101 - ETA: 3:52 - loss: 0.5887 - accuracy: 0.7423 - f1: 0.102 - ETA: 3:30 - loss: 0.5888 - accuracy: 0.7420 - f1: 0.103 - ETA: 3:09 - loss: 0.5893 - accuracy: 0.7416 - f1: 0.102 - ETA: 2:48 - loss: 0.5887 - accuracy: 0.7421 - f1: 0.104 - ETA: 2:26 - loss: 0.5885 - accuracy: 0.7421 - f1: 0.103 - ETA: 2:05 - loss: 0.5880 - accuracy: 0.7426 - f1: 0.103 - ETA: 1:44 - loss: 0.5884 - accuracy: 0.7422 - f1: 0.102 - ETA: 1:22 - loss: 0.5884 - accuracy: 0.7422 - f1: 0.102 - ETA: 1:01 - loss: 0.5890 - accuracy: 0.7420 - f1: 0.102 - ETA: 40s - loss: 0.5898 - accuracy: 0.7414 - f1: 0.103 - ETA: 18s - loss: 0.5891 - accuracy: 0.7416 - f1: 0.10 - 5143s 741ms/sample - loss: 0.5895 - accuracy: 0.7414 - f1: 0.1032 - val_loss: 0.5090 - val_accuracy: 0.7924 - val_f1: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Defining hyperparameters\n",
    "EPOCHS = 1\n",
    "BS = 32\n",
    "LR=1e-5\n",
    "# Compiling and training\n",
    "bertModel.compile(\n",
    "  optimizer=keras.optimizers.Adam(LR),\n",
    "  loss=\"binary_crossentropy\",\n",
    "  metrics=[\"accuracy\",f1]\n",
    ")\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "(X_train, X_temp, Y_train, Y_temp) = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "(X_valid, X_test, Y_valid, Y_test) = train_test_split(X_temp,Y_temp,test_size=0.5, random_state=42)\n",
    "history = bertModel.fit(\n",
    "  X_train, \n",
    "  Y_train,\n",
    "  validation_data=(X_valid,Y_valid),\n",
    "  batch_size=BS,\n",
    "  shuffle=True,\n",
    "  epochs=EPOCHS,\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868/868 [==============================] - ETA: 2:06 - loss: 0.6506 - accuracy: 0.6875 - f1: 0.0000e+0 - ETA: 2:01 - loss: 0.5433 - accuracy: 0.7656 - f1: 0.0000e+0 - ETA: 1:55 - loss: 0.5227 - accuracy: 0.7812 - f1: 0.0000e+0 - ETA: 1:51 - loss: 0.5448 - accuracy: 0.7656 - f1: 0.0000e+0 - ETA: 1:46 - loss: 0.5512 - accuracy: 0.7625 - f1: 0.0000e+0 - ETA: 1:41 - loss: 0.5468 - accuracy: 0.7656 - f1: 0.0000e+0 - ETA: 1:36 - loss: 0.5374 - accuracy: 0.7723 - f1: 0.0000e+0 - ETA: 1:31 - loss: 0.5465 - accuracy: 0.7656 - f1: 0.0000e+0 - ETA: 1:26 - loss: 0.5393 - accuracy: 0.7708 - f1: 0.0000e+0 - ETA: 1:22 - loss: 0.5341 - accuracy: 0.7750 - f1: 0.0000e+0 - ETA: 1:17 - loss: 0.5338 - accuracy: 0.7756 - f1: 0.0000e+0 - ETA: 1:12 - loss: 0.5374 - accuracy: 0.7734 - f1: 0.0000e+0 - ETA: 1:07 - loss: 0.5488 - accuracy: 0.7644 - f1: 0.0000e+0 - ETA: 1:03 - loss: 0.5387 - accuracy: 0.7723 - f1: 0.0000e+0 - ETA: 58s - loss: 0.5483 - accuracy: 0.7646 - f1: 0.0000e+0 - ETA: 53s - loss: 0.5490 - accuracy: 0.7637 - f1: 0.0000e+ - ETA: 48s - loss: 0.5428 - accuracy: 0.7684 - f1: 0.0000e+ - ETA: 44s - loss: 0.5533 - accuracy: 0.7604 - f1: 0.0000e+ - ETA: 39s - loss: 0.5582 - accuracy: 0.7566 - f1: 0.0000e+ - ETA: 34s - loss: 0.5590 - accuracy: 0.7563 - f1: 0.0000e+ - ETA: 29s - loss: 0.5598 - accuracy: 0.7560 - f1: 0.0000e+ - ETA: 24s - loss: 0.5543 - accuracy: 0.7599 - f1: 0.0000e+ - ETA: 20s - loss: 0.5566 - accuracy: 0.7582 - f1: 0.0000e+ - ETA: 15s - loss: 0.5531 - accuracy: 0.7604 - f1: 0.0000e+ - ETA: 10s - loss: 0.5474 - accuracy: 0.7650 - f1: 0.0000e+ - ETA: 5s - loss: 0.5450 - accuracy: 0.7668 - f1: 0.0000e+00 - ETA: 0s - loss: 0.5429 - accuracy: 0.7685 - f1: 0.0000e+0 - 132s 153ms/sample - loss: 0.5445 - accuracy: 0.7673 - f1: 0.0000e+00\n",
      "Accuracy :  0.7672811\n",
      "F1 Score : 0.0\n",
      "Loss : 0.5444843219721922\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc,test_f1 = bertModel.evaluate(X_test,Y_test)\n",
    "print(\"Accuracy : \",test_acc)\n",
    "print(\"F1 Score :\",test_f1)\n",
    "print(\"Loss :\",test_loss)\n",
    "Yhat=bertModel.predict(X_test)\n",
    "correct=0\n",
    "correctlabels=[0,0,0,0]\n",
    "for i,prediction in enumerate(Yhat):\n",
    "    for j,value in enumerate(prediction):\n",
    "        if (value<0.5):\n",
    "            prediction[j]=0\n",
    "        else:\n",
    "            prediction[j]=1\n",
    "        if(prediction[j]==Y_test[i][j]):\n",
    "            correctlabels[j]+=1\n",
    "    if (np.array_equal(prediction,Y_test[i])):\n",
    "        correct+=1\n",
    "    print(\"Prediction :\",prediction,\" Actual Value :\",Y_test[i] )\n",
    "print(\"Total accuracy : \",correct/len(X_test))\n",
    "print(\"I/E accuracy : \",correctlabels[0]/len(X_test))\n",
    "print(\"N/S accuracy : \",correctlabels[1]/len(X_test))\n",
    "print(\"F/T accuracy : \",correctlabels[2]/len(X_test))\n",
    "print(\"J/P accuracy : \",correctlabels[3]/len(X_test))\n",
    "print(\"Binary accuracy :\",np.sum(correctlabels)/(4*len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
