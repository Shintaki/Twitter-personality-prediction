{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.8.0)\n",
      "Requirement already satisfied: py-params>=0.9.6 in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: numpy in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\shintaki\\anaconda3\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the first 5 posts of a user\n",
    "def split(strng, sep, pos):\n",
    "    strng = strng.split(sep)\n",
    "    return sep.join(strng[:pos]), sep.join(strng[pos:])\n",
    "for i,element in enumerate(data.posts):\n",
    "    data.posts[i],_=split(element,\"|||\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart = sns.countplot(data.type)\n",
    "plt.title(\"Number of users per personality\")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "  \n",
    "unique_type_list = unique_type_list + [x.lower() for x in unique_type_list]\n",
    "# Preprocess data\n",
    "def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments using regular expressions\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "        temp = re.sub('\\|\\|\\|',\"[SEP] \",temp)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp)\n",
    "        temp = re.sub(\"(SEP )+\",\"[SEP] \",temp) \n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t,\"\")\n",
    "        '''temp= \"[CLS]\" + temp\n",
    "        temp = re.sub(\"(\\[CLS\\] *\\[SEP\\])\",\"[CLS] \",temp)'''\n",
    "        temp = re.sub(\"\\[SEP\\]+\",\"| \",temp) \n",
    "        type_labelized = translate_personality(row[1].type)\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |   and  moments sportscenter not top ten plays pranks |  What has been the most life changing experience in your life |  On repeat for most of today \n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X=list_posts\n",
    "Y=list_personality\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', \"'\", 'm', 'here', 'to', 'stay', ',', 'hello', 'nice', 'to', 'meet', 'you', 'sir', '[SEP]']\n",
      "[101, 1045, 1005, 1049, 2182, 2000, 2994, 1010, 7592, 3835, 2000, 3113, 2017, 2909, 102]\n"
     ]
    }
   ],
   "source": [
    "# importing the tokenizer and testing it\n",
    "tokenizer = FullTokenizer(\n",
    "  vocab_file=\"./dataset/bert/vocab.txt\"\n",
    ")\n",
    "tokens=tokenizer.tokenize(\"I'm here to stay , hello nice to meet you sir\")\n",
    "tokens=[\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "print(tokens)\n",
    "indices=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 8675 rows\n",
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the features\n",
    "def tokenize_features(X):\n",
    "    X_tokenized=[]\n",
    "    for i,entry in enumerate(X):\n",
    "        if (i % 500 == 0 or i == 1 or i == len(X)):\n",
    "            print(\"%s of %s rows\" % (i, len(X)))\n",
    "        temp=[]\n",
    "        temp=tokenizer.tokenize(entry)\n",
    "        temp = [w.replace('|', '[SEP]') for w in temp]\n",
    "        if temp[0] == \"[SEP]\":\n",
    "            temp.pop(0)\n",
    "        temp.insert(0,\"[CLS]\")\n",
    "        temp.append(\"[SEP]\")\n",
    "        X_tokenized.append(temp)\n",
    "    return(X_tokenized)\n",
    "\n",
    "X_tokenized=tokenize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1998, 5312, 2998, 13013, 2121, 2025, 2327, 2702, 3248, 26418, 2015, 102, 2054, 2038, 2042, 1996, 2087, 2166, 5278, 3325, 1999, 2115, 2166, 102, 2006, 9377, 2005, 2087, 1997, 2651, 102]\n"
     ]
    }
   ],
   "source": [
    "# indexing the tokens\n",
    "def tokens_to_indices(tokens):\n",
    "    result = []\n",
    "    for element in tokens:\n",
    "        indices =[]\n",
    "        indices=tokenizer.convert_tokens_to_ids(element)\n",
    "        result.append(indices)\n",
    "    return result\n",
    "X_indexed=tokens_to_indices(X_tokenized)\n",
    "print(X_indexed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "# getting the longest indexed array\n",
    "def max_post_words(posts):\n",
    "    maxLen=0\n",
    "    averageLen=0\n",
    "    for post in posts:\n",
    "        averageLen=averageLen+len(post)\n",
    "        if maxLen < len(post):\n",
    "            maxLen=len(post)\n",
    "    averageLen=int(averageLen/len(posts))\n",
    "    return maxLen,averageLen\n",
    "maxLen,avgLen=max_post_words(X_indexed)\n",
    "print(maxLen)\n",
    "print(avgLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding padding \n",
    "def padding(indices):\n",
    "    for i,element in enumerate(indices):\n",
    "        pad=maxLen-len(element)\n",
    "        l = [0] * pad\n",
    "        indices[i]=indices[i] + l\n",
    "    return(indices)\n",
    "X=np.array(padding(X_indexed))\n",
    "Y=(np.array(list_personality[:,0])).astype(np.float32)\n",
    "#Y=(np.array(list_personality)).astype(np.float32)\n",
    "# Not enough memory to train model with maxLen input size so i'll take the first 512\n",
    "#X=X[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    with tf.io.gfile.GFile(\"./dataset/bert/bert_config.json\", \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "    input_ids = keras.layers.Input(shape=(maxLen,), dtype='int16', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=384, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.25)(logits)\n",
    "    X1 = keras.layers.Dense(units=1,name=\"I/E_classifier\", activation=\"sigmoid\")(logits)\n",
    "    '''X2 = keras.layers.Dense(units=1,name=\"N/S_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X3 = keras.layers.Dense(units=1,name=\"F/T_classifier\", activation=\"sigmoid\")(logits)\n",
    "    X4 = keras.layers.Dense(units=1,name=\"J/P_classifier\", activation=\"sigmoid\")(logits)\n",
    "    finalOutput=keras.layers.concatenate(\n",
    "    inputs=[X1,X2,X3,X4],\n",
    "    name='final_output')'''\n",
    "    model = keras.Model(inputs=input_ids, outputs=X1)\n",
    "    model.build(input_shape=(None, maxLen))\n",
    "    load_stock_weights(bert, \"./dataset/bert/bert_model.ckpt\")\n",
    "    model.layers[1].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 248, 768)\n",
      "loader: Skipping weight:[bert/embeddings/position_embeddings/embeddings:0] as the weight shape:[(1083, 768)] is not compatible with the checkpoint:[bert/embeddings/position_embeddings] shape:(512, 768)\n",
      "Done loading 195 BERT weights from: ./dataset/bert/bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000021DB9138EF0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [1]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/position_embeddings\n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 248)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 248, 768)          109328640 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 384)               295296    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "I/E_classifier (Dense)       (None, 1)                 385       \n",
      "=================================================================\n",
      "Total params: 109,624,321\n",
      "Trainable params: 295,681\n",
      "Non-trainable params: 109,328,640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bertModel = create_model()\n",
    "bertModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss(y_true,y_pred):\n",
    "    return K.mean(K.sum(K.binary_crossentropy(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 248)\n",
      "(8675,)\n",
      "Train on 6940 samples, validate on 867 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/6940 [==================>...........] - ETA: 46:40 - loss: 0.6703 - accuracy: 0.6250 - f1: 0.33 - ETA: 26:47 - loss: 0.6944 - accuracy: 0.5625 - f1: 0.30 - ETA: 20:07 - loss: 0.6767 - accuracy: 0.5938 - f1: 0.34 - ETA: 16:46 - loss: 0.6691 - accuracy: 0.5859 - f1: 0.37 - ETA: 14:44 - loss: 0.6616 - accuracy: 0.5938 - f1: 0.34 - ETA: 13:22 - loss: 0.6556 - accuracy: 0.6094 - f1: 0.33 - ETA: 12:24 - loss: 0.6553 - accuracy: 0.6116 - f1: 0.30 - ETA: 11:39 - loss: 0.6598 - accuracy: 0.6016 - f1: 0.30 - ETA: 11:03 - loss: 0.6577 - accuracy: 0.5972 - f1: 0.28 - ETA: 10:35 - loss: 0.6586 - accuracy: 0.6031 - f1: 0.29 - ETA: 10:11 - loss: 0.6630 - accuracy: 0.5994 - f1: 0.27 - ETA: 9:51 - loss: 0.6522 - accuracy: 0.6172 - f1: 0.2880 - ETA: 9:33 - loss: 0.6551 - accuracy: 0.6130 - f1: 0.282 - ETA: 9:18 - loss: 0.6523 - accuracy: 0.6161 - f1: 0.287 - ETA: 9:05 - loss: 0.6481 - accuracy: 0.6208 - f1: 0.293 - ETA: 8:53 - loss: 0.6413 - accuracy: 0.6328 - f1: 0.291 - ETA: 8:42 - loss: 0.6426 - accuracy: 0.6305 - f1: 0.287 - ETA: 8:32 - loss: 0.6399 - accuracy: 0.6372 - f1: 0.290 - ETA: 8:23 - loss: 0.6365 - accuracy: 0.6382 - f1: 0.283 - ETA: 8:15 - loss: 0.6338 - accuracy: 0.6438 - f1: 0.278 - ETA: 8:08 - loss: 0.6335 - accuracy: 0.6473 - f1: 0.265 - ETA: 8:00 - loss: 0.6318 - accuracy: 0.6520 - f1: 0.253 - ETA: 7:54 - loss: 0.6344 - accuracy: 0.6535 - f1: 0.242 - ETA: 7:47 - loss: 0.6311 - accuracy: 0.6615 - f1: 0.255 - ETA: 7:41 - loss: 0.6260 - accuracy: 0.6675 - f1: 0.265 - ETA: 7:36 - loss: 0.6260 - accuracy: 0.6683 - f1: 0.261 - ETA: 7:30 - loss: 0.6232 - accuracy: 0.6701 - f1: 0.262 - ETA: 7:25 - loss: 0.6215 - accuracy: 0.6719 - f1: 0.260 - ETA: 7:20 - loss: 0.6205 - accuracy: 0.6735 - f1: 0.251 - ETA: 7:15 - loss: 0.6191 - accuracy: 0.6781 - f1: 0.251 - ETA: 7:11 - loss: 0.6242 - accuracy: 0.6784 - f1: 0.248 - ETA: 7:07 - loss: 0.6209 - accuracy: 0.6807 - f1: 0.240 - ETA: 7:02 - loss: 0.6168 - accuracy: 0.6856 - f1: 0.241 - ETA: 6:58 - loss: 0.6165 - accuracy: 0.6866 - f1: 0.234 - ETA: 6:54 - loss: 0.6164 - accuracy: 0.6866 - f1: 0.236 - ETA: 6:50 - loss: 0.6156 - accuracy: 0.6858 - f1: 0.233 - ETA: 6:47 - loss: 0.6158 - accuracy: 0.6841 - f1: 0.231 - ETA: 6:43 - loss: 0.6150 - accuracy: 0.6850 - f1: 0.225 - ETA: 6:39 - loss: 0.6124 - accuracy: 0.6867 - f1: 0.219 - ETA: 6:36 - loss: 0.6067 - accuracy: 0.6930 - f1: 0.230 - ETA: 6:32 - loss: 0.6086 - accuracy: 0.6913 - f1: 0.225 - ETA: 6:29 - loss: 0.6144 - accuracy: 0.6890 - f1: 0.219 - ETA: 6:26 - loss: 0.6121 - accuracy: 0.6911 - f1: 0.214 - ETA: 6:22 - loss: 0.6161 - accuracy: 0.6903 - f1: 0.209 - ETA: 6:19 - loss: 0.6111 - accuracy: 0.6951 - f1: 0.205 - ETA: 6:16 - loss: 0.6084 - accuracy: 0.6970 - f1: 0.200 - ETA: 6:13 - loss: 0.6139 - accuracy: 0.6941 - f1: 0.196 - ETA: 6:10 - loss: 0.6117 - accuracy: 0.6979 - f1: 0.202 - ETA: 6:07 - loss: 0.6139 - accuracy: 0.6971 - f1: 0.198 - ETA: 6:04 - loss: 0.6163 - accuracy: 0.6963 - f1: 0.197 - ETA: 6:01 - loss: 0.6156 - accuracy: 0.6967 - f1: 0.193 - ETA: 5:58 - loss: 0.6147 - accuracy: 0.6977 - f1: 0.190 - ETA: 5:56 - loss: 0.6112 - accuracy: 0.7005 - f1: 0.186 - ETA: 5:53 - loss: 0.6122 - accuracy: 0.7002 - f1: 0.183 - ETA: 5:50 - loss: 0.6118 - accuracy: 0.7011 - f1: 0.179 - ETA: 5:47 - loss: 0.6129 - accuracy: 0.6992 - f1: 0.176 - ETA: 5:44 - loss: 0.6100 - accuracy: 0.7012 - f1: 0.173 - ETA: 5:42 - loss: 0.6099 - accuracy: 0.7026 - f1: 0.174 - ETA: 5:39 - loss: 0.6083 - accuracy: 0.7044 - f1: 0.171 - ETA: 5:36 - loss: 0.6033 - accuracy: 0.7083 - f1: 0.168 - ETA: 5:34 - loss: 0.6028 - accuracy: 0.7085 - f1: 0.165 - ETA: 5:31 - loss: 0.6003 - accuracy: 0.7107 - f1: 0.163 - ETA: 5:29 - loss: 0.5991 - accuracy: 0.7113 - f1: 0.160 - ETA: 5:26 - loss: 0.6006 - accuracy: 0.7109 - f1: 0.157 - ETA: 5:24 - loss: 0.5996 - accuracy: 0.7125 - f1: 0.155 - ETA: 5:21 - loss: 0.5974 - accuracy: 0.7145 - f1: 0.153 - ETA: 5:19 - loss: 0.5963 - accuracy: 0.7160 - f1: 0.154 - ETA: 5:16 - loss: 0.5953 - accuracy: 0.7178 - f1: 0.152 - ETA: 5:14 - loss: 0.5941 - accuracy: 0.7188 - f1: 0.150 - ETA: 5:11 - loss: 0.5972 - accuracy: 0.7179 - f1: 0.148 - ETA: 5:09 - loss: 0.5977 - accuracy: 0.7183 - f1: 0.145 - ETA: 5:06 - loss: 0.5969 - accuracy: 0.7201 - f1: 0.143 - ETA: 5:04 - loss: 0.5969 - accuracy: 0.7200 - f1: 0.141 - ETA: 5:01 - loss: 0.6009 - accuracy: 0.7175 - f1: 0.140 - ETA: 4:59 - loss: 0.5985 - accuracy: 0.7196 - f1: 0.138 - ETA: 4:57 - loss: 0.5970 - accuracy: 0.7208 - f1: 0.136 - ETA: 4:54 - loss: 0.5985 - accuracy: 0.7196 - f1: 0.134 - ETA: 4:52 - loss: 0.5956 - accuracy: 0.7212 - f1: 0.132 - ETA: 4:50 - loss: 0.5974 - accuracy: 0.7203 - f1: 0.133 - ETA: 4:47 - loss: 0.5961 - accuracy: 0.7215 - f1: 0.131 - ETA: 4:45 - loss: 0.5954 - accuracy: 0.7222 - f1: 0.129 - ETA: 4:43 - loss: 0.5972 - accuracy: 0.7210 - f1: 0.128 - ETA: 4:40 - loss: 0.5987 - accuracy: 0.7206 - f1: 0.126 - ETA: 4:38 - loss: 0.5974 - accuracy: 0.7221 - f1: 0.125 - ETA: 4:36 - loss: 0.5991 - accuracy: 0.7217 - f1: 0.123 - ETA: 4:33 - loss: 0.5996 - accuracy: 0.7217 - f1: 0.122 - ETA: 4:31 - loss: 0.5990 - accuracy: 0.7223 - f1: 0.123 - ETA: 4:29 - loss: 0.5984 - accuracy: 0.7223 - f1: 0.124 - ETA: 4:27 - loss: 0.5985 - accuracy: 0.7233 - f1: 0.122 - ETA: 4:24 - loss: 0.5985 - accuracy: 0.7243 - f1: 0.125 - ETA: 4:22 - loss: 0.5984 - accuracy: 0.7242 - f1: 0.124 - ETA: 4:20 - loss: 0.5983 - accuracy: 0.7242 - f1: 0.123 - ETA: 4:18 - loss: 0.6005 - accuracy: 0.7228 - f1: 0.121 - ETA: 4:15 - loss: 0.6020 - accuracy: 0.7221 - f1: 0.120 - ETA: 4:13 - loss: 0.6008 - accuracy: 0.7227 - f1: 0.119 - ETA: 4:11 - loss: 0.6025 - accuracy: 0.7220 - f1: 0.117 - ETA: 4:09 - loss: 0.6007 - accuracy: 0.7233 - f1: 0.116 - ETA: 4:06 - loss: 0.6013 - accuracy: 0.7226 - f1: 0.115 - ETA: 4:04 - loss: 0.6003 - accuracy: 0.7238 - f1: 0.117 - ETA: 4:02 - loss: 0.5984 - accuracy: 0.7256 - f1: 0.116 - ETA: 4:00 - loss: 0.5974 - accuracy: 0.7271 - f1: 0.118 - ETA: 3:58 - loss: 0.5974 - accuracy: 0.7276 - f1: 0.117 - ETA: 3:55 - loss: 0.5984 - accuracy: 0.7269 - f1: 0.115 - ETA: 3:53 - loss: 0.5979 - accuracy: 0.7278 - f1: 0.114 - ETA: 3:51 - loss: 0.5978 - accuracy: 0.7280 - f1: 0.113 - ETA: 3:49 - loss: 0.5975 - accuracy: 0.7288 - f1: 0.112 - ETA: 3:47 - loss: 0.5960 - accuracy: 0.7298 - f1: 0.111 - ETA: 3:45 - loss: 0.5951 - accuracy: 0.7303 - f1: 0.112 - ETA: 3:42 - loss: 0.5952 - accuracy: 0.7308 - f1: 0.114 - ETA: 3:40 - loss: 0.5961 - accuracy: 0.7304 - f1: 0.113 - ETA: 3:38 - loss: 0.5974 - accuracy: 0.7294 - f1: 0.112 - ETA: 3:36 - loss: 0.5960 - accuracy: 0.7299 - f1: 0.111 - ETA: 3:34 - loss: 0.5960 - accuracy: 0.7304 - f1: 0.110 - ETA: 3:32 - loss: 0.5955 - accuracy: 0.7311 - f1: 0.112 - ETA: 3:29 - loss: 0.5962 - accuracy: 0.7312 - f1: 0.111 - ETA: 3:27 - loss: 0.5963 - accuracy: 0.7311 - f1: 0.110 - ETA: 3:25 - loss: 0.5977 - accuracy: 0.7302 - f1: 0.109 - ETA: 3:23 - loss: 0.5983 - accuracy: 0.7301 - f1: 0.108 - ETA: 3:21 - loss: 0.5993 - accuracy: 0.7295 - f1: 0.107 - ETA: 3:19 - loss: 0.5983 - accuracy: 0.7307 - f1: 0.106 - ETA: 3:17 - loss: 0.5971 - accuracy: 0.7317 - f1: 0.105 - ETA: 3:14 - loss: 0.5968 - accuracy: 0.7316 - f1: 0.104 - ETA: 3:12 - loss: 0.5955 - accuracy: 0.7327 - f1: 0.106 - ETA: 3:10 - loss: 0.5949 - accuracy: 0.7331 - f1: 0.107 - ETA: 3:08 - loss: 0.5957 - accuracy: 0.7325 - f1: 0.106 - ETA: 3:06 - loss: 0.5971 - accuracy: 0.7316 - f1: 0.105 - ETA: 3:04 - loss: 0.5965 - accuracy: 0.7323 - f1: 0.104 - ETA: 3:02 - loss: 0.5955 - accuracy: 0.7327 - f1: 0.104 - ETA: 3:00 - loss: 0.5946 - accuracy: 0.7330 - f1: 0.103 - ETA: 2:58 - loss: 0.5942 - accuracy: 0.7337 - f1: 0.102 - ETA: 2:55 - loss: 0.5942 - accuracy: 0.7338 - f1: 0.101 - ETA: 2:53 - loss: 0.5944 - accuracy: 0.7332 - f1: 0.101 - ETA: 2:51 - loss: 0.5936 - accuracy: 0.7340 - f1: 0.100 - ETA: 2:49 - loss: 0.5923 - accuracy: 0.7351 - f1: 0.099 - ETA: 2:47 - loss: 0.5916 - accuracy: 0.7356 - f1: 0.101 - ETA: 2:45 - loss: 0.5920 - accuracy: 0.7353 - f1: 0.100 - ETA: 2:43 - loss: 0.5922 - accuracy: 0.7352 - f1: 0.100 - ETA: 2:41 - loss: 0.5920 - accuracy: 0.7355 - f1: 0.099 - ETA: 2:39 - loss: 0.5922 - accuracy: 0.7352 - f1: 0.098 - ETA: 2:37 - loss: 0.5915 - accuracy: 0.7357 - f1: 0.0986940/6940 [==============================] - ETA: 2:34 - loss: 0.5921 - accuracy: 0.7354 - f1: 0.097 - ETA: 2:32 - loss: 0.5917 - accuracy: 0.7355 - f1: 0.096 - ETA: 2:30 - loss: 0.5908 - accuracy: 0.7360 - f1: 0.096 - ETA: 2:28 - loss: 0.5927 - accuracy: 0.7352 - f1: 0.095 - ETA: 2:26 - loss: 0.5916 - accuracy: 0.7360 - f1: 0.094 - ETA: 2:24 - loss: 0.5897 - accuracy: 0.7367 - f1: 0.094 - ETA: 2:22 - loss: 0.5889 - accuracy: 0.7372 - f1: 0.093 - ETA: 2:20 - loss: 0.5898 - accuracy: 0.7367 - f1: 0.092 - ETA: 2:18 - loss: 0.5899 - accuracy: 0.7370 - f1: 0.092 - ETA: 2:16 - loss: 0.5896 - accuracy: 0.7371 - f1: 0.091 - ETA: 2:14 - loss: 0.5902 - accuracy: 0.7370 - f1: 0.090 - ETA: 2:12 - loss: 0.5908 - accuracy: 0.7373 - f1: 0.090 - ETA: 2:10 - loss: 0.5911 - accuracy: 0.7373 - f1: 0.089 - ETA: 2:08 - loss: 0.5901 - accuracy: 0.7384 - f1: 0.089 - ETA: 2:05 - loss: 0.5911 - accuracy: 0.7377 - f1: 0.089 - ETA: 2:03 - loss: 0.5925 - accuracy: 0.7370 - f1: 0.088 - ETA: 2:01 - loss: 0.5920 - accuracy: 0.7373 - f1: 0.088 - ETA: 1:59 - loss: 0.5925 - accuracy: 0.7373 - f1: 0.087 - ETA: 1:57 - loss: 0.5915 - accuracy: 0.7380 - f1: 0.087 - ETA: 1:55 - loss: 0.5927 - accuracy: 0.7373 - f1: 0.087 - ETA: 1:53 - loss: 0.5938 - accuracy: 0.7368 - f1: 0.087 - ETA: 1:51 - loss: 0.5937 - accuracy: 0.7371 - f1: 0.086 - ETA: 1:49 - loss: 0.5943 - accuracy: 0.7364 - f1: 0.086 - ETA: 1:47 - loss: 0.5946 - accuracy: 0.7365 - f1: 0.085 - ETA: 1:45 - loss: 0.5951 - accuracy: 0.7366 - f1: 0.086 - ETA: 1:43 - loss: 0.5947 - accuracy: 0.7370 - f1: 0.085 - ETA: 1:41 - loss: 0.5946 - accuracy: 0.7371 - f1: 0.085 - ETA: 1:39 - loss: 0.5947 - accuracy: 0.7374 - f1: 0.085 - ETA: 1:37 - loss: 0.5950 - accuracy: 0.7369 - f1: 0.086 - ETA: 1:35 - loss: 0.5951 - accuracy: 0.7368 - f1: 0.085 - ETA: 1:33 - loss: 0.5946 - accuracy: 0.7370 - f1: 0.085 - ETA: 1:31 - loss: 0.5959 - accuracy: 0.7367 - f1: 0.084 - ETA: 1:28 - loss: 0.5948 - accuracy: 0.7374 - f1: 0.084 - ETA: 1:26 - loss: 0.5952 - accuracy: 0.7371 - f1: 0.083 - ETA: 1:24 - loss: 0.5947 - accuracy: 0.7375 - f1: 0.083 - ETA: 1:22 - loss: 0.5960 - accuracy: 0.7370 - f1: 0.083 - ETA: 1:20 - loss: 0.5955 - accuracy: 0.7373 - f1: 0.084 - ETA: 1:18 - loss: 0.5960 - accuracy: 0.7370 - f1: 0.084 - ETA: 1:16 - loss: 0.5956 - accuracy: 0.7371 - f1: 0.083 - ETA: 1:14 - loss: 0.5960 - accuracy: 0.7370 - f1: 0.083 - ETA: 1:12 - loss: 0.5954 - accuracy: 0.7376 - f1: 0.082 - ETA: 1:10 - loss: 0.5949 - accuracy: 0.7375 - f1: 0.082 - ETA: 1:08 - loss: 0.5953 - accuracy: 0.7370 - f1: 0.081 - ETA: 1:06 - loss: 0.5950 - accuracy: 0.7373 - f1: 0.081 - ETA: 1:04 - loss: 0.5958 - accuracy: 0.7368 - f1: 0.080 - ETA: 1:02 - loss: 0.5965 - accuracy: 0.7364 - f1: 0.080 - ETA: 1:00 - loss: 0.5960 - accuracy: 0.7368 - f1: 0.080 - ETA: 58s - loss: 0.5969 - accuracy: 0.7365 - f1: 0.079 - ETA: 56s - loss: 0.5966 - accuracy: 0.7369 - f1: 0.07 - ETA: 54s - loss: 0.5959 - accuracy: 0.7373 - f1: 0.07 - ETA: 52s - loss: 0.5965 - accuracy: 0.7363 - f1: 0.07 - ETA: 50s - loss: 0.5964 - accuracy: 0.7365 - f1: 0.07 - ETA: 48s - loss: 0.5967 - accuracy: 0.7367 - f1: 0.07 - ETA: 46s - loss: 0.5963 - accuracy: 0.7370 - f1: 0.07 - ETA: 44s - loss: 0.5957 - accuracy: 0.7373 - f1: 0.07 - ETA: 42s - loss: 0.5949 - accuracy: 0.7380 - f1: 0.07 - ETA: 40s - loss: 0.5951 - accuracy: 0.7381 - f1: 0.07 - ETA: 38s - loss: 0.5948 - accuracy: 0.7382 - f1: 0.07 - ETA: 36s - loss: 0.5939 - accuracy: 0.7389 - f1: 0.07 - ETA: 34s - loss: 0.5941 - accuracy: 0.7387 - f1: 0.07 - ETA: 32s - loss: 0.5931 - accuracy: 0.7393 - f1: 0.07 - ETA: 30s - loss: 0.5925 - accuracy: 0.7398 - f1: 0.07 - ETA: 28s - loss: 0.5935 - accuracy: 0.7392 - f1: 0.07 - ETA: 25s - loss: 0.5933 - accuracy: 0.7393 - f1: 0.07 - ETA: 23s - loss: 0.5937 - accuracy: 0.7390 - f1: 0.07 - ETA: 21s - loss: 0.5926 - accuracy: 0.7398 - f1: 0.07 - ETA: 19s - loss: 0.5936 - accuracy: 0.7393 - f1: 0.07 - ETA: 17s - loss: 0.5945 - accuracy: 0.7387 - f1: 0.07 - ETA: 15s - loss: 0.5951 - accuracy: 0.7385 - f1: 0.07 - ETA: 13s - loss: 0.5940 - accuracy: 0.7393 - f1: 0.07 - ETA: 11s - loss: 0.5937 - accuracy: 0.7395 - f1: 0.07 - ETA: 9s - loss: 0.5940 - accuracy: 0.7392 - f1: 0.0754 - ETA: 7s - loss: 0.5941 - accuracy: 0.7393 - f1: 0.075 - ETA: 5s - loss: 0.5940 - accuracy: 0.7392 - f1: 0.074 - ETA: 3s - loss: 0.5943 - accuracy: 0.7391 - f1: 0.074 - ETA: 1s - loss: 0.5942 - accuracy: 0.7391 - f1: 0.074 - 488s 70ms/sample - loss: 0.5935 - accuracy: 0.7395 - f1: 0.0750 - val_loss: 0.5222 - val_accuracy: 0.7924 - val_f1: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Defining hyperparameters\n",
    "EPOCHS = 1\n",
    "BS = 32\n",
    "LR=1e-5\n",
    "# Compiling and training\n",
    "bertModel.compile(\n",
    "  optimizer=keras.optimizers.Adam(LR),\n",
    "  loss=\"binary_crossentropy\",\n",
    "  metrics=[\"accuracy\",f1]\n",
    ")\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "(X_train, X_temp, Y_train, Y_temp) = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "(X_valid, X_test, Y_valid, Y_test) = train_test_split(X_temp,Y_temp,test_size=0.5, random_state=42)\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train= shuffle(X_train, Y_train,random_state=0)\n",
    "history = bertModel.fit(\n",
    "  X_train, \n",
    "  Y_train,\n",
    "  validation_data=(X_valid,Y_valid),\n",
    "  batch_size=BS,\n",
    "  #shuffle=True,\n",
    "  epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868/868 [==============================] - ETA: 46s - loss: 0.7459 - accuracy: 0.6875 - f1: 0.0000e+ - ETA: 44s - loss: 0.5972 - accuracy: 0.7656 - f1: 0.0000e+ - ETA: 43s - loss: 0.5535 - accuracy: 0.7812 - f1: 0.0000e+ - ETA: 41s - loss: 0.5825 - accuracy: 0.7656 - f1: 0.0000e+ - ETA: 39s - loss: 0.5868 - accuracy: 0.7625 - f1: 0.0000e+ - ETA: 37s - loss: 0.5830 - accuracy: 0.7656 - f1: 0.0000e+ - ETA: 35s - loss: 0.5701 - accuracy: 0.7723 - f1: 0.0000e+ - ETA: 34s - loss: 0.5791 - accuracy: 0.7656 - f1: 0.0000e+ - ETA: 32s - loss: 0.5677 - accuracy: 0.7708 - f1: 0.0000e+ - ETA: 30s - loss: 0.5601 - accuracy: 0.7750 - f1: 0.0000e+ - ETA: 28s - loss: 0.5593 - accuracy: 0.7756 - f1: 0.0000e+ - ETA: 27s - loss: 0.5617 - accuracy: 0.7734 - f1: 0.0000e+ - ETA: 25s - loss: 0.5775 - accuracy: 0.7644 - f1: 0.0000e+ - ETA: 23s - loss: 0.5634 - accuracy: 0.7723 - f1: 0.0000e+ - ETA: 21s - loss: 0.5777 - accuracy: 0.7646 - f1: 0.0000e+ - ETA: 19s - loss: 0.5794 - accuracy: 0.7637 - f1: 0.0000e+ - ETA: 18s - loss: 0.5707 - accuracy: 0.7684 - f1: 0.0000e+ - ETA: 16s - loss: 0.5846 - accuracy: 0.7604 - f1: 0.0000e+ - ETA: 14s - loss: 0.5902 - accuracy: 0.7566 - f1: 0.0000e+ - ETA: 12s - loss: 0.5900 - accuracy: 0.7563 - f1: 0.0000e+ - ETA: 10s - loss: 0.5910 - accuracy: 0.7560 - f1: 0.0000e+ - ETA: 9s - loss: 0.5841 - accuracy: 0.7599 - f1: 0.0000e+00 - ETA: 7s - loss: 0.5865 - accuracy: 0.7582 - f1: 0.0000e+0 - ETA: 5s - loss: 0.5823 - accuracy: 0.7604 - f1: 0.0000e+0 - ETA: 3s - loss: 0.5730 - accuracy: 0.7650 - f1: 0.0000e+0 - ETA: 2s - loss: 0.5701 - accuracy: 0.7668 - f1: 0.0000e+0 - ETA: 0s - loss: 0.5681 - accuracy: 0.7685 - f1: 0.0000e+0 - 48s 56ms/sample - loss: 0.5703 - accuracy: 0.7673 - f1: 0.0000e+00\n",
      "Accuracy :  0.7672811\n",
      "F1 Score : 0.0\n",
      "Loss : 0.5702850060528873\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 1.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]  Actual Value : 0.0\n",
      "Prediction : [0.]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc,test_f1 = bertModel.evaluate(X_test,Y_test)\n",
    "print(\"Accuracy : \",test_acc)\n",
    "print(\"F1 Score :\",test_f1)\n",
    "print(\"Loss :\",test_loss)\n",
    "Yhat=bertModel.predict(X_test)\n",
    "for i,prediction in enumerate(Yhat):\n",
    "    for j,value in enumerate(prediction):\n",
    "        if (value<0.5):\n",
    "            prediction[j]=0\n",
    "        else:\n",
    "            prediction[j]=1\n",
    "    print(\"Prediction :\",prediction,\" Actual Value :\",Y_test[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
